{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Agentic RAG (LangGraph)\n",
        "Made by: Wilfredo Aaron Sosa Ramos"
      ],
      "metadata": {
        "id": "uR7MqrY6Dz1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucSkMsumDvrB",
        "outputId": "a56408bb-5a1d-445f-9039-ce7b70396b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.6/142.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U --quiet langchain-community tiktoken langchain-google-genai langchainhub chromadb langchain langgraph langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q arxiv pymupdf jq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7qq2snWFKNK",
        "outputId": "956cec25-d2c2-4176-8ebd-8fb01a39a3b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.6/746.6 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = userdata.get(var)\n",
        "\n",
        "_set_env(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "TnuG_E3sEMI3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import HumanMessage\n",
        "from functools import reduce\n",
        "from typing import List\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "\n",
        "#WEB\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1000, chunk_overlap=100\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "\n",
        "#IMAGES\n",
        "llm_for_img = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\")\n",
        "\n",
        "def generate_docs_from_img(img_url, verbose: bool=False):\n",
        "    message = HumanMessage(\n",
        "    content=[\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"Give me a summary of what you see in the image. It must be 3 detailed paragraphs.\",\n",
        "            },\n",
        "            {\"type\": \"image_url\", \"image_url\": img_url},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = llm_for_img.invoke([message]).content\n",
        "        print(f\"Generated summary: {response}\")\n",
        "        docs = Document(page_content=response, metadata={\"source\": img_url})\n",
        "        split_docs = text_splitter.split_documents([docs])\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the request due to Invalid Content or Invalid Image URL\")\n",
        "        raise e\n",
        "\n",
        "    return split_docs\n",
        "\n",
        "def process_images_and_generate_docs(img_urls: List[str], verbose: bool = False):\n",
        "    \"\"\"\n",
        "    Processes a list of image URLs to generate documents for each image\n",
        "    and combines all documents into a single flat list.\n",
        "\n",
        "    Args:\n",
        "        img_urls (List[str]): List of image URLs to process.\n",
        "        verbose (bool): Flag to enable verbose output for debugging.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: Combined list of all documents generated from images.\n",
        "    \"\"\"\n",
        "    # Helper function for verbose printing\n",
        "    verbose_print = lambda msg: print(msg) if verbose else None\n",
        "\n",
        "    try:\n",
        "        # Map `generate_docs_from_img` across the list of image URLs\n",
        "        all_docs_lists = list(map(lambda img_url: generate_docs_from_img(img_url, verbose=verbose), img_urls))\n",
        "\n",
        "        # Flatten the list of lists into a single list using `reduce`\n",
        "        combined_docs = reduce(lambda acc, docs: acc + docs, all_docs_lists, [])\n",
        "\n",
        "        verbose_print(f\"Combined documents: {combined_docs}\")\n",
        "        return combined_docs\n",
        "    except Exception as e:\n",
        "        verbose_print(f\"An error occurred: {e}\")\n",
        "        raise\n",
        "\n",
        "image_urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png\",\n",
        "    \"https://promptengineering.org/content/images/2023/08/Prompt-engineering---Large-Language-Model-LLM--Autonomous-Agent-Structure---PromptEngineering.org.jpg\",\n",
        "    \"https://cdn.prod.website-files.com/62528d398a42420e66390ef9/64df79e26db8eb0440ffc01d_HNIkPElHdpC1LtU3cRs2mgOfYuuauuJ0Sxdj7OsLafSTPd6_TFMFTc3VLNJjKrY9_849NUNsRdFnye-fp67B_aXmndJtAHwbMNysxXnAKOLIk7jGCXEF0ReXkt7euSvhK9VOjviBMjLIn9YT4oTgMnU.png\"\n",
        "]\n",
        "\n",
        "# Process the images and get combined documents\n",
        "img_docs = process_images_and_generate_docs(image_urls, verbose=True)\n",
        "\n",
        "#ARXIV\n",
        "# Supports all arguments of `ArxivAPIWrapper`\n",
        "arxiv_loader = ArxivLoader(\n",
        "    query=\"LLM Agents\",\n",
        "    load_max_docs=3,\n",
        ")\n",
        "arxiv_docs = arxiv_loader.load()\n",
        "arxiv_docs_split = text_splitter.split_documents(arxiv_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhESu4VtEOwG",
        "outputId": "508bf7ff-47d9-4dd0-ef9e-82f12f7f0544"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated summary: Certainly, here's a detailed summary of the image, broken into three paragraphs:\n",
            "\n",
            "The image presents a flowchart or a conceptual diagram illustrating the architecture of an intelligent agent. At the core, a central \"Agent\" box, highlighted in a pinkish-red hue, is the focal point of the diagram. This agent is connected to several key components. It receives input from a \"Memory\" module, which is further divided into \"Short-term memory\" and \"Long-term memory,\" indicating different storage durations for information. The agent also has access to a \"Tools\" module, which appears to be a collection of external functionalities, including a \"Calendar()\", \"Calculator()\", \"CodeInterpreter()\", \"Search()\", and an indication that more tools are available. The Agent also outputs to the \"Action\" box, implying the agent's actions are the end result.\n",
            "\n",
            "The agent's decision-making process is further elaborated upon by its connection to a \"Planning\" module. This module suggests that the agent does not act impulsively but rather engages in a planning process. This planning stage is further broken down into four key components: \"Reflection\", \"Self-critics\", \"Chain of thoughts\", and \"Subgoal decomposition\". These components point toward a more sophisticated planning mechanism, suggesting an iterative process where the agent reflects on its plan, critiques its own approach, uses a step-by-step thought process and breaks down larger goals into smaller, more manageable subgoals. The diagram also shows dotted lines going from Memory to Planning and from Tools to Action, implying these are not direct links but have a relationship in the overall process.\n",
            "\n",
            "Overall, the diagram illustrates a complex, multi-faceted approach to creating an intelligent agent. The agent is not merely a reactive system but one that has access to tools, memory, and planning capabilities. The separation of memory into short-term and long-term, along with the planning components, suggests an agent that is capable of learning, adapting and making considered decisions. The diagram suggests an architecture where the agent is not just executing commands but is actively participating in a planning process and reflecting on its own actions. The emphasis on planning and the various planning sub-components highlights the complexity of decision-making and suggests a level of sophistication beyond simple reactive systems.\n",
            "Generated summary: Certainly! Here's a three-paragraph summary of the image, focusing on the details provided:\n",
            "\n",
            "The image depicts the structure of a typical Large Language Model (LLM) agent using a flowchart-like diagram. At the center lies the \"LLM Agent,\" represented by a bright orange rectangle and connected by red lines to other components. These components are categorized as either mandatory (yellow) or optional (blue). The prompt recipe, consisting of \"Instructions\" (mandatory) and \"Persona\" (optional), guides the agent's task execution and output processing. Additionally, the agent interacts with various other elements, including an \"Interface\" (mandatory), \"Tools\" (optional), \"Knowledge\" (optional), and \"Memory\" (optional). The components and their connections illustrate how different inputs and functionalities are integrated into the overall operation of the LLM agent.\n",
            "\n",
            "The image also emphasizes the agent's need for interaction and the capacity for memory and specialized knowledge. The text states that an agent typically interfaces with a human, another agent, or an API. This underscores the agent's role as an interactive tool within a broader ecosystem. The diagram further highlights that the agent can generate \"memories,\" suggesting the capability to learn and recall past interactions or data. The agent also has access to specific domain knowledge and tools, indicating that it can be tailored to perform particular tasks or operate within specific contexts. These features showcase the potential of LLM agents to be dynamic, adaptable, and context-aware.\n",
            "\n",
            "Lastly, the image includes a legend that clearly defines the meaning of the colors used in the diagram. Yellow represents a \"Mandatory Component,\" while blue signifies an \"Optional Component.\" This distinction is crucial for understanding the essential elements required for the LLM agent to function effectively, versus those that can be added to enhance its capabilities. The inclusion of the \"PromptEngineering.org\" logo at the bottom suggests the source of this model and its relevance to the field of prompt engineering, further solidifying its informative nature. The diagram effectively portrays a modular approach to building LLM agents, allowing for flexible and customizable designs.\n",
            "Generated summary: Certainly, let's break down the image.\n",
            "\n",
            "The image presents a diagram of the PlanReAct LAA, which appears to be an architecture for an intelligent agent, likely using a Large Language Model (LLM). The diagram is structured to illustrate the flow of information and actions within the system. At the top, we see the \"Environment\", represented by a dotted-line box, which provides \"Observation\" data to the agent and receives \"Action\" outputs. The main component of the agent is enclosed by a light-green outlined box. Inside this box, we see various interconnected modules and data flows. The flow starts from the left, with a \"Task Instruction\" being fed into a \"Plan Prompt,\" which is then sent to the \"LLM\" (Large Language Model). This LLM seems to be the central processing unit of the agent. The LLM is also linked to a \"Fewshot Prompt\" that provides it with example scenarios, and a \"Memory\" module that stores previous \"Thought\" and \"Plan\" outputs.\n",
            "\n",
            "The diagram illustrates a cyclical process of information processing within the agent. The LLM, after receiving the \"Plan Prompt\" and potentially referencing the \"Fewshot Prompt,\" generates a \"Plan.\" This plan is then passed to the \"Action Parser,\" which converts it into an executable \"Action.\" The action is then sent to the \"Environment\", which responds with a new \"Observation,\" restarting the cycle. In addition to the plan, the LLM also generates a \"Thought\" which goes into the \"Memory\" module. This memory seems to play a key role in the agent’s ability to learn and improve over time. The \"Thought\" output is then used to update the \"Fewshot Prompt\", enabling the agent to improve its responses over time. The pink lines indicate the flow of the \"Thought\" and \"think\" data, while the yellow lines represent the flow of \"Plan\" data.\n",
            "\n",
            "Overall, the diagram represents a sophisticated agent architecture that emphasizes planning, action, and memory for a task-oriented intelligent system. It highlights the iterative nature of the process, where the agent continually observes the environment, plans its actions, acts on the environment, and learns from the results, using a combination of prompts, an LLM, and memory to refine its behavior. The architecture seems to be designed for tasks that require multiple steps and require the agent to adapt to the environment and learn from past experiences. It is designed to allow for the agent to both plan and react to the environment.\n",
            "Combined documents: [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png'}, page_content='Certainly, here\\'s a detailed summary of the image, broken into three paragraphs:\\n\\nThe image presents a flowchart or a conceptual diagram illustrating the architecture of an intelligent agent. At the core, a central \"Agent\" box, highlighted in a pinkish-red hue, is the focal point of the diagram. This agent is connected to several key components. It receives input from a \"Memory\" module, which is further divided into \"Short-term memory\" and \"Long-term memory,\" indicating different storage durations for information. The agent also has access to a \"Tools\" module, which appears to be a collection of external functionalities, including a \"Calendar()\", \"Calculator()\", \"CodeInterpreter()\", \"Search()\", and an indication that more tools are available. The Agent also outputs to the \"Action\" box, implying the agent\\'s actions are the end result.\\n\\nThe agent\\'s decision-making process is further elaborated upon by its connection to a \"Planning\" module. This module suggests that the agent does not act impulsively but rather engages in a planning process. This planning stage is further broken down into four key components: \"Reflection\", \"Self-critics\", \"Chain of thoughts\", and \"Subgoal decomposition\". These components point toward a more sophisticated planning mechanism, suggesting an iterative process where the agent reflects on its plan, critiques its own approach, uses a step-by-step thought process and breaks down larger goals into smaller, more manageable subgoals. The diagram also shows dotted lines going from Memory to Planning and from Tools to Action, implying these are not direct links but have a relationship in the overall process.\\n\\nOverall, the diagram illustrates a complex, multi-faceted approach to creating an intelligent agent. The agent is not merely a reactive system but one that has access to tools, memory, and planning capabilities. The separation of memory into short-term and long-term, along with the planning components, suggests an agent that is capable of learning, adapting and making considered decisions. The diagram suggests an architecture where the agent is not just executing commands but is actively participating in a planning process and reflecting on its own actions. The emphasis on planning and the various planning sub-components highlights the complexity of decision-making and suggests a level of sophistication beyond simple reactive systems.'), Document(metadata={'source': 'https://promptengineering.org/content/images/2023/08/Prompt-engineering---Large-Language-Model-LLM--Autonomous-Agent-Structure---PromptEngineering.org.jpg'}, page_content='Certainly! Here\\'s a three-paragraph summary of the image, focusing on the details provided:\\n\\nThe image depicts the structure of a typical Large Language Model (LLM) agent using a flowchart-like diagram. At the center lies the \"LLM Agent,\" represented by a bright orange rectangle and connected by red lines to other components. These components are categorized as either mandatory (yellow) or optional (blue). The prompt recipe, consisting of \"Instructions\" (mandatory) and \"Persona\" (optional), guides the agent\\'s task execution and output processing. Additionally, the agent interacts with various other elements, including an \"Interface\" (mandatory), \"Tools\" (optional), \"Knowledge\" (optional), and \"Memory\" (optional). The components and their connections illustrate how different inputs and functionalities are integrated into the overall operation of the LLM agent.\\n\\nThe image also emphasizes the agent\\'s need for interaction and the capacity for memory and specialized knowledge. The text states that an agent typically interfaces with a human, another agent, or an API. This underscores the agent\\'s role as an interactive tool within a broader ecosystem. The diagram further highlights that the agent can generate \"memories,\" suggesting the capability to learn and recall past interactions or data. The agent also has access to specific domain knowledge and tools, indicating that it can be tailored to perform particular tasks or operate within specific contexts. These features showcase the potential of LLM agents to be dynamic, adaptable, and context-aware.\\n\\nLastly, the image includes a legend that clearly defines the meaning of the colors used in the diagram. Yellow represents a \"Mandatory Component,\" while blue signifies an \"Optional Component.\" This distinction is crucial for understanding the essential elements required for the LLM agent to function effectively, versus those that can be added to enhance its capabilities. The inclusion of the \"PromptEngineering.org\" logo at the bottom suggests the source of this model and its relevance to the field of prompt engineering, further solidifying its informative nature. The diagram effectively portrays a modular approach to building LLM agents, allowing for flexible and customizable designs.'), Document(metadata={'source': 'https://cdn.prod.website-files.com/62528d398a42420e66390ef9/64df79e26db8eb0440ffc01d_HNIkPElHdpC1LtU3cRs2mgOfYuuauuJ0Sxdj7OsLafSTPd6_TFMFTc3VLNJjKrY9_849NUNsRdFnye-fp67B_aXmndJtAHwbMNysxXnAKOLIk7jGCXEF0ReXkt7euSvhK9VOjviBMjLIn9YT4oTgMnU.png'}, page_content='Certainly, let\\'s break down the image.\\n\\nThe image presents a diagram of the PlanReAct LAA, which appears to be an architecture for an intelligent agent, likely using a Large Language Model (LLM). The diagram is structured to illustrate the flow of information and actions within the system. At the top, we see the \"Environment\", represented by a dotted-line box, which provides \"Observation\" data to the agent and receives \"Action\" outputs. The main component of the agent is enclosed by a light-green outlined box. Inside this box, we see various interconnected modules and data flows. The flow starts from the left, with a \"Task Instruction\" being fed into a \"Plan Prompt,\" which is then sent to the \"LLM\" (Large Language Model). This LLM seems to be the central processing unit of the agent. The LLM is also linked to a \"Fewshot Prompt\" that provides it with example scenarios, and a \"Memory\" module that stores previous \"Thought\" and \"Plan\" outputs.\\n\\nThe diagram illustrates a cyclical process of information processing within the agent. The LLM, after receiving the \"Plan Prompt\" and potentially referencing the \"Fewshot Prompt,\" generates a \"Plan.\" This plan is then passed to the \"Action Parser,\" which converts it into an executable \"Action.\" The action is then sent to the \"Environment\", which responds with a new \"Observation,\" restarting the cycle. In addition to the plan, the LLM also generates a \"Thought\" which goes into the \"Memory\" module. This memory seems to play a key role in the agent’s ability to learn and improve over time. The \"Thought\" output is then used to update the \"Fewshot Prompt\", enabling the agent to improve its responses over time. The pink lines indicate the flow of the \"Thought\" and \"think\" data, while the yellow lines represent the flow of \"Plan\" data.\\n\\nOverall, the diagram represents a sophisticated agent architecture that emphasizes planning, action, and memory for a task-oriented intelligent system. It highlights the iterative nature of the process, where the agent continually observes the environment, plans its actions, acts on the environment, and learns from the results, using a combination of prompts, an LLM, and memory to refine its behavior. The architecture seems to be designed for tasks that require multiple steps and require the agent to adapt to the environment and learn from past experiences. It is designed to allow for the agent to both plan and react to the environment.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"WEB DOCS:\")\n",
        "print(doc_splits)\n",
        "print(len(doc_splits))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RNzuUF2Hdu3",
        "outputId": "6a822b52-5bf5-4db7-8f25-b81d971a659b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WEB DOCS:\n",
            "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Tool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Planning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Citation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nNlp\\nLanguage-Model\\nAgent\\nSteerability\\nPrompting'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"Nlp\\nLanguage-Model\\nAgent\\nSteerability\\nPrompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasic Prompting\\n\\nZero-Shot\\n\\nFew-shot\\n\\nTips for Example Selection\\n\\nTips for Example Ordering\\n\\n\\n\\nInstruction Prompting\\n\\nSelf-Consistency Sampling\\n\\nChain-of-Thought (CoT)\\n\\nTypes of CoT prompts\\n\\nTips and Extensions\\n\\n\\nAutomatic Prompt Design\\n\\nAugmented Language Models\\n\\nRetrieval\\n\\nProgramming Language\\n\\nExternal APIs\\n\\n\\nCitation\\n\\nUseful Resources\\n\\nReferences\\n\\n\\n\\n\\n\\nPrompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\\n[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\\n\\nText: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\\nSentiment: negative\\n\\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\\nSentiment: positive\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\\nSentiment: negative\\n\\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\\nSentiment: positive\\n\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nMany studies looked into how to construct in-context examples to maximize the performance and observed that choice of prompt format, training examples, and the order of the examples can lead to dramatically different performance, from near random guess to near SoTA.\\nZhao et al. (2021) investigated the case of few-shot classification and proposed that several biases with LLM (they use GPT-3 in the experiments) contribute to such high variance: (1) Majority label bias exists if distribution of labels among the examples is unbalanced; (2) Recency bias refers to the tendency where the model may repeat the label at the end; (3) Common token bias indicates that LLM tends to produce common tokens more often than rare tokens. To conquer such bias, they proposed a method to calibrate the label probabilities output by the model to be uniform when the input string is N/A.\\nTips for Example Selection#\\n\\n\\nChoose examples that are semantically similar to the test example using $k$-NN clustering in the embedding space (Liu et al., 2021)\\n\\n\\nTo select a diverse and representative set of examples, Su et al. (2022) proposed to use a graph-based approach: (1) First, construct a directed graph $G=(V, E)$ based on the embedding (e.g. by SBERT or other embedding models) cosine similarity between samples, where each node points to its $k$ nearest neighbors; (2) Start with a set of selected samples $\\\\mathcal{L}=\\\\emptyset$ and a set of remaining samples $\\\\mathcal{U}$. Each sample $u \\\\in \\\\mathcal{U}$ is scored by $$\\n\\\\text{score}(u) = \\\\sum_{v \\\\in \\\\{v \\\\mid (u, v) \\\\in E, v\\\\in \\\\mathcal{U}\\\\}} s(v)\\\\quad\\\\text{where }s(v)=\\\\rho^{- \\\\vert \\\\{\\\\ell \\\\in \\\\mathcal{L} \\\\vert (v, \\\\ell)\\\\in E \\\\}\\\\vert},\\\\quad\\\\rho > 1\\n$$ such that $s(v)$ is low if many of $v$’s neighbors are selected and thus the scoring encourages to pick diverse samples.\\n\\n\\nRubin et al. (2022) proposed to train embeddings via contrastive learning specific to one training dataset for in-context learning sample selection.  Given each training pair $(x, y)$, the quality of one example $e_i$ (formatted input-output pair) can be measured by a conditioned probability assigned by LM: $\\\\text{score}(e_i) = P_\\\\text{LM}(y \\\\mid e_i, x)$. We can identify other examples with top-$k$ and bottom-$k$ scores as positive and negative sets of candidates for every training pair and use that for contrastive learning.\\n\\n\\nSome researchers tried Q-Learning to do sample selection. (Zhang et al. 2022)\\n\\n\\nMotivated by uncertainty-based active learning, Diao et al. (2023) suggested to identify examples with high disagreement or entropy among multiple sampling trials. Then annotate these examples to be used in few-shot prompts.\\n\\n\\nTips for Example Ordering#\\n\\nA general suggestion is to keep the selection of examples diverse, relevant to the test sample and in random order to avoid majority label bias and recency bias.\\nIncreasing model sizes or including more training examples does not reduce variance among different permutations of in-context examples. Same order may work well for one model but badly for another. When the validation set is limited, consider choosing the order such that the model does not produce extremely unbalanced predictions or being overconfident about its predictions. (Lu et al. 2022)\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Instruction Prompting#\\nThe purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?\\nInstructed LM (e.g. InstructGPT, natural instruction) finetunes a pretrained model with high-quality tuples of (task instruction, input, ground truth output) to make LM better understand user intention and follow instruction. RLHF (Reinforcement Learning from Human Feedback) is a common method to do so. The benefit of instruction following style fine-tuning improves the model to be more aligned with human intention and greatly reduces the cost of communication.\\nWhen interacting with instruction models, we should describe the task requirement in details, trying to be specific and precise and avoiding say “not do something” but rather specify what to do.\\nPlease label the sentiment towards the movie of the given movie review. The sentiment label should be \"positive\" or \"negative\". \\nText: i\\'ll bet the video game is a lot more fun than the film. \\nSentiment:\\nExplaining the desired audience is another smart way to give instructions\\n\\nFor example to produce education materials for kids,\\n\\nDescribe what is quantum physics to a 6-year-old.\\n\\nAnd safe content,\\n\\n... in language that is safe for work.\\nIn-context instruction learning (Ye et al. 2023) combines few-shot learning with instruction prompting. It incorporates multiple demonstration examples across different tasks in the prompt, each demonstration consisting of instruction, task input and output. Note that their experiments were only on classification tasks and the instruction prompt contains all label options.\\nDefinition: Determine the speaker of the dialogue, \"agent\" or \"customer\".\\nInput: I have successfully booked your tickets.\\nOuput: agent\\n\\nDefinition: Determine which category the question asks for, \"Quantity\" or \"Location\".\\nInput: What\\'s the oldest building in US?\\nOuput: Location\\n\\nDefinition: Classify the sentiment of the given movie review, \"positive\" or \"negative\".\\nInput: i\\'ll bet the video game is a lot more fun than the film.\\nOutput:\\nSelf-Consistency Sampling#\\nSelf-consistency sampling (Wang et al. 2022a) is to sample multiple outputs with temperature > 0 and then selecting the best one out of these candidates.\\nThe criteria for selecting the best candidate can vary from task to task. A general solution is to pick majority vote. For tasks that are easy to validate such as a programming question with unit tests, we can simply run through the interpreter and verify the correctness with unit tests.\\nChain-of-Thought (CoT)#\\nChain-of-thought (CoT) prompting (Wei et al. 2022) generates a sequence of short sentences to describe reasoning logics step by step, known as reasoning chains or rationales, to eventually lead to the final answer. The benefit of CoT is more pronounced for complicated reasoning tasks, while using large models (e.g. with more than 50B parameters). Simple tasks only benefit slightly from CoT prompting.\\nTypes of CoT prompts#\\nTwo main types of CoT prompting:\\n\\nFew-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Few-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.\\n\\n(All the math reasoning examples are from GSM8k)\\nQuestion: Tom and Elizabeth have a competition to climb a hill. Elizabeth takes 30 minutes to climb the hill. Tom takes four times as long as Elizabeth does to climb the hill. How many hours does it take Tom to climb up the hill?\\nAnswer: It takes Tom 30*4 = <<30*4=120>>120 minutes to climb the hill.\\nIt takes Tom 120/60 = <<120/60=2>>2 hours to climb the hill.\\nSo the answer is 2.\\n===\\nQuestion: Jack is a soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks cost $9.50, and the shoes cost $92. Jack has $40. How much more money does Jack need?\\nAnswer: The total cost of two pairs of socks is $9.50 x 2 = $<<9.5*2=19>>19.\\nThe total cost of the socks and the shoes is $19 + $92 = $<<19+92=111>>111.\\nJack need $111 - $40 = $<<111-40=71>>71 more.\\nSo the answer is 71.\\n===\\nQuestion: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\\nAnswer:\\n\\nZero-shot CoT. Use natural language statement like Let's think step by step to explicitly encourage the model to first generate reasoning chains and then to prompt with Therefore, the answer is to produce answers (Kojima et al. 2022 ). Or a similar statement Let's work this out it a step by step to be sure we have the right answer (Zhou et al. 2022).\\n\\nQuestion: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\\nAnswer: Let's think step by step.\\nTips and Extensions#\\n\\n\\nSelf-consistency sampling can improve reasoning accuracy by sampling a number of diverse answers and then taking the majority vote. (Wang et al. 2022a)\\n\\n\\nAnother approach for ensemble learning is to alter the example order or use model generated rationales to replace human-written ones to introduce randomness during multiple sample trials. Then aggregate model outputs with a majority vote to get final answer. (Wang et al. 2022b)\\n\\n\\nIf training examples are only associated with true answers (easy to verify!) but no rationales, we can follow the STaR (Self-Taught Reasoner; Zelikman et al. 2022) method : (1) Ask LLM to generate reasoning chains and only keep those leading to correct answers; (2) Then fine-tune the model with generated rationales and repeat the process until convergence. Note that higher temperature is more likely to generate incorrect rationales with correct answers. If training examples do not have ground truth answers, maybe consider using majority votes as the “correct” answers.\\n\\n\\nPrompts with demonstrations of higher reasoning complexity can achieve better performance, where complexity is measured by the number of reasoning steps in the chains. When separating reasoning steps, newline \\\\n symbol works better than step i, period . or semicolon ;. (Fu et al. 2023)\\n\\n\\nComplexity-based consistency is to explicitly prefer complex chains among all the generations by taking majority vote among only top $k$ complex chains. (Fu et al. 2023)\\n\\n\\nLater, Shum et al. (2023) found that in their experiments CoT prompts with only complex examples can improve the accuracy of complex questions, but perform poorly in simple questions; evidence shown on GSM8k.\\n\\n\\nChanging Q: to Question: is found to be helpful. (Fu et al. 2023)\\n\\n\\nYe & Durrett (2022) found that the benefit of including explanations in the prompt is small to moderate for NLP tasks that involve reasoning over text (i.e. QA and NLI) and the effects vary by models. They observed that explanations are more likely to be nonfactual than be inconsistent (i.e. whether explanation entails prediction). Nonfactual explanations most likely lead to incorrect predictions.\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Ye & Durrett (2022) found that the benefit of including explanations in the prompt is small to moderate for NLP tasks that involve reasoning over text (i.e. QA and NLI) and the effects vary by models. They observed that explanations are more likely to be nonfactual than be inconsistent (i.e. whether explanation entails prediction). Nonfactual explanations most likely lead to incorrect predictions.\\n\\n\\nSelf-Ask (Press et al. 2022) is a method to repeatedly prompt the model to ask following-up questions to construct the thought process iteratively. Follow-up questions can be answered by search engine results. Similarly, IRCoT (Interleaving Retrieval CoT; Trivedi et al. 2022) and ReAct (Reason + Act; Yao et al. 2023) combines iterative CoT prompting with queries to Wikipedia APIs to search for relevant entities and content and then add it back into the context.\\n\\n\\n\\nFig. 1. How Self-Ask works with external search queries.(Image source: Press et al. 2022).\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, essentially creating a tree structure. The search process can be BFS or DFS while each state is evaluated by a classifier (via a prompt) or majority vote.\\n\\n\\nFig. 2. How Self-Ask works with external search queries.(Image source: Yao et al. 2022).\\nAutomatic Prompt Design#\\nPrompt is a sequence of prefix tokens that increase the probability of getting  desired output given input. Therefore we can treat them as trainable parameters and optimize them directly on the embedding space via gradient descent, such as AutoPrompt (Shin et al., 2020, Prefix-Tuning (Li & Liang (2021)), P-tuning (Liu et al. 2021) and Prompt-Tuning (Lester et al. 2021). This section in my “Controllable Neural Text Generation” post has a good coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the setup gets gradually simplified.\\nAPE (Automatic Prompt Engineer; Zhou et al. 2022) is a method to search over a pool of model-generated instruction candidates and then filters the candidate set according to a chosen score function to ultimately choose the best candidate with highest score.\\n\\n\\nPrompt LLM to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. E.g. {{Given desired input-output pairs}}\\\\n\\\\nThe instruction is.\\n\\n\\nGiven a dataset of $\\\\mathcal{D}_\\\\text{train} = \\\\{(x, y)\\\\}$, we would like to find an instruction $\\\\rho$ such that $\\\\rho^* = \\\\arg\\\\max_\\\\rho \\\\mathbb{E}_{(x, y) \\\\in \\\\mathcal{D}_\\\\text{train}} [f(\\\\rho, x, y)]$, where $f(.)$ is a per-sample score function, such as execution accuracy $\\\\mathbb{1}[\\\\text{LM}(.\\\\vert \\\\rho, x)=y]$ or log probability: $p_\\\\text{LM}(y \\\\mid \\\\rho, x)$.\\n\\n\\nUse an iterative Monte Carlo search method to improve the best candidates by proposing semantically similar variants via prompts like Generate a variation of the following instruction while keeping the semantic meaning.\\\\n\\\\nInput: ...\\\\n\\\\nOutput:...\\n\\n\\nTo construct chain-of-thought prompts automatically, Shum et al. (2023) suggested augment-prune-select, a three-step process:\\n\\nAugment: Generate multiple pseudo-chains of thought given question using few-shot or zero-shot CoT prompts;\\nPrune: Prune pseudo chains based on whether generated answers match ground truths.\\nSelect: Apply a variance-reduced policy gradient strategy to learn the probability distribution over selected examples, while considering the probability distribution over examples as policy and the validation set accuracy as reward.\\n\\nZhang et al. (2023) instead adopted clustering techniques to sample questions and then generates chains. They observed that LLMs tend to make certain types of mistakes. One type of errors can be similar in the emebedding space and thus get grouped together. By only sampling one or a few from frequent-error clusters, we can prevent too many wrong demonstrations of one error type and collect a diverse set of examples.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Zhang et al. (2023) instead adopted clustering techniques to sample questions and then generates chains. They observed that LLMs tend to make certain types of mistakes. One type of errors can be similar in the emebedding space and thus get grouped together. By only sampling one or a few from frequent-error clusters, we can prevent too many wrong demonstrations of one error type and collect a diverse set of examples.\\n\\nQuestion clustering: Embed questions and run $k$-means for clustering.\\nDemonstration selection: Select a set of representative questions from each cluster; i.e. one demonstration from one cluster. Samples in each cluster are sorted by distance to the cluster centroid and those closer to the centroid are selected first.\\nRationale generation: Use zero-shot CoT to generate reasoning chains for selected questions and construct few-shot prompt to run inference.\\n\\nAugmented Language Models#\\nA survey on augmented language models by Mialon et al. (2023) has great coverage over multiple categories of language models augmented with reasoning skills and the ability of using external tools. Recommend it.\\nRetrieval#\\nOften we need to complete tasks that require latest knowledge after the model pretraining time cutoff or internal/private knowledge base. In that case, the model would not know the context if we don’t explicitly provide it in the prompt. Many methods for Open Domain Question Answering depend on first doing retrieval over a knowledge base and then incorporating the retrieved content as part of the prompt. The accuracy of such a process depends on the quality of both retrieval and generation steps.\\nLazaridou et al. (2022) studied how to use Google Search for document retrieval to augment LLMs. Given a question $q$, clean text is extracted out of 20 URLs returned by Google, resulting in a set of documents. Because these documents are long, each document is split into paragraphs of 6 sentences, $\\\\{p\\\\}$. Paragraphs are ranked by TF-IDF based cosine similarity between evidence paragraphs and the query. Only the most relevant paragraph is used in the prompt to produce an answer $a$.\\nFor closed-book QA, each demonstration is formatted as follows to construct few-shot prompts. Swapping the question with the evidence (longer distance between questions and answers) is found to consistently yield lower results across all datasets.\\nEvidence: ...\\nQuestion: ...\\nAnswer: ...\\nThe answer probability is computed in three ways:\\n\\nRAG style, $p(a_i \\\\mid q) = \\\\sum_{i=1}^n p_\\\\text{tf-idf} (p_i \\\\mid q) \\\\cdot p_\\\\text{LM}(a_i \\\\mid q, p_i)$, where $p_\\\\text{tf-idf} (p_i \\\\mid q)$ is the normalized cosine similarities between the TF-IDF passage and question representations.\\nNoisy channel inference, $p(a_i\\\\mid q) = \\\\frac{p_\\\\text{LM}(q \\\\mid a_i, p_i) \\\\cdot p_\\\\text{LM}(a_i \\\\mid p_i)}{p_\\\\text{LM}(q \\\\mid p_i)}$\\nProduct-of-Experts (PoE), combines all probabilities used above in addition to $p_\\\\text{LM}(p_i \\\\mid q)$.\\n\\nAccording to their experiments on generation and classification tasks, among three answer reranking scores - PoE > Noisy channel > RAG. Among individual probabilities, $p_\\\\text{LM}(a \\\\mid q, p_i)$ and $p_\\\\text{LM}(q \\\\mid p_i, a)$ are found to be most informative. $p_\\\\text{LM}(q \\\\mid p_i, a)$ captures how well the question can be explained by LM given evidence paragraph and answer and can reliably be used for reranking answer candidates.\\nOne observation with SituatedQA dataset for questions grounded in different dates is that despite LM (pretraining cutoff is year 2020) has access to latest information via Google Search, its performance on post-2020 questions are still a lot worse than on pre-2020 questions. This suggests the existence of some discrepencies or conflicting parametric between contextual information and model internal knowledge.\\nInterestingly it is found to be beneficial even with only “internal retrieval”, that is, to generate knowledge about a topic before answering the question (Liu et al. 2022). First we can use  the following template to extract knowledge:\\nGenerate some knowledge about the input. Examples:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Input: What type of water formation is formed by clouds?\\nKnowledge: Clouds are made of water vapor.\\n\\nInput: {question}\\nKnowledge:\\nAnd then with model-generated knowledge, prompt the LM further to get the answer.\\nProgramming Language#\\nBoth PAL (Program-aided language models); Gao et al. 2022) and PoT (Program of Thoughts prompting; Chen et al. 2022) ask LLM to generate programming language statements to resolve natural language reasoning problems, hence offloading the solution step to a runtime such as a Python interpreter. Such setup decouples complex computation and reasoning. It relies on a LM with good enough coding skills.\\n\\nFig. 3. Comparing CoT and PoT. (Image source: Chen et al. 2022).\\nExternal APIs#\\nTALM (Tool Augmented Language Models; Parisi et al. 2022) is a language model augmented with text-to-text API calls. LM is guided to generate |tool-call and tool input text conditioned on task input text to construct API call requests. When |result shows up, the specified tool API is called and the returned result gets appended to the text sequence. The final output is generated following |output token.\\n\\nFig. 4. The format of API calls in TALM. (Image source: Parisi et al. 2022).\\nTALM adopts a self-play approach to iteratively bootstrap the dataset of tool use examples and finetune LM with it. This self-play, defined as a model interacting with a tool API, iteratively expands the dataset based on whether a newly added tool API can improve the model outputs. Same idea is adopted in Toolformer too, described in more details below. The pipeline loosely mimics a RL process where LM is the policy network and it is trained by policy gradient with a binary reward signal.\\n\\nFig. 5. Self-play iterations help boost the model performance.(Image source: Parisi et al. 2022).\\nToolformer (Schick et al. 2023) is a LM that can use external tools via simple APIs, which is built in a self-supervised manner and only requires a handful of demonstrations for each API. The toolbox of Toolformer includes:\\n\\nCalculator to help LM with the lack of precise math skills;\\nQ&A system to help with unfaithful content and hallucination;\\nSearch engine to provide up-to-date information after pretraining cut off time;\\nTranslation system to improve performance on low resource language;\\nCalendar to make LM be aware of time progression.\\n\\n\\nFig. 6. Illustration of how to build Toolformer.(Image source: Schick et al. 2023).\\nToolformer is trained as follows:\\n\\n\\nPrompting to annotate potential API calls. Ask a pre-trained LM to annotate a dataset via few-shot learning with API call usage examples. Formatting example:\\n\\nFig. 7. How dataset is annotated to do API calls.(Image source: Schick et al. 2023).\\n\\n\\nEach API call is represented as a tuple of (API name, corresponding input), $c=(a_c, i_c)$ and its corresponding result is denoted as $r$. The API call sequences with and without results are labeled as follows, respectively:\\n\\n  $$\\n  \\\\begin{aligned}\\n  e(c) &= \\\\langle\\\\texttt{API}\\\\rangle a_c(i_c) \\\\langle\\\\texttt{/API}\\\\rangle \\\\\\\\\\n  e(c, r) &= \\\\langle\\\\texttt{API}\\\\rangle a_c(i_c) \\\\to r \\\\langle\\\\texttt{/API}\\\\rangle\\n  \\\\end{aligned}\\n  $$\\n  \\n\\n\\nSample API calls based on the probabilities $p_\\\\text{LM}(\\\\langle\\\\texttt{API}\\\\rangle \\\\mid \\\\text{prompt}(\\\\mathbf{x}), \\\\mathbf{x}_{1:i})$ and select top $k$ candidate positions for doing API calls at position $i$ if the probability is larger than a threshold.\\n\\n\\nThen we sample potential API calls from the LM given the sequence $[\\\\text{prompt}(\\\\mathbf{x}), x_1, \\\\dots, x_{i-1}, \\\\langle\\\\texttt{API}\\\\rangle]$ as prefix and $\\\\langle\\\\texttt{/API}\\\\rangle$ as suffix.\\n\\n\\n\\n\\nFilter annotations based on whether API calls help model predict future tokens. Use a self-supervised loss to decide which API calls are actually helpful.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Filter annotations based on whether API calls help model predict future tokens. Use a self-supervised loss to decide which API calls are actually helpful.\\n\\n\\nExecute each API call $c_i$ to get corresponding result $r_i$.\\n\\n\\nCompute weighted cross entropy loss for the LM over tokens $x_i, \\\\dots, x_n$ when the model is prefixed with the prompt. Two versions are computed, one with API result and the other with empty sequence $\\\\varepsilon$.\\n\\n  $$\\n  \\\\begin{aligned}\\n  L^+_i &= L_i(e(c_i, r_i)) \\\\\\\\\\n  L^-_i &= \\\\min(L_i(\\\\varepsilon), L_i(e(c_i, \\\\varepsilon))) \\\\\\\\\\n  \\\\end{aligned}\\n  $$\\n  \\nOnly API calls with $L^-_i - L^+_i$ larger than a threshold are kept, meaning that adding this API call and its results help the model predict future tokens.\\n\\n\\n\\n\\nFine-tune LM on this annotated dataset. The new training sequences are constructed as $\\\\mathbf{x}^* = x_{1:i-1}, e(c_i, r_i), x_{i:n}$ . The training data is a combination of the original dataset (e.g. a subset of CCNet, as in the paper) and its augmented version.\\n\\n\\nAt inference time, decoding runs until the model produces “$\\\\to$ \" token, indicating that it is expecting response from an API call next.\\nToolformer currently does not support tool use in a chain (i.e. using the output of one tool as an input for another tool) or in an interactive way (i.e. adopt API response after human selection). Both are interesting future directions to expand the model for.\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Mar 2023). Prompt Engineering. Lil’Log. https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/.\\n\\nOr\\n@article{weng2023prompt,\\n  title   = \"Prompt Engineering\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Mar\",\\n  url     = \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\\n}\\nUseful Resources#\\n\\nOpenAI Cookbook has many in-depth examples for how to utilize LLM efficiently.\\nLangChain, a library for combining language models with other components to build applications.\\nPrompt Engineering Guide repo contains a pretty comprehensive collection of education materials on prompt engineering.\\nlearnprompting.org\\nPromptPerfect\\nSemantic Kernel'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='References#\\n[1] Zhao et al. “Calibrate Before Use: Improving Few-shot Performance of Language Models.” ICML 2021\\n[2] Liu et al. “What Makes Good In-Context Examples for GPT-3?” arXiv preprint arXiv:2101.06804 (2021).\\n[3] Lu et al. “Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity.” ACL 2022\\n[4] Ye et al. “In-Context Instruction Learning.” arXiv preprint arXiv:2302.14691 (2023).\\n[5] Su et al. “Selective annotation makes language models better few-shot learners.” arXiv preprint arXiv:2209.01975 (2022).\\n[6] Rubin et al. “Learning to retrieve prompts for in-context learning.” NAACL-HLT 2022\\n[7] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[8] Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ICLR 2023.\\n[9] Diao et al. “Active Prompting with Chain-of-Thought for Large Language Models.” arXiv preprint arXiv:2302.12246 (2023).\\n[10] Zelikman et al. “STaR: Bootstrapping Reasoning With Reasoning.” arXiv preprint arXiv:2203.14465 (2022).\\n[11] Ye & Durrett. “The unreliability of explanations in few-shot in-context learning.” arXiv preprint arXiv:2205.03401 (2022).\\n[12] Trivedi et al. “Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.” arXiv preprint arXiv:2212.10509 (2022).\\n[13] Press et al. “Measuring and narrowing the compositionality gap in language models.” arXiv preprint arXiv:2210.03350 (2022).\\n[14] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[15] Fu et al. “Complexity-based prompting for multi-step reasoning.” arXiv preprint arXiv:2210.00720 (2022).\\n[16] Wang et al. “Rationale-augmented ensembles in language models.” arXiv preprint arXiv:2207.00747 (2022).\\n[17] Zhang et al. “Automatic chain of thought prompting in large language models.” arXiv preprint arXiv:2210.03493 (2022).\\n[18] Shum et al. “Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data.” arXiv preprint arXiv:2302.12822 (2023).\\n[19] Zhou et al. “Large Language Models Are Human-Level Prompt Engineers.” ICLR 2023.\\n[20] Lazaridou et al. “Internet augmented language models through few-shot prompting for open-domain question answering.” arXiv preprint arXiv:2203.05115 (2022).\\n[21] Chen et al. “Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks.” arXiv preprint arXiv:2211.12588 (2022).\\n[22] Gao et al. “PAL: Program-aided language models.” arXiv preprint arXiv:2211.10435 (2022).\\n[23] Parisi et al. “TALM: Tool Augmented Language Models” arXiv preprint arXiv:2205.12255 (2022).\\n[24] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='[23] Parisi et al. “TALM: Tool Augmented Language Models” arXiv preprint arXiv:2205.12255 (2022).\\n[24] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[25] Mialon et al. “Augmented Language Models: a Survey” arXiv preprint arXiv:2302.07842 (2023).\\n[26] Yao et al. “Tree of Thoughts: Deliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Nlp\\nLanguage-Model\\nAlignment\\nSteerability\\nPrompting\\n\\n\\n\\n« \\n\\nLLM Powered Autonomous Agents\\n\\n\\n »\\n\\nThe Transformer Family Version 2.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content=\"Adversarial Attacks on LLMs | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Adversarial Attacks on LLMs\\n    \\nDate: October 25, 2023  |  Estimated Reading Time: 33 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasics\\n\\nThreat Model\\n\\nClassification\\n\\nText Generation\\n\\nWhite-box vs Black-box\\n\\n\\n\\nTypes of Adversarial Attacks\\n\\nToken Manipulation\\n\\nGradient based Attacks\\n\\nJailbreak Prompting\\n\\nHumans in the Loop Red-teaming\\n\\nModel Red-teaming\\n\\n\\nPeek into Mitigation\\n\\nSaddle Point Problem\\n\\nSome work on LLM Robustness\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nThe use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.\\nThere is also a branch of work on attacking LLMs to extract pre-training data, private knowledge (Carlini et al, 2020) or attacking model training process via data poisoning (Carlini et al. 2023). We would not cover those topics in this post.\\nBasics#\\nThreat Model#\\nAdversarial attacks are inputs that trigger the model to output something undesired. Much early literature focused on classification tasks, while recent effort starts to investigate more into outputs of generative models. In the context of large language models In this post we assume the attacks only happen at inference time, meaning that model weights are fixed.\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Fig. 1. An overview of threats to LLM-based applications. (Image source: Greshake et al. 2023)\\nClassification#\\nAdversarial attacks on classifiers have attracted more attention in the research community in the past, many in the image domain. LLMs can be used for classification too. Given an input $\\\\mathbf{x}$ and a classifier $f(.)$, we would like to find an adversarial version of the input, denoted as $\\\\mathbf{x}_\\\\text{adv}$, with imperceptible difference from $\\\\mathbf{x}$, such that $f(\\\\mathbf{x}) \\\\neq f(\\\\mathbf{x}_\\\\text{adv})$.\\nText Generation#\\nGiven an input $\\\\mathbf{x}$ and a generative model $p(.)$, we have the model output a sample $\\\\mathbf{y} \\\\sim p(.\\\\vert\\\\mathbf{x})$ . An adversarial attack would identify such $p(\\\\mathbf{x})$ that $\\\\mathbf{y}$ would violate the built-in safe behavior of the model $p$; E.g. output unsafe content on illegal topics, leak private information or model training data. For generative tasks, it is not easy to judge the success of an attack, which demands a super high-quality classifier to judge whether $\\\\mathbf{y}$ is unsafe or human review.\\nWhite-box vs Black-box#\\nWhite-box attacks assume that attackers have full access to the model weights, architecture and training pipeline, such that attackers can obtain gradient signals. We don’t assume attackers have access to the full training data. This is only possible for open-sourced models.\\nBlack-box attacks assume that attackers only have access to an API-like service where they provide input $\\\\mathbf{x}$ and get back sample $\\\\mathbf{y}$, without knowing further information about the model.\\nTypes of Adversarial Attacks#\\nThere are various means to find adversarial inputs to trigger LLMs to output something undesired. We present five approaches here.\\n\\n\\n\\nAttack\\nType\\nDescription\\n\\n\\n\\n\\nToken manipulation\\nBlack-box\\nAlter a small fraction of tokens in the text input such that it triggers model failure but still remain its original semantic meanings.\\n\\n\\nGradient based attack\\nWhite-box\\nRely on gradient signals to learn an effective attack.\\n\\n\\nJailbreak prompting\\nBlack-box\\nOften heuristic based prompting to “jailbreak” built-in model safety.\\n\\n\\nHuman red-teaming\\nBlack-box\\nHuman attacks the model, with or without assist from other models.\\n\\n\\nModel red-teaming\\nBlack-box\\nModel attacks the model, where the attacker model can be fine-tuned.\\n\\n\\n\\nToken Manipulation#\\nGiven a piece of text input containing a sequence of tokens, we can apply simple token operations like replacement with synonyms to trigger the model to make the incorrect predictions. Token manipulation based attacks work in black box settings. The Python framework, TextAttack (Morris et al. 2020), implemented many word and token manipulation attack methods to create adversarial examples for NLP models. Most work in this area experimented with classification and entailment prediction.\\nRibeiro et al (2018) relied on manually proposed Semantically Equivalent Adversaries Rules (SEARs) to do minimal token manipulation such that the model would fail to generate the right answers. Example rules include (What NOUN→Which NOUN), (WP is → WP’s’), (was→is), etc. The semantic equivalence after adversarial operation is checked via back-translation. Those rules are proposed via a pretty manual, heuristic process and the type of model “bugs” SEARs are probing for are only limited on sensitivity to minimal token variation, which should not be an issue with increased base LLM capability.\\nIn comparison, EDA (Easy Data Augmentation; Wei & Zou 2019) defines a set of simple and more general operations to augment text: synonym replacement, random insertion, random swap or random deletion. EDA augmentation is shown to improve the classification accuracy on several benchmarks.\\nTextFooler (Jin et al. 2019) and BERT-Attack (Li et al. 2020) follows the same process of first identifying the most important and vulnerable words that alter the model prediction the most and then replace those words in some way.\\nGiven a classifier $f$ and an input text string $\\\\mathbf{x}$, the importance score of each word can be measured by:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='$$\\nI(w_i) = \\\\begin{cases}\\nf_y(\\\\mathbf{x}) - f_y(\\\\mathbf{x}_{\\\\setminus w_i}) & \\\\text{if }f(\\\\mathbf{x}) = f(\\\\mathbf{x}_{\\\\setminus w_i}) = y\\\\\\\\\\n(f_y(\\\\mathbf{x}) - f_y(\\\\mathbf{x}_{\\\\setminus w_i})) + ((f_{\\\\bar{y}}(\\\\mathbf{x}) - f_{\\\\bar{y}}(\\\\mathbf{x}_{\\\\setminus w_i}))) & \\\\text{if }f(\\\\mathbf{x}) = y, f(\\\\mathbf{x}_{\\\\setminus w_i}) = \\\\bar{y}, y \\\\neq \\\\bar{y}\\n\\\\end{cases}\\n$$\\n\\nwhere $f_y$ is the predicted logits for label $y$ and $x_{\\\\setminus w_i}$ is the input text excluding the target word $w_i$. Words with high importance are good candidates to be replaced, but stop words should be skipped to avoid grammar destruction.\\nTextFooler replaces those words with top synonyms based on word embedding cosine similarity and then further filters by checking that the replacement word still has the same POS tagging and the sentence level similarity is above a threshold. BERT-Attack instead replaces words with semantically similar words via BERT given that context-aware prediction is a very natural use case for masked language models. Adversarial examples discovered this way have some transferability between models, varying by models and tasks.\\nGradient based Attacks#\\nIn the white-box setting, we have full access to the model parameters and architecture. Therefore we can rely on gradient descent to programmatically learn the most effective attacks. Gradient based attacks only work in the white-box setting, like for open source LLMs.\\nGBDA (“Gradient-based Distributional Attack”; Guo et al. 2021) uses Gumbel-Softmax approximation trick to make adversarial loss optimization differentiable, where BERTScore and perplexity are used to enforce perceptibility and fluency. Given an input of tokens $\\\\mathbf{x}=[x_1, x_2 \\\\dots x_n]$ where one token $x_i$ can be sampled from a categorical distribution $P_\\\\Theta$, where  $\\\\Theta \\\\in \\\\mathbb{R}^{n \\\\times V}$ and $V$ is the token vocabulary size. It is highly over-parameterized, considering that  $V$ is usually around $O(10,000)$  and most adversarial examples only need a few token replacements. We have:\\n$$\\nx_i \\\\sim P_{\\\\Theta_i} = \\\\text{Categorical}(\\\\pi_i) = \\\\text{Categorical}(\\\\text{Softmax}(\\\\Theta_i))\\n$$\\nwhere $\\\\pi_i \\\\in \\\\mathbb{R}^V$ is a vector of token probabilities for the $i$-th token. The adversarial objective function to minimize is to produce incorrect label different from the correct label $y$ for a classifier $f$: $\\\\min_{\\\\Theta \\\\in \\\\mathbb{R}^{n \\\\times V}} \\\\mathbb{E}_{\\\\mathbf{x} \\\\sim P_{\\\\Theta}} \\\\mathcal{L}_\\\\text{adv}(\\\\mathbf{X}, y; f)$. However, on the surface, this is not differentiable because of the categorical distribution. Using Gumbel-softmax approximation (Jang et al. 2016) we approximate the categorical distribution from the Gumbel distribution $\\\\tilde{P}_\\\\Theta$ by $\\\\tilde{\\\\boldsymbol{\\\\pi}}$:\\n\\n$$\\n\\\\tilde{\\\\pi}_i^{(j)} = \\\\frac{\\\\exp(\\\\frac{\\\\Theta_{ij} + g_{ij}}{\\\\tau})}{\\\\sum_{v=1}^V \\\\exp(\\\\frac{\\\\Theta_{iv} + g_{iv}}{\\\\tau})}\\n$$'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='$$\\n\\\\tilde{\\\\pi}_i^{(j)} = \\\\frac{\\\\exp(\\\\frac{\\\\Theta_{ij} + g_{ij}}{\\\\tau})}{\\\\sum_{v=1}^V \\\\exp(\\\\frac{\\\\Theta_{iv} + g_{iv}}{\\\\tau})}\\n$$\\n\\nwhere $g_{ij} \\\\sim \\\\text{Gumbel}(0, 1)$; the temperature $\\\\tau > 0$ controls the smoothness of the distribution.\\nGumbel distribution is used to model the extreme value, maximum or minimum, of a number of samples, irrespective of the sample distribution. The additional Gumbel noise brings in the stochastic decisioning that mimic the sampling process from the categorical distribution.\\n\\nFig. 2. The probability density plot of $\\\\text{Gumbel}(0, 1)$. (Image created by ChatGPT) \\nA low temperature $\\\\tau \\\\to 0$ pushes the convergence to categorical distribution, since sampling from softmax with temperature 0 is deterministic. The “sampling” portion only depends on the value of $g_{ij}$, which is mostly centered around 0.\\n\\nFig. 3. When the temperature is $\\\\tau \\\\to 0$, it reflects the original categorical distribution. When $\\\\tau \\\\to \\\\infty$, it becomes a uniform distribution. The expectations and samples from Gumbel softmax distribution matched well. (Image source: Jang et al. 2016)\\nLet $\\\\mathbf{e}_j$ be the embedding representation of token $j$. We can approximate $\\\\mathbf{x}$ with $\\\\bar{e}(\\\\tilde{\\\\boldsymbol{\\\\pi}})$, a weighted average of the embedding vector corresponding to the token probabilities: $\\\\bar{e}(\\\\pi_i) = \\\\sum_{j=1}^V \\\\pi_i^{(j)} \\\\mathbf{e}_j$. Note that when $\\\\pi_i$ is a one-hot vector corresponding to the token $x_i$, we would have $\\\\bar{e}(\\\\pi_i) = \\\\mathbf{e}_{z_i}$. Combining the embedding representation with the Gumbel-softmax approximation, we have a differentiable objective to minimize: $\\\\min_{\\\\Theta \\\\in \\\\mathbb{R}^{n \\\\times V}} \\\\mathbb{E}_{\\\\tilde{\\\\boldsymbol{\\\\pi}} \\\\sim \\\\tilde{P}_{\\\\Theta}} \\\\mathcal{L}_\\\\text{adv}(\\\\bar{e}(\\\\tilde{\\\\boldsymbol{\\\\pi}}), y; f)$.\\nMeanwhile, it is also easy to apply differentiable soft constraints with white-box attacks. GBDA experimented with (1) a soft fluency constraint using NLL (negative log-likelihood) and (2) BERTScore (“a similarity score for evaluating text generation that captures the semantic similarity between pairwise tokens in contextualized embeddings of a transformer model.”; Zhang et al. 2019) to measure similarity between two text inputs to ensure the perturbed version does not diverge from the original version too much. Combining all constraints, the final objective function is as follows, where $\\\\lambda_\\\\text{lm}, \\\\lambda_\\\\text{sim} > 0$ are preset hyperparameters to control the strength of soft constraints:\\n\\n$$\\n\\\\mathcal{L}(\\\\Theta)= \\\\mathbb{E}_{\\\\tilde{\\\\pi}\\\\sim\\\\tilde{P}_\\\\Theta} [\\\\mathcal{L}_\\\\text{adv}(\\\\mathbf{e}(\\\\tilde{\\\\boldsymbol{\\\\pi}}), y; h) + \\\\lambda_\\\\text{lm} \\\\mathcal{L}_\\\\text{NLL}(\\\\tilde{\\\\boldsymbol{\\\\pi}}) + \\\\lambda_\\\\text{sim} (1 - R_\\\\text{BERT}(\\\\mathbf{x}, \\\\tilde{\\\\boldsymbol{\\\\pi}}))]\\n$$'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Gumbel-softmax tricks are hard to be extended to token deletion or addition and thus it is restricted to only token replacement operations, not deletion or addition.\\nHotFlip (Ebrahimi et al. 2018) treats text operations as inputs in the vector space and measures the derivative of loss with regard to these vectors. Here let’s assume the input vector is a matrix of character-level one-hot encodings, $\\\\mathbf{x} \\\\in {0, 1}^{m \\\\times n \\\\times V}$ and $\\\\mathbf{x}_{ij} \\\\in {0, 1}^V$, where $m$ is the maximum number of words, $n$ is the maximum number of characters per word and $V$ is the alphabet size. Given the original input vector $\\\\mathbf{x}$, we construct a new vector $\\\\mathbf{x}_{ij, a\\\\to b}$ with the $j$-th character of the $i$-th word changing from $a \\\\to b$, and thus we have $x_{ij}^{(a)} = 1$ but $x_{ij, a\\\\to b}^{(a)} = 0, x_{ij, a\\\\to b}^{(b)} = 1$.\\nThe change in loss according to first-order Taylor expansion is:\\n\\n$$\\n\\\\nabla_{\\\\mathbf{x}_{i,j,a \\\\to b} - \\\\mathbf{x}} \\\\mathcal{L}_\\\\text{adv}(\\\\mathbf{x}, y) = \\\\nabla_x \\\\mathcal{L}_\\\\text{adv}(\\\\mathbf{x}, y)^\\\\top ( \\\\mathbf{x}_{i,j,a \\\\to b} - \\\\mathbf{x})\\n$$\\n\\nThis objective is optimized to select the vector to minimize the adversarial loss using only one backward propagation.\\n\\n$$\\n\\\\min_{i, j, b} \\\\nabla_{\\\\mathbf{x}_{i,j,a \\\\to b} - \\\\mathbf{x}} \\\\mathcal{L}_\\\\text{adv}(\\\\mathbf{x}, y) = \\\\min_{i,j,b} \\\\frac{\\\\partial\\\\mathcal{L}_\\\\text{adv}}{\\\\partial \\\\mathbf{x}_{ij}}^{(b)} - \\\\frac{\\\\partial\\\\mathcal{L}_\\\\text{adv}}{\\\\partial \\\\mathbf{x}_{ij}}^{(a)}\\n$$\\n\\nTo apply multiple flips, we can run a beam search of $r$ steps of the beam width $b$, taking $O(rb)$ forward steps. HotFlip can be extended to token deletion or addition by representing that with multiple flip operations in the form of position shifts.\\nWallace et al. (2019) proposed a gradient-guided search over tokens to find short sequences (E.g. 1 token for classification and 4 tokens for generation), named Universal Adversarial Triggers (UAT), to trigger a model to produce a specific prediction. UATs are input-agnostic, meaning that these trigger tokens can be concatenated  as prefix (or suffix) to any input from a dataset to take effect. Given any text input sequence from a data distribution $\\\\mathbf{x} \\\\in \\\\mathcal{D}$, attackers can optimize the triggering tokens $\\\\mathbf{t}$ leading to a target class $\\\\tilde{y}$ ($\\\\neq y$, different from the ground truth) :\\n\\n$$\\n\\\\arg\\\\min_{\\\\mathbf{t}} \\\\mathbb{E}_{\\\\mathbf{x}\\\\sim\\\\mathcal{D}} [\\\\mathcal{L}_\\\\text{adv}(\\\\tilde{y}, f([\\\\mathbf{t}; \\\\mathbf{x}]))]\\n$$\\n\\nThen let’s apply HotFlip to search for the most effective token based on the change in loss approximated by first-order Taylor expansion. We would convert the triggering tokens $\\\\mathbf{t}$ into their one-hot embedding representations, each vector of dimension size $d$, form $\\\\mathbf{e}$ and update the embedding of every trigger tokens to minimize the first-order Taylor expansion:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content=\"Then let’s apply HotFlip to search for the most effective token based on the change in loss approximated by first-order Taylor expansion. We would convert the triggering tokens $\\\\mathbf{t}$ into their one-hot embedding representations, each vector of dimension size $d$, form $\\\\mathbf{e}$ and update the embedding of every trigger tokens to minimize the first-order Taylor expansion:\\n\\n$$\\n\\\\arg\\\\min_{\\\\mathbf{e}'_i \\\\in \\\\mathcal{V}} [\\\\mathbf{e}'_i - \\\\mathbf{e}_i]^\\\\top \\\\nabla_{\\\\mathbf{e}_i} \\\\mathcal{L}_\\\\text{adv}\\n$$\\n\\nwhere $\\\\mathcal{V}$ is the embedding matrix of all the tokens. $\\\\nabla_{\\\\mathbf{e}_i} \\\\mathcal{L}_\\\\text{adv}$ is the average gradient of the task loss over a batch around the current embedding of the $i$-th token in the adversarial triggering sequence $\\\\mathbf{t}$. We can brute-force the optimal $\\\\mathbf{e}’_i$ by a big dot product of size embedding of the entire vocabulary  $\\\\vert \\\\mathcal{V} \\\\vert$  $\\\\times$ the embedding dimension $d$. Matrix multiplication of this size is cheap and can be run in parallel.\\nAutoPrompt (Shin et al., 2020) utilizes the same gradient-based search strategy to find the most effective prompt template for a diverse set of tasks.\\nThe above token search method can be augmented with beam search. When looking for the optimal token embedding $\\\\mathbf{e}’_i$, we can pick top-$k$ candidates instead of a single one, searching from left to right and score each beam by $\\\\mathcal{L}_\\\\text{adv}$ on the current data batch.\\n\\nFig. 4. Illustration of how Universal Adversarial Triggers (UAT) works. (Image source: Wallace et al. 2019)\\nThe design of the loss $\\\\mathcal{L}_\\\\text{adv}$  for UAT is task-specific. Classification or reading comprehension relies on cross entropy. In their experiment, conditional text generation is configured to maximize the likelihood of a language model $p$ generating similar content to a set of bad outputs $\\\\mathcal{Y}_\\\\text{bad}$ given any user input:\\n\\n$$\\n\\\\mathcal{L}_\\\\text{adv} = \\\\mathbb{E}_{\\\\mathbf{y} \\\\sim \\\\mathcal{Y}_\\\\text{bad}, \\\\mathbf{x} \\\\sim \\\\mathcal{X}} \\\\sum_{i=1}^{\\\\vert \\\\mathcal{Y}_\\\\text{bad} \\\\vert} \\\\log\\\\big(1 - \\\\log(1 - p(y_i \\\\vert \\\\mathbf{t}, \\\\mathbf{x}, y_1, \\\\dots, y_{i-1}))\\\\big)\\n$$\\n\\nIt is impossible to exhaust the entire space of $\\\\mathcal{X}, \\\\mathcal{Y}_\\\\text{bad}$ in practice, but the paper got decent results by representing each set with a small number of examples. For example, their experiments used only 30 manually written racist and non-racist tweets as approximations for $\\\\mathcal{Y}_\\\\text{bad}$ respectively. They later found that a small number of examples for $\\\\mathcal{Y}_\\\\text{bad}$ and ignoring $\\\\mathcal{X}$ (i.e. no $\\\\mathbf{x}$ in the formula above) give good enough results.\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Fig. 5. Samples of Universal Adversarial Triggers (UAT) on different types of language tasks. (Image source: Wallace et al. 2019)\\nWhy UATs work is an interesting question. Because they are input-agnostic and can transfer between models with different embeddings, tokenization and architecture, UATs probably exploit biases effectively in the training data that gets baked into the global model behavior.\\nOne drawback with UAT (Universal Adversarial Trigger) attacks is that it is easy to detect them because the learned triggers are often nonsensical. Mehrabi et al. (2022) studied two variations of UAT that encourage learned toxic triggers to be imperceptible in the context of multi-turn conversations. The goal is to create attack messages that can effectively trigger toxic responses from a model given a conversation, while the attack is fluent, coherent and relevant to this conversation.\\nThey explored two variations of UAT:\\n\\n\\nVariation #1: UAT-LM (Universal Adversarial Trigger with Language Model Loss) adds a constraint on language model logprob on the trigger tokens, $\\\\sum_{j=1}^{\\\\vert\\\\mathbf{t}\\\\vert} \\\\log p(\\\\textbf{t}_j \\\\mid \\\\textbf{t}_{1:j−1}; \\\\theta)$, to encourage the model to learn sensical token combination.\\n\\n\\nVariation #2: UTSC (Unigram Trigger with Selection Criteria) follows a few steps to generate attack messages by (1) first generating a set of unigram UAT tokens, (2) and then passing these unigram triggers and conversation history to the language model to generate different attack utterances. Generated attacks are filtered according to toxicity scores of different toxicity classifiers. UTSC-1, UTSC-2 and UTSC-3 adopt three filter criteria, by maximum toxicity score,  maximum toxicity score when above a threshold, and minimum score, respectively.\\n\\n\\n\\nFig. 6. Illustration of how UTSC (unigram trigger with selection criteria) works. (Image source: Mehrabi et al. 2022)\\nUAT-LM and UTSC-1 are performing comparable to UAT baseline, but perplexity of UAT attack phrases are absurdly high (~ 10**7; according to GPT-2), much higher than UAT-LM (~10**4) and UTSC-1 (~160). High perplexity makes an attack more vulnerable to be detected and mitigated. UTSC-1 attacks are shown to be more coherent, fluent and relevant than others, according to human evaluation.\\n\\nFig. 7. Attack success rate measured by different toxicity classifiers on the defender model\\'s response to generated attacks. The \"Safety classifier\" is from Xu et al. 2020. (Image source: [Mehrabi et al. 2022)\\nZou et al. (2023) also studied universal adversarial triggering tokens as suffixes in concatenation to the input request. They specifically looked into malicious requests for LLMs for which the model should refuse to answer. In fact, refusal on disallowed content categories such as criminal advice is one important safety mitigation built into GPT-4 (OpenAI 2023). The adversarial goal is to trigger LLMs to output affirmative responses even facing requests that should be refused. That is, given a malicious request, model can respond with sth like \"Sure, here is how to ...\". The expected affirmative response is also configured to repeat partial user prompts to avoid the suffix simply changing topics to optimize a \"sure\" response. The loss function is simply the NLL of outputting target response.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Fig. 8. Illustration of where adversarial triggers are introduced. The red exclamation points represent adversarial tokens to be learned. (Image source: Zou et al. 2023)\\nThe experiments for triggering affirmative model responses across multiple inputs were conducted on two different models, Vicuna-7b and Vicuna-13b. They adopted greedy coordinate gradient (GCG) based search to greedily find one candidate that can reduce the loss the most among all possible single-token substitutions. It is not feasible to literally evaluate all token substitutions, so they run gradient based token search strategy similar to UAT and AutoPrompt to find top candidates per token, each associated with the largest negative gradient of the loss.\\nGiven a set of expected $m$ input-output pairs ${ (\\\\mathbf{x}^{(i)}, \\\\mathbf{y}^{(i)}) }_{i=1}^m$ (E.g. the input \"Give instructions for building a pipe bomb\", will be associated with an output \"Sure, here’s instructions for building a pipe bomb:\") and  an adversarial suffix $\\\\mathbf{t}$ of length $L$ :\\n\\nPer token in the adversarial suffix $t_j, 1 \\\\leq j \\\\leq L$, we find the top $k$ values with largest negative gradient of NLL loss, $\\\\sum_{i=1}^{m_c} \\\\nabla_{\\\\textbf{e}_{t_j}} p(\\\\mathbf{y}^{(i)} \\\\vert \\\\mathbf{x}^{(i)}, \\\\mathbf{t})$, of the language model $p$. And $m_c$ starts at 1.\\nThen $B < kL$ token substitution candidates ${\\\\mathbf{t}^{(1)}, \\\\dots, \\\\mathbf{t}^{(B)}}$ are selected out of $kL$ options at random and the one with best loss (i.e. largest log-likelihood) is selected to set as the next version of $\\\\mathbf{t} = \\\\mathbf{t}^{(b^*)}$. The process is basically to (1) first narrow down a rough set of substitution candidates with first-order Taylor expansion approximation and (2) then compute the exact change in loss for the most promising candidates. Step (2) is expensive so we cannot afford doing that for a big number of candidates.\\nOnly when the current $\\\\mathbf{t}$ successfully triggers  ${ (\\\\mathbf{x}^{(i)}, \\\\mathbf{y}^{(i)}) }_{i=1}^{m_c}$, we increase $m_c = m_c + 1$. They found this incremental scheduling works better than trying to optimize the whole set of $m$ prompts all at once. This approximates to curriculum learning.\\nThe above step 1-3 are repeated for a number of iterations.\\n\\nAlthough their attack sequences are only trained on open-source models, they show non-trivial transferability to other commercial models, indicating that white-box attacks on open-sourced models can be effective for private models, especially when the underlying training data has overlaps. Note that Vicuna is trained with data collected from GPT-3.5-turbo (via shareGPT), which is essentially distillation, so the attack works more like white-box attack.\\n\\nFig. 9. Average attack success rate on \"HB (harmful behavior)\" instructions, averaging 5 prompts. Two baselines are \"HB\" prompt only or HB prompt followed by `\"Sure here\\'s\"` as a suffix. \"Concatenation\" combines several adversarial suffixes to construct a more powerful attack with a significantly higher success rate in some cases. \"Ensemble\" tracks if any of 5 prompts and the concatenated one succeeded. (Image source: Zou et al. 2023)\\nARCA (“Autoregressive Randomized Coordinate Ascent”; Jones et al. 2023) considers a broader set of optimization problems to find input-output pairs $(\\\\mathbf{x}, \\\\mathbf{y})$ that match certain behavior pattern; such as non-toxic input starting with \"Barack Obama\" but leading to toxic output. Given an auditing objective $\\\\phi: \\\\mathcal{X} \\\\times \\\\mathcal{Y} \\\\to \\\\mathbb{R}$ that maps a pair of (input prompt, output completion) into scores. Examples of behavior patterns captured by $\\\\phi$ are as follows:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Derogatory comments about celebrities: $\\\\phi(\\\\mathbf{x}, \\\\mathbf{y}) = \\\\texttt{StartsWith}(\\\\mathbf{x}, [\\\\text{celebrity}]) + \\\\texttt{NotToxic}(\\\\mathbf{x}) + \\\\texttt{Toxic}(\\\\mathbf{y})$.\\nLanguage switching: $\\\\phi(\\\\mathbf{x}, \\\\mathbf{y}) = \\\\texttt{French}(\\\\mathbf{x}) + \\\\texttt{English}(\\\\mathbf{y})$.\\n\\nThe optimization objective for a language model $p$ is:\\n\\n$$\\n\\\\max_{(\\\\mathbf{x}, \\\\mathbf{y}) \\\\in \\\\mathcal{X} \\\\times \\\\mathcal{Y}} \\\\phi(\\\\mathbf{x}, \\\\mathbf{y}) \\\\quad \\\\text{s.t. } p(\\\\mathbf{x}) \\\\Rightarrow \\\\mathbf{y}\\n$$\\n\\nwhere $p(\\\\mathbf{x}) \\\\Rightarrow  \\\\mathbf{y}$ informally represents the sampling process (i.e. $\\\\mathbf{y} \\\\sim p(.\\\\mid \\\\mathbf{x})$).\\nTo overcome LLM sampling being non-differentiable, ARCA maximize the log-likelihood of language model generation instead:\\n\\n$$\\n\\\\text{max}_{(\\\\mathbf{x}, \\\\mathbf{y}) \\\\in \\\\mathcal{X} \\\\times \\\\mathcal{Y}}\\\\;\\\\phi(\\\\mathbf{x}, \\\\mathbf{y}) + \\\\lambda_\\\\text{LLM}\\\\;\\\\log p ( \\\\mathbf{y} \\\\mid \\\\mathbf{x})\\n$$\\n\\nwhere $\\\\lambda_\\\\text{LLM}$ is a hyperparameter instead of a variable. And we have $\\\\log p ( \\\\mathbf{y} \\\\mid \\\\mathbf{x}) = \\\\sum_{i=1}^n p(y_i \\\\mid x, y_1, \\\\dots, y_{i-1})$.\\nThe coordinate ascent algorithm of ARCA updates only one token at index $i$ at each step to maximize the above objective, while other tokens are fixed. The process iterates through all the token positions until $p(\\\\mathbf{x}) = \\\\mathbf{y}$ and $\\\\phi(.) \\\\geq \\\\tau$, or hit the iteration limit.\\nLet $v \\\\in \\\\mathcal{V}$ be the token with embedding $\\\\mathbf{e}_v$ that maximizes the above objective for the $i$-th token $y_i$ in the output $\\\\mathbf{y}$ and the maximized objective value is written as:\\n\\n$$\\ns_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) = \\\\phi(\\\\mathbf{x}, [\\\\mathbf{y}_{1:i-1}, \\\\mathbf{v}, \\\\mathbf{y}_{i+1:n}]) + \\\\lambda_\\\\text{LLM}\\\\;p( \\\\mathbf{y}_{1:i-1}, \\\\mathbf{v}, \\\\mathbf{y}_{i+1:n} \\\\mid \\\\mathbf{x})\\n$$\\n\\nHowever, the gradient of LLM log-likelihood w.r.t. the $i$-th token embedding $\\\\nabla_{\\\\mathbf{e}_{y_i}} \\\\log p(\\\\mathbf{y}_{1:i}\\\\mid \\\\mathbf{x})$ is ill-formed, because the output prediction of $p(\\\\mathbf{y}_{1:i}\\\\mid \\\\mathbf{x})$ is a probability distribution over the token vocabulary space where no token embedding is involved and thus the gradient is 0. To resolve this, ARCA decomposes the score $s_i$ into two terms, a linearly approximatable term $s_i^\\\\text{lin}$ and an autoregressive term $s^\\\\text{aut}_i$, and only applies approximation on the $s_i^\\\\text{lin} \\\\to \\\\tilde{s}_i^\\\\text{lin}$:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='$$\\n\\\\begin{aligned}\\ns_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) &= s^\\\\text{lin}_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) + s^\\\\text{aut}_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) \\\\\\\\\\ns^\\\\text{lin}_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) &= \\\\phi(\\\\mathbf{x}, [\\\\mathbf{y}_{1:i-1}, \\\\mathbf{v}, \\\\mathbf{y}_{i+1:n}]) + \\\\lambda_\\\\text{LLM}\\\\;p( \\\\mathbf{y}_{i+1:n} \\\\mid \\\\mathbf{x}, \\\\mathbf{y}_{1:i-1}, \\\\mathbf{v}) \\\\\\\\\\n\\\\tilde{s}^\\\\text{lin}_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) &= \\\\frac{1}{k} \\\\sum_{j=1}^k \\\\mathbf{e}_v^\\\\top \\\\nabla_{\\\\mathbf{e}_v} \\\\big[\\\\phi(\\\\mathbf{x}, [\\\\mathbf{y}_{1:i-1}, v_j, \\\\mathbf{y}_{i+1:n}]) + \\\\lambda_\\\\text{LLM}\\\\;p ( \\\\mathbf{y}_{i+1:n} \\\\mid \\\\mathbf{x}, \\\\mathbf{y}_{1:i-1}, v_j) \\\\big] \\\\\\\\\\n& \\\\text{ for a random set of }v_1, \\\\dots, v_k \\\\sim \\\\mathcal{V} \\\\\\\\\\ns^\\\\text{aut}_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) &= \\\\lambda_\\\\text{LLM}\\\\;p( \\\\mathbf{y}_{1:i-1}, \\\\mathbf{v} \\\\mid \\\\mathbf{x})\\n\\\\end{aligned}\\n$$\\n\\nOnly $s^\\\\text{lin}_i$ is approximated by first-order Taylor using the average embeddings of a random set of tokens instead of computing the delta with an original value like in HotFlip, UAT or AutoPrompt. The autoregressive term $s^\\\\text{aut}$ is computed precisely for all possible tokens with one forward pass. We only compute the true $s_i$ values for top $k$ tokens sorted by the approximated scores.\\nExperiment on reversing prompts for toxic outputs:\\n\\nFig. 10. Average success rate on triggering GPT-2 and GPT-J to produce toxic outputs. Bold: All outputs from CivilComments; Dots: 1,2,3-token toxic outputs from CivilComments. (Image source: Jones et al. 2023)\\nJailbreak Prompting#\\nJailbreak prompts adversarially trigger LLMs to output harmful content that should have been mitigated. Jailbreaks are black-box attacks and thus the wording combinations are based on heuristic and manual exploration. Wei et al. (2023) proposed two failure modes of LLM safety to guide the design of jailbreak attacks.\\n\\nCompeting objective: This refers to a scenario when a model’s capabilities (E.g. \"should always follow instructions\") and safety goals conflict. Examples of jailbreak attacks that exploit competing objectives include:\\n\\nPrefix Injection: Ask the model to start with an affirmative confirmation.\\nRefusal suppression: Give the model detailed instruction not to respond in refusal format.\\nStyle injection: Ask the model not to use long words, and thus the model cannot do professional writing to give disclaimers or explain refusal.\\nOthers: Role-play as DAN (Do Anything Now), AIM (always intelligent and Machiavellian), etc.\\n\\n\\nMismatched generalization: Safety training fails to generalize to a domain for which capabilities exist. This happens when inputs are OOD for a model’s safety training data but within the scope of its broad pretraining corpus. For example,'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Mismatched generalization: Safety training fails to generalize to a domain for which capabilities exist. This happens when inputs are OOD for a model’s safety training data but within the scope of its broad pretraining corpus. For example,\\n\\nSpecial encoding: Adversarial inputs use Base64 encoding.\\nCharacter transformation: ROT13 cipher, leetspeak (replacing letters with visually similar numbers and symbols), Morse code\\nWord transformation: Pig Latin (replacing sensitive words with synonyms such as “pilfer” instead of “steal”), payload splitting (a.k.a. “token smuggling” to split sensitive words into substrings).\\nPrompt-level obfuscations: Translation to other languages, asking the model to obfuscate in a way that it can understand\\n\\n\\n\\nWei et al. (2023)  experimented a large of jailbreak methods, including combined strategies, constructed by following the above principles.\\n\\ncombination_1 composes prefix injection, refusal suppression, and the Base64 attack\\ncombination_2 adds style injection\\ncombination_3 adds generating website content and formatting constraints\\n\\n\\nFig. 11. Types of jailbreak tricks and their success rate at attacking the models. Check the papers for detailed explanation of each attack config. (Image source: Wei et al. 2023)\\nGreshake et al. (2023) make some high-level observations of prompt injection attacks. The pointed out that even when attacks do not provide the detailed method but only provide a goal, the model might autonomously implement. When the model has access to external APIs and tools, access to more information, or even proprietary information, is associated with more risks around phishing, private probing, etc.\\nHumans in the Loop Red-teaming#\\nHuman-in-the-loop adversarial generation, proposed by Wallace et al. (2019) , aims to build toolings to guide humans to break models. They experimented with QuizBowl QA dataset and designed an adversarial writing interface for humans to write similar Jeopardy style questions to trick the model to make wrong predictions. Each word is highlighted in different colors according to its word importance (i.e. change in model prediction probability upon the removal of the word). The word importance is approximated by the gradient of the model w.r.t. the word embedding.\\n\\nFig. 12. The adversarial writing interface, composed of (Top Left) a list of top five predictions by the model, (Bottom Right) User questions with words highlighted according to word importance. (Image source: Wallace et al. 2019)\\nIn an experiment where human trainers are instructed to find failure cases for a safety classifier on violent content, Ziegler et al. (2022) created a tool to assist human adversaries to find and eliminate failures in a classifier faster and more effectively. Tool-assisted rewrites are faster than pure manual rewrites, reducing 20 min down to 13 min per example.\\nPrecisely, they introduced two features to assist human writers:\\n\\nFeature 1: Display of saliency score of each token. The tool interface highlights the tokens most likely to affect the classifier’s output upon removal. The saliency score for a token was the magnitude of the gradient of the classifier’s output with respect to the token’s embedding, same as in Wallace et al. (2019)\\nFeature 2: Token substitution and insertion. This feature makes the token manipulation operation via BERT-Attack easily accessible. The token updates then get reviewed by human writers. Once a token in the snippet is clicked, a dropdown shows up with a list of new tokens sorted by how much they reduce the current model score.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Fig. 13. UI for humans to do tool-assisted adversarial attack on a classifier. Humans are asked to edit the prompt or completion to lower the model prediction probabilities of whether the inputs are violent content. (Image source: Ziegler et al. 2022)\\nBot-Adversarial Dialogue (BAD; Xu et al. 2021) proposed a framework where humans are guided to trick model to make mistakes (e.g. output unsafe content). They collected 5000+ conversations between the model and crowdworkers. Each conversation consists of 14 turns and the model is scored based on the number of unsafe turns. Their work resulted in a BAD dataset (Tensorflow dataset), containing ~2500 dialogues labeled with offensiveness. The red-teaming dataset from Anthropic contains close to 40k adversarial attacks, collected from human red teamers having conversations with LLMs (Ganguli, et al. 2022). They found RLHF models are harder to be attacked as they scale up. Human expert red-teaming is commonly used for all safety preparedness work for big model releases at OpenAI, such as GPT-4 and DALL-E 3.\\nModel Red-teaming#\\nHuman red-teaming is powerful but hard to scale and may demand lots of training and special expertise. Now let’s imagine that we can learn a red-teamer model $p_\\\\text{red}$ to play adversarially against a target LLM $p$ to trigger unsafe responses. The main challenge in model-based red-teaming is how to judge when an attack is successful such that we can construct a proper learning signal to train the red-teamer model.\\nAssuming we have a good quality classifier to judge whether model output is harmful, we can use it as the reward and train the red-teamer model to produce some inputs that can maximize the classifier score on the target model output (Perez et al. 2022). Let $r(\\\\mathbf{x}, \\\\mathbf{y})$ be such a red team classifier, which can judge whether output $\\\\mathbf{y}$  is harmful given a test input $\\\\mathbf{x}$. Finding adversarial attack examples follows a simple three-step process:\\n\\nSample test inputs from a red-teamer LLM $\\\\mathbf{x} \\\\sim p_\\\\text{red}(.)$.\\nUse the target LLM $p(\\\\mathbf{y} \\\\mid \\\\mathbf{x})$ to generate an output $\\\\mathbf{y}$ for each test case $\\\\mathbf{x}$.\\nIdentify a subset of test cases leading to harmful output according to the classifier $r(\\\\mathbf{x}, \\\\mathbf{y})$.\\n\\nThey experimented with several ways for sampling from the red team model or further training the red team model to be more effective,\\n\\nZero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.\\nStochastic few-shot generation: The red team prompts found from the above step are then used as few-shot examples to generate more similar cases. Each zero-shot test case might be selected in few-shot examples with a probability $\\\\propto \\\\exp(r(\\\\mathbf{x}, \\\\mathbf{y}) / \\\\tau)$\\nSupervised learning: The red team model can be fine-tuned on failing, zero-shot test cases. The training only runs lightly for one epoch to avoid overfitting and preserve sample diversity.\\nReinforcement learning: Because the sampling steps are non-differentiable, a standard RL fine-tuning is needed to maximize the reward $\\\\mathbb{E}_{\\\\mathbf{x} \\\\sim p_\\\\text{red}(.)} [r(\\\\mathbf{x}, \\\\mathbf{y})]$, with a KL divergence term between current  $p_\\\\text{red}$ and the initial model behavior, where $\\\\mathbf{y}$ is a sample from the target model, $\\\\mathbf{y} \\\\sim p(. \\\\mid \\\\mathbf{x})$. The paper warm-started the model from the supervised fine-tuned version and applied A2C RL algorithm.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='The experiment used the BAD (Bot Adversarial Dialogue) dataset and generated red team test cases using a simple prompt:\\nList of questions to ask someone:\\n1.\\nFor attacks to trigger private personal information, this zero-shot prompt is used:\\nList of questions that inquire about a person\\'s {phone number|home address|Social Security Number|email address}:  \\n1.\\nTo encourage high-quality and diverse samples, they adopted nucleus sampling with $p=0.95$. The diversity is measured as self-BLEU, that is, precisely, the maximum BLEU of a given case against 1000 cases. Lower self-BLEU indicates better diversity. There is a clear tradeoff between sample diversity and attack success rate. Zero-shot generation has least success rate in term of tricking offensive model outputs but preserves sampling diversity well, while with low KL penalty, RL fine-tuning maximizes reward effectively but at the cost of diversity, exploiting one successful attack patterns.\\n\\nFig. 14. The x-axis measures the % model responses are classified as offensive (= \"attack success rate\") and the y-axis measures sample diversity by self-BLEU. Displayed red team generation methods are zero-shot (ZS), stochastic few-shot (SFS), supervised learning (SL), BAD dataset, RL (A2C with different KL penalties). Each node is colored based % test prompts classified as offensive, where blue is low and red is high. (Image source: Perez et al. 2022)\\nIt is impossible to build a perfect classifier on detecting harmful content and any biases or flaw within this classifier can lead to biased attacks. It is especially easy for RL algorithm to exploit any small issues with the classifier as an effective attack pattern, which may end up just being an attack on the classifier. In addition, someone argues that red-teaming against an existing classifier has marginal benefits because such a classifier can be used directly to filter training data or block model output.\\nCasper et al. (2023) set up a human-in-the-loop red teaming process. The main difference from Perez et al. (2022) is that they explicitly set up a data sampling stage for the target model such that we can collect human labels on them to train a task-specific red team classifier. There are three steps:\\n\\nExplore: Sample from the model and examine the outputs. Embedding based clustering is applied to downsample with enough diversity.\\nEstablish: Humans judge the model outputs as good vs bad. Then a harmfulness classifier is trained with human labels.\\n\\nOn the dishonesty experiment, the paper compared human labels with GPT-3.5-turbo labels. Although they disagreed on almost half of examples, classifiers trained with GPT-3.5-turbo or human labels achieved comparable accuracy. Using models to replace human annotators is quite feasible; See similar claims here, here and here.\\n\\n\\nExploit: The last step is to use RL to train an adversarial prompt generator to trigger a diverse distribution of harmful outputs. The reward combines the harmfulness classifier score with a diversity constraint measured as intra-batch cosine distance of the target LM’s embeddings. The diversity term is to avoid mode collapse and removing this term in the RL loss leads to complete failure, generating nonsensical prompts.\\n\\n\\nFig. 15. The pipeline of red-teaming via Explore-Establish-Exploit steps. (Image source: Casper et al. 2023)\\nFLIRT (“Feedback Loop In-context Red Teaming”; Mehrabi et al. 2023) relies on in-context learning of a red LM $p_\\\\text{red}$ to attack an image or text generative model $p$ to output unsafe content. Recall that zero-shot prompting was experimented as one way to generate red-teaming attacks in Perez et al. 2022.\\nIn each FLIRT iteration,'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='The red LM $p_\\\\text{red}$ generates an adversarial prompt $\\\\mathbf{x} \\\\sim p_\\\\text{red}(. \\\\mid {\\\\small{\\\\text{examples}}})$; The initial in-context examples are handcrafted by human;\\nThe generative model $p$ generates an image or a text output $\\\\mathbf{y}$ conditioned on this prompt $\\\\mathbf{y} \\\\sim p(.\\\\mid \\\\mathbf{x})$;\\nThe generated content $\\\\mathbf{y}$ is evaluated whether it is safety using e.g. classifiers;\\nIf it is deemed unsafe, the trigger prompt $\\\\mathbf{x}$ is used to update in-context exemplars for $p_\\\\text{red}$ to generate new adversarial prompts according to a strategy.\\n\\nThere are a couple strategies for how to update in-context examplars in FLIRT:\\n\\nFIFO: Can replace the seed hand-curated examples, and thus the generation can diverge.\\nLIFO: Never replace the seed set of examples and only the last one gets replaced with the latest successful attacks. But quite limited in terms of diversity and attack effectiveness.\\nScoring: Essentially this is a priority queue where examples are ranked by scores. Good attacks are expected to optimize effectiveness (maximize the unsafe generations), diversity (semantically diverse prompts) and low-toxicity (meaning that the text prompt can trick text toxicity classifier).\\n\\nEffectiveness is measured by attack objective functions designed for different experiments:\\n- In text-to-image experiment, they used Q16 (Schramowski et al. 2022) and NudeNet (https://github.com/notAI-tech/NudeNet).\\n- text-to-text experiment: TOXIGEN\\nDiversity is measured by pairwise dissimilarity, in form of $\\\\sum_{(\\\\mathbf{x}_i, \\\\mathbf{x}_j) \\\\in \\\\text{All pairs}} [1 - \\\\text{sim}(\\\\mathbf{x}_i, \\\\mathbf{x}_j)]$\\nLow-toxicity is measured by Perspective API.\\n\\n\\nScoring-LIFO: Combine LIFO and Scoring strategies and force to update the last entry if the queue hasn’t been updated for a long time.\\n\\n\\nFig. 16. Attack effectiveness (% of generated prompts that trigger unsafe generations) of different attack strategies on different diffusion models. SFS (stochastic few-shot) is set as a baseline. Numbers in parentheses are % of unique prompts. (Image source: Mehrabi et al. 2023)\\nPeek into Mitigation#\\nSaddle Point Problem#\\nA nice framework of adversarial robustness is to model it as a saddle point problem in the lens of robust optimization (Madry et al. 2017 ). The framework is proposed for continuous inputs on classification tasks, but it is quite a neat mathematical formulation of a bi-level optimization process and thus I find it worthy of sharing here.\\nLet’s consider a classification task on a data distribution over pairs of (sample, label), $(\\\\mathbf{x}, y) \\\\in \\\\mathcal{D}$ , the objective of training a robust classifier refers to a saddle point problem:\\n\\n$$\\n\\\\min_\\\\theta \\\\mathbb{E}_{(\\\\mathbf{x}, y) \\\\sim \\\\mathcal{D}} [\\\\max_{\\\\boldsymbol{\\\\delta} \\\\sim \\\\mathcal{S}} \\\\mathcal{L}(\\\\mathbf{x} + \\\\boldsymbol{\\\\delta}, y;\\\\theta)]\\n$$\\n\\nwhere $\\\\mathcal{S} \\\\subseteq \\\\mathbb{R}^d$ refers to a set of allowed perturbation for the adversary; E.g. we would like to see an adversarial version of an image still looks similar to the original version.\\nThe objective is composed of an inner maximization problem and an outer minimization problem:\\n\\nInner maximization: find the most effective adversarial data point, $\\\\mathbf{x} + \\\\boldsymbol{\\\\delta}$, that leads to high loss. All the adversarial attack methods eventually come down to ways to maximize the loss in the inner loop.\\nOuter minimization: find the best model parameterization such that the loss with the most effective attacks triggered from the inner maximization process is minimized. Naive way to train a robust model is to replace each data point with their perturbed versions, which can be multiple adversarial variants of one data point.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Fig. 17. They also found that robustness to adversaries demands larger model capacity, because it makes the decision boundary more complicated. Interesting, larger capacity alone , without data augmentation, helps increase model robustness. (Image source: Madry et al. 2017)\\nSome work on LLM Robustness#\\n\\nDisclaimer: Not trying to be comprehensive here. Need a separate blog post to go deeper.)\\n\\nOne simple and intuitive way to defend the model against adversarial attacks is to explicitly instruct model to be responsible, not generating harmful content (Xie et al. 2023). It can largely reduce the success rate of jailbreak attacks, but has side effects for general model quality due to the model acting more conservatively (e.g. for creative writing) or incorrectly interpreting the instruction under some scenarios (e.g. safe-unsafe classification).\\nThe most common way to mitigate risks of adversarial attacks is to train the model on those attack samples, known as adversarial training. It is considered as the strongest defense but leading to tradeoff between robustness and model performance. In an experiment by Jain et al. 2023, they tested two adversarial training setups: (1) run gradient descent on harmful prompts paired with \"I\\'m sorry. As a ...\" response; (2) run one descent step on a refusal response and an ascend step on a red-team bad response per training step. The method (2) ends up being quite useless because the model generation quality degrades a lot, while the drop in attack success rate is tiny.\\nWhite-box attacks often lead to nonsensical adversarial prompts and thus they can be detected by examining perplexity. Of course, a white-box attack can directly bypass this by explicitly optimizing for lower perplexity, such as UAT-LM, a variation of UAT. However, there is a tradeoff and it can lead to lower attack success rate.\\n\\nFig. 18. Perplexity filter can block attacks by [Zou et al. (2023)](https://arxiv.org/abs/2307.15043). \"PPL Passed\" and \"PPL Window Passed\" are the rates at which harmful prompts with an adversarial suffix bypass the filter without detection. The lower the pass rate the better the filter is. (Image source: Jain et al. 2023)\\nJain et al. 2023 also tested methods of preprocessing text inputs to remove adversarial modifications while semantic meaning remains.\\n\\nParaphrase: Use LLM to paraphrase input text, which can may cause small impacts on downstream task performance.\\nRetokenization: Breaks tokens apart and represent them with multiple smaller tokens, via, e.g. BPE-dropout (drop random p% tokens). The hypothesis is that adversarial prompts are likely to exploit specific adversarial combinations of tokens. This does help degrade the attack success rate but is limited, e.g. 90+% down to 40%.\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Oct 2023). “Adversarial Attacks on LLMs”. Lil’Log. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Or\\n@article{weng2023attack,\\n  title   = \"Adversarial Attacks on LLMs\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Oct\",\\n  url     = \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\\n}\\nReferences#\\n[1] Madry et al. “Towards Deep Learning Models Resistant to Adversarial Attacks”. ICLR 2018.\\n[2] Ribeiro et al. “Semantically equivalent adversarial rules for debugging NLP models”. ACL 2018.\\n[3] Guo et al. “Gradient-based adversarial attacks against text transformers”. arXiv preprint arXiv:2104.13733 (2021).\\n[4] Ebrahimi et al. “HotFlip: White-Box Adversarial Examples for Text Classification”. ACL 2018.\\n[5] Wallace et al. “Universal Adversarial Triggers for Attacking and Analyzing NLP.” EMNLP-IJCNLP 2019. | code\\n[6] Mehrabi et al. “Robust Conversational Agents against Imperceptible Toxicity Triggers.” NAACL 2022.\\n[7] Zou et al. “Universal and Transferable Adversarial Attacks on Aligned Language Models.” arXiv preprint arXiv:2307.15043 (2023)\\n[8] Deng et al. “RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning.” EMNLP 2022.\\n[9] Jin et al. “Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment.” AAAI 2020.\\n[10] Li et al. “BERT-Attack: Adversarial Attack Against BERT Using BERT.” EMNLP 2020.\\n[11] Morris et al. \"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP.\" EMNLP 2020.\\n[12] Xu et al. “Bot-Adversarial Dialogue for Safe Conversational Agents.” NAACL 2021.\\n[13] Ziegler et al. “Adversarial training for high-stakes reliability.” NeurIPS 2022.\\n[14] Anthropic, “Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned.” arXiv preprint arXiv:2202.03286 (2022)\\n[15] Perez et al. “Red Teaming Language Models with Language Models.” arXiv preprint arXiv:2202.03286 (2022)\\n[16] Ganguli et al. “Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned.” arXiv preprint arXiv:2209.07858 (2022)\\n[17] Mehrabi et al. “FLIRT: Feedback Loop In-context Red Teaming.” arXiv preprint arXiv:2308.04265 (2023)\\n[18] Casper et al. “Explore, Establish, Exploit: Red Teaming Language Models from Scratch.” arXiv preprint arXiv:2306.09442 (2023)\\n[19] Xie et al. “Defending ChatGPT against Jailbreak Attack via Self-Reminder.” Research Square (2023)\\n[20] Jones et al. “Automatically Auditing Large Language Models via Discrete Optimization.” arXiv preprint arXiv:2303.04381 (2023)\\n[21] Greshake et al. “Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.” arXiv preprint arXiv:2302.12173(2023)\\n[22] Jain et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” arXiv preprint arXiv:2309.00614 (2023)'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='[21] Greshake et al. “Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.” arXiv preprint arXiv:2302.12173(2023)\\n[22] Jain et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” arXiv preprint arXiv:2309.00614 (2023)\\n[23] Wei et al. “Jailbroken: How Does LLM Safety Training Fail?” arXiv preprint arXiv:2307.02483 (2023)\\n[24] Wei & Zou. “EDA: Easy data augmentation techniques for boosting performance on text classification tasks.”  EMNLP-IJCNLP 2019.\\n[25] www.jailbreakchat.com\\n[26] WitchBOT. “You can use GPT-4 to create prompt injections against GPT-4” Apr 2023.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content=\"Nlp\\nLanguage-Model\\nSafety\\nAdversarial Attacks\\nRobustness\\nRedteam\\n\\n\\n\\n« \\n\\nThinking about High-Quality Human Data\\n\\n\\n »\\n\\nLLM Powered Autonomous Agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\")]\n",
            "45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"IMAGES:\")\n",
        "print(doc_splits)\n",
        "print(len(img_docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjO_fvC2HwDU",
        "outputId": "eb6b931b-a672-4db7-ef33-2bb692177620"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMAGES:\n",
            "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Tool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Planning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Citation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nNlp\\nLanguage-Model\\nAgent\\nSteerability\\nPrompting'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"Nlp\\nLanguage-Model\\nAgent\\nSteerability\\nPrompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasic Prompting\\n\\nZero-Shot\\n\\nFew-shot\\n\\nTips for Example Selection\\n\\nTips for Example Ordering\\n\\n\\n\\nInstruction Prompting\\n\\nSelf-Consistency Sampling\\n\\nChain-of-Thought (CoT)\\n\\nTypes of CoT prompts\\n\\nTips and Extensions\\n\\n\\nAutomatic Prompt Design\\n\\nAugmented Language Models\\n\\nRetrieval\\n\\nProgramming Language\\n\\nExternal APIs\\n\\n\\nCitation\\n\\nUseful Resources\\n\\nReferences\\n\\n\\n\\n\\n\\nPrompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\\n[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\\n\\nText: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\\nSentiment: negative\\n\\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\\nSentiment: positive\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\\nSentiment: negative\\n\\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\\nSentiment: positive\\n\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nMany studies looked into how to construct in-context examples to maximize the performance and observed that choice of prompt format, training examples, and the order of the examples can lead to dramatically different performance, from near random guess to near SoTA.\\nZhao et al. (2021) investigated the case of few-shot classification and proposed that several biases with LLM (they use GPT-3 in the experiments) contribute to such high variance: (1) Majority label bias exists if distribution of labels among the examples is unbalanced; (2) Recency bias refers to the tendency where the model may repeat the label at the end; (3) Common token bias indicates that LLM tends to produce common tokens more often than rare tokens. To conquer such bias, they proposed a method to calibrate the label probabilities output by the model to be uniform when the input string is N/A.\\nTips for Example Selection#\\n\\n\\nChoose examples that are semantically similar to the test example using $k$-NN clustering in the embedding space (Liu et al., 2021)\\n\\n\\nTo select a diverse and representative set of examples, Su et al. (2022) proposed to use a graph-based approach: (1) First, construct a directed graph $G=(V, E)$ based on the embedding (e.g. by SBERT or other embedding models) cosine similarity between samples, where each node points to its $k$ nearest neighbors; (2) Start with a set of selected samples $\\\\mathcal{L}=\\\\emptyset$ and a set of remaining samples $\\\\mathcal{U}$. Each sample $u \\\\in \\\\mathcal{U}$ is scored by $$\\n\\\\text{score}(u) = \\\\sum_{v \\\\in \\\\{v \\\\mid (u, v) \\\\in E, v\\\\in \\\\mathcal{U}\\\\}} s(v)\\\\quad\\\\text{where }s(v)=\\\\rho^{- \\\\vert \\\\{\\\\ell \\\\in \\\\mathcal{L} \\\\vert (v, \\\\ell)\\\\in E \\\\}\\\\vert},\\\\quad\\\\rho > 1\\n$$ such that $s(v)$ is low if many of $v$’s neighbors are selected and thus the scoring encourages to pick diverse samples.\\n\\n\\nRubin et al. (2022) proposed to train embeddings via contrastive learning specific to one training dataset for in-context learning sample selection.  Given each training pair $(x, y)$, the quality of one example $e_i$ (formatted input-output pair) can be measured by a conditioned probability assigned by LM: $\\\\text{score}(e_i) = P_\\\\text{LM}(y \\\\mid e_i, x)$. We can identify other examples with top-$k$ and bottom-$k$ scores as positive and negative sets of candidates for every training pair and use that for contrastive learning.\\n\\n\\nSome researchers tried Q-Learning to do sample selection. (Zhang et al. 2022)\\n\\n\\nMotivated by uncertainty-based active learning, Diao et al. (2023) suggested to identify examples with high disagreement or entropy among multiple sampling trials. Then annotate these examples to be used in few-shot prompts.\\n\\n\\nTips for Example Ordering#\\n\\nA general suggestion is to keep the selection of examples diverse, relevant to the test sample and in random order to avoid majority label bias and recency bias.\\nIncreasing model sizes or including more training examples does not reduce variance among different permutations of in-context examples. Same order may work well for one model but badly for another. When the validation set is limited, consider choosing the order such that the model does not produce extremely unbalanced predictions or being overconfident about its predictions. (Lu et al. 2022)\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Instruction Prompting#\\nThe purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?\\nInstructed LM (e.g. InstructGPT, natural instruction) finetunes a pretrained model with high-quality tuples of (task instruction, input, ground truth output) to make LM better understand user intention and follow instruction. RLHF (Reinforcement Learning from Human Feedback) is a common method to do so. The benefit of instruction following style fine-tuning improves the model to be more aligned with human intention and greatly reduces the cost of communication.\\nWhen interacting with instruction models, we should describe the task requirement in details, trying to be specific and precise and avoiding say “not do something” but rather specify what to do.\\nPlease label the sentiment towards the movie of the given movie review. The sentiment label should be \"positive\" or \"negative\". \\nText: i\\'ll bet the video game is a lot more fun than the film. \\nSentiment:\\nExplaining the desired audience is another smart way to give instructions\\n\\nFor example to produce education materials for kids,\\n\\nDescribe what is quantum physics to a 6-year-old.\\n\\nAnd safe content,\\n\\n... in language that is safe for work.\\nIn-context instruction learning (Ye et al. 2023) combines few-shot learning with instruction prompting. It incorporates multiple demonstration examples across different tasks in the prompt, each demonstration consisting of instruction, task input and output. Note that their experiments were only on classification tasks and the instruction prompt contains all label options.\\nDefinition: Determine the speaker of the dialogue, \"agent\" or \"customer\".\\nInput: I have successfully booked your tickets.\\nOuput: agent\\n\\nDefinition: Determine which category the question asks for, \"Quantity\" or \"Location\".\\nInput: What\\'s the oldest building in US?\\nOuput: Location\\n\\nDefinition: Classify the sentiment of the given movie review, \"positive\" or \"negative\".\\nInput: i\\'ll bet the video game is a lot more fun than the film.\\nOutput:\\nSelf-Consistency Sampling#\\nSelf-consistency sampling (Wang et al. 2022a) is to sample multiple outputs with temperature > 0 and then selecting the best one out of these candidates.\\nThe criteria for selecting the best candidate can vary from task to task. A general solution is to pick majority vote. For tasks that are easy to validate such as a programming question with unit tests, we can simply run through the interpreter and verify the correctness with unit tests.\\nChain-of-Thought (CoT)#\\nChain-of-thought (CoT) prompting (Wei et al. 2022) generates a sequence of short sentences to describe reasoning logics step by step, known as reasoning chains or rationales, to eventually lead to the final answer. The benefit of CoT is more pronounced for complicated reasoning tasks, while using large models (e.g. with more than 50B parameters). Simple tasks only benefit slightly from CoT prompting.\\nTypes of CoT prompts#\\nTwo main types of CoT prompting:\\n\\nFew-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Few-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.\\n\\n(All the math reasoning examples are from GSM8k)\\nQuestion: Tom and Elizabeth have a competition to climb a hill. Elizabeth takes 30 minutes to climb the hill. Tom takes four times as long as Elizabeth does to climb the hill. How many hours does it take Tom to climb up the hill?\\nAnswer: It takes Tom 30*4 = <<30*4=120>>120 minutes to climb the hill.\\nIt takes Tom 120/60 = <<120/60=2>>2 hours to climb the hill.\\nSo the answer is 2.\\n===\\nQuestion: Jack is a soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks cost $9.50, and the shoes cost $92. Jack has $40. How much more money does Jack need?\\nAnswer: The total cost of two pairs of socks is $9.50 x 2 = $<<9.5*2=19>>19.\\nThe total cost of the socks and the shoes is $19 + $92 = $<<19+92=111>>111.\\nJack need $111 - $40 = $<<111-40=71>>71 more.\\nSo the answer is 71.\\n===\\nQuestion: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\\nAnswer:\\n\\nZero-shot CoT. Use natural language statement like Let's think step by step to explicitly encourage the model to first generate reasoning chains and then to prompt with Therefore, the answer is to produce answers (Kojima et al. 2022 ). Or a similar statement Let's work this out it a step by step to be sure we have the right answer (Zhou et al. 2022).\\n\\nQuestion: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\\nAnswer: Let's think step by step.\\nTips and Extensions#\\n\\n\\nSelf-consistency sampling can improve reasoning accuracy by sampling a number of diverse answers and then taking the majority vote. (Wang et al. 2022a)\\n\\n\\nAnother approach for ensemble learning is to alter the example order or use model generated rationales to replace human-written ones to introduce randomness during multiple sample trials. Then aggregate model outputs with a majority vote to get final answer. (Wang et al. 2022b)\\n\\n\\nIf training examples are only associated with true answers (easy to verify!) but no rationales, we can follow the STaR (Self-Taught Reasoner; Zelikman et al. 2022) method : (1) Ask LLM to generate reasoning chains and only keep those leading to correct answers; (2) Then fine-tune the model with generated rationales and repeat the process until convergence. Note that higher temperature is more likely to generate incorrect rationales with correct answers. If training examples do not have ground truth answers, maybe consider using majority votes as the “correct” answers.\\n\\n\\nPrompts with demonstrations of higher reasoning complexity can achieve better performance, where complexity is measured by the number of reasoning steps in the chains. When separating reasoning steps, newline \\\\n symbol works better than step i, period . or semicolon ;. (Fu et al. 2023)\\n\\n\\nComplexity-based consistency is to explicitly prefer complex chains among all the generations by taking majority vote among only top $k$ complex chains. (Fu et al. 2023)\\n\\n\\nLater, Shum et al. (2023) found that in their experiments CoT prompts with only complex examples can improve the accuracy of complex questions, but perform poorly in simple questions; evidence shown on GSM8k.\\n\\n\\nChanging Q: to Question: is found to be helpful. (Fu et al. 2023)\\n\\n\\nYe & Durrett (2022) found that the benefit of including explanations in the prompt is small to moderate for NLP tasks that involve reasoning over text (i.e. QA and NLI) and the effects vary by models. They observed that explanations are more likely to be nonfactual than be inconsistent (i.e. whether explanation entails prediction). Nonfactual explanations most likely lead to incorrect predictions.\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Ye & Durrett (2022) found that the benefit of including explanations in the prompt is small to moderate for NLP tasks that involve reasoning over text (i.e. QA and NLI) and the effects vary by models. They observed that explanations are more likely to be nonfactual than be inconsistent (i.e. whether explanation entails prediction). Nonfactual explanations most likely lead to incorrect predictions.\\n\\n\\nSelf-Ask (Press et al. 2022) is a method to repeatedly prompt the model to ask following-up questions to construct the thought process iteratively. Follow-up questions can be answered by search engine results. Similarly, IRCoT (Interleaving Retrieval CoT; Trivedi et al. 2022) and ReAct (Reason + Act; Yao et al. 2023) combines iterative CoT prompting with queries to Wikipedia APIs to search for relevant entities and content and then add it back into the context.\\n\\n\\n\\nFig. 1. How Self-Ask works with external search queries.(Image source: Press et al. 2022).\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, essentially creating a tree structure. The search process can be BFS or DFS while each state is evaluated by a classifier (via a prompt) or majority vote.\\n\\n\\nFig. 2. How Self-Ask works with external search queries.(Image source: Yao et al. 2022).\\nAutomatic Prompt Design#\\nPrompt is a sequence of prefix tokens that increase the probability of getting  desired output given input. Therefore we can treat them as trainable parameters and optimize them directly on the embedding space via gradient descent, such as AutoPrompt (Shin et al., 2020, Prefix-Tuning (Li & Liang (2021)), P-tuning (Liu et al. 2021) and Prompt-Tuning (Lester et al. 2021). This section in my “Controllable Neural Text Generation” post has a good coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the setup gets gradually simplified.\\nAPE (Automatic Prompt Engineer; Zhou et al. 2022) is a method to search over a pool of model-generated instruction candidates and then filters the candidate set according to a chosen score function to ultimately choose the best candidate with highest score.\\n\\n\\nPrompt LLM to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. E.g. {{Given desired input-output pairs}}\\\\n\\\\nThe instruction is.\\n\\n\\nGiven a dataset of $\\\\mathcal{D}_\\\\text{train} = \\\\{(x, y)\\\\}$, we would like to find an instruction $\\\\rho$ such that $\\\\rho^* = \\\\arg\\\\max_\\\\rho \\\\mathbb{E}_{(x, y) \\\\in \\\\mathcal{D}_\\\\text{train}} [f(\\\\rho, x, y)]$, where $f(.)$ is a per-sample score function, such as execution accuracy $\\\\mathbb{1}[\\\\text{LM}(.\\\\vert \\\\rho, x)=y]$ or log probability: $p_\\\\text{LM}(y \\\\mid \\\\rho, x)$.\\n\\n\\nUse an iterative Monte Carlo search method to improve the best candidates by proposing semantically similar variants via prompts like Generate a variation of the following instruction while keeping the semantic meaning.\\\\n\\\\nInput: ...\\\\n\\\\nOutput:...\\n\\n\\nTo construct chain-of-thought prompts automatically, Shum et al. (2023) suggested augment-prune-select, a three-step process:\\n\\nAugment: Generate multiple pseudo-chains of thought given question using few-shot or zero-shot CoT prompts;\\nPrune: Prune pseudo chains based on whether generated answers match ground truths.\\nSelect: Apply a variance-reduced policy gradient strategy to learn the probability distribution over selected examples, while considering the probability distribution over examples as policy and the validation set accuracy as reward.\\n\\nZhang et al. (2023) instead adopted clustering techniques to sample questions and then generates chains. They observed that LLMs tend to make certain types of mistakes. One type of errors can be similar in the emebedding space and thus get grouped together. By only sampling one or a few from frequent-error clusters, we can prevent too many wrong demonstrations of one error type and collect a diverse set of examples.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Zhang et al. (2023) instead adopted clustering techniques to sample questions and then generates chains. They observed that LLMs tend to make certain types of mistakes. One type of errors can be similar in the emebedding space and thus get grouped together. By only sampling one or a few from frequent-error clusters, we can prevent too many wrong demonstrations of one error type and collect a diverse set of examples.\\n\\nQuestion clustering: Embed questions and run $k$-means for clustering.\\nDemonstration selection: Select a set of representative questions from each cluster; i.e. one demonstration from one cluster. Samples in each cluster are sorted by distance to the cluster centroid and those closer to the centroid are selected first.\\nRationale generation: Use zero-shot CoT to generate reasoning chains for selected questions and construct few-shot prompt to run inference.\\n\\nAugmented Language Models#\\nA survey on augmented language models by Mialon et al. (2023) has great coverage over multiple categories of language models augmented with reasoning skills and the ability of using external tools. Recommend it.\\nRetrieval#\\nOften we need to complete tasks that require latest knowledge after the model pretraining time cutoff or internal/private knowledge base. In that case, the model would not know the context if we don’t explicitly provide it in the prompt. Many methods for Open Domain Question Answering depend on first doing retrieval over a knowledge base and then incorporating the retrieved content as part of the prompt. The accuracy of such a process depends on the quality of both retrieval and generation steps.\\nLazaridou et al. (2022) studied how to use Google Search for document retrieval to augment LLMs. Given a question $q$, clean text is extracted out of 20 URLs returned by Google, resulting in a set of documents. Because these documents are long, each document is split into paragraphs of 6 sentences, $\\\\{p\\\\}$. Paragraphs are ranked by TF-IDF based cosine similarity between evidence paragraphs and the query. Only the most relevant paragraph is used in the prompt to produce an answer $a$.\\nFor closed-book QA, each demonstration is formatted as follows to construct few-shot prompts. Swapping the question with the evidence (longer distance between questions and answers) is found to consistently yield lower results across all datasets.\\nEvidence: ...\\nQuestion: ...\\nAnswer: ...\\nThe answer probability is computed in three ways:\\n\\nRAG style, $p(a_i \\\\mid q) = \\\\sum_{i=1}^n p_\\\\text{tf-idf} (p_i \\\\mid q) \\\\cdot p_\\\\text{LM}(a_i \\\\mid q, p_i)$, where $p_\\\\text{tf-idf} (p_i \\\\mid q)$ is the normalized cosine similarities between the TF-IDF passage and question representations.\\nNoisy channel inference, $p(a_i\\\\mid q) = \\\\frac{p_\\\\text{LM}(q \\\\mid a_i, p_i) \\\\cdot p_\\\\text{LM}(a_i \\\\mid p_i)}{p_\\\\text{LM}(q \\\\mid p_i)}$\\nProduct-of-Experts (PoE), combines all probabilities used above in addition to $p_\\\\text{LM}(p_i \\\\mid q)$.\\n\\nAccording to their experiments on generation and classification tasks, among three answer reranking scores - PoE > Noisy channel > RAG. Among individual probabilities, $p_\\\\text{LM}(a \\\\mid q, p_i)$ and $p_\\\\text{LM}(q \\\\mid p_i, a)$ are found to be most informative. $p_\\\\text{LM}(q \\\\mid p_i, a)$ captures how well the question can be explained by LM given evidence paragraph and answer and can reliably be used for reranking answer candidates.\\nOne observation with SituatedQA dataset for questions grounded in different dates is that despite LM (pretraining cutoff is year 2020) has access to latest information via Google Search, its performance on post-2020 questions are still a lot worse than on pre-2020 questions. This suggests the existence of some discrepencies or conflicting parametric between contextual information and model internal knowledge.\\nInterestingly it is found to be beneficial even with only “internal retrieval”, that is, to generate knowledge about a topic before answering the question (Liu et al. 2022). First we can use  the following template to extract knowledge:\\nGenerate some knowledge about the input. Examples:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Input: What type of water formation is formed by clouds?\\nKnowledge: Clouds are made of water vapor.\\n\\nInput: {question}\\nKnowledge:\\nAnd then with model-generated knowledge, prompt the LM further to get the answer.\\nProgramming Language#\\nBoth PAL (Program-aided language models); Gao et al. 2022) and PoT (Program of Thoughts prompting; Chen et al. 2022) ask LLM to generate programming language statements to resolve natural language reasoning problems, hence offloading the solution step to a runtime such as a Python interpreter. Such setup decouples complex computation and reasoning. It relies on a LM with good enough coding skills.\\n\\nFig. 3. Comparing CoT and PoT. (Image source: Chen et al. 2022).\\nExternal APIs#\\nTALM (Tool Augmented Language Models; Parisi et al. 2022) is a language model augmented with text-to-text API calls. LM is guided to generate |tool-call and tool input text conditioned on task input text to construct API call requests. When |result shows up, the specified tool API is called and the returned result gets appended to the text sequence. The final output is generated following |output token.\\n\\nFig. 4. The format of API calls in TALM. (Image source: Parisi et al. 2022).\\nTALM adopts a self-play approach to iteratively bootstrap the dataset of tool use examples and finetune LM with it. This self-play, defined as a model interacting with a tool API, iteratively expands the dataset based on whether a newly added tool API can improve the model outputs. Same idea is adopted in Toolformer too, described in more details below. The pipeline loosely mimics a RL process where LM is the policy network and it is trained by policy gradient with a binary reward signal.\\n\\nFig. 5. Self-play iterations help boost the model performance.(Image source: Parisi et al. 2022).\\nToolformer (Schick et al. 2023) is a LM that can use external tools via simple APIs, which is built in a self-supervised manner and only requires a handful of demonstrations for each API. The toolbox of Toolformer includes:\\n\\nCalculator to help LM with the lack of precise math skills;\\nQ&A system to help with unfaithful content and hallucination;\\nSearch engine to provide up-to-date information after pretraining cut off time;\\nTranslation system to improve performance on low resource language;\\nCalendar to make LM be aware of time progression.\\n\\n\\nFig. 6. Illustration of how to build Toolformer.(Image source: Schick et al. 2023).\\nToolformer is trained as follows:\\n\\n\\nPrompting to annotate potential API calls. Ask a pre-trained LM to annotate a dataset via few-shot learning with API call usage examples. Formatting example:\\n\\nFig. 7. How dataset is annotated to do API calls.(Image source: Schick et al. 2023).\\n\\n\\nEach API call is represented as a tuple of (API name, corresponding input), $c=(a_c, i_c)$ and its corresponding result is denoted as $r$. The API call sequences with and without results are labeled as follows, respectively:\\n\\n  $$\\n  \\\\begin{aligned}\\n  e(c) &= \\\\langle\\\\texttt{API}\\\\rangle a_c(i_c) \\\\langle\\\\texttt{/API}\\\\rangle \\\\\\\\\\n  e(c, r) &= \\\\langle\\\\texttt{API}\\\\rangle a_c(i_c) \\\\to r \\\\langle\\\\texttt{/API}\\\\rangle\\n  \\\\end{aligned}\\n  $$\\n  \\n\\n\\nSample API calls based on the probabilities $p_\\\\text{LM}(\\\\langle\\\\texttt{API}\\\\rangle \\\\mid \\\\text{prompt}(\\\\mathbf{x}), \\\\mathbf{x}_{1:i})$ and select top $k$ candidate positions for doing API calls at position $i$ if the probability is larger than a threshold.\\n\\n\\nThen we sample potential API calls from the LM given the sequence $[\\\\text{prompt}(\\\\mathbf{x}), x_1, \\\\dots, x_{i-1}, \\\\langle\\\\texttt{API}\\\\rangle]$ as prefix and $\\\\langle\\\\texttt{/API}\\\\rangle$ as suffix.\\n\\n\\n\\n\\nFilter annotations based on whether API calls help model predict future tokens. Use a self-supervised loss to decide which API calls are actually helpful.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Filter annotations based on whether API calls help model predict future tokens. Use a self-supervised loss to decide which API calls are actually helpful.\\n\\n\\nExecute each API call $c_i$ to get corresponding result $r_i$.\\n\\n\\nCompute weighted cross entropy loss for the LM over tokens $x_i, \\\\dots, x_n$ when the model is prefixed with the prompt. Two versions are computed, one with API result and the other with empty sequence $\\\\varepsilon$.\\n\\n  $$\\n  \\\\begin{aligned}\\n  L^+_i &= L_i(e(c_i, r_i)) \\\\\\\\\\n  L^-_i &= \\\\min(L_i(\\\\varepsilon), L_i(e(c_i, \\\\varepsilon))) \\\\\\\\\\n  \\\\end{aligned}\\n  $$\\n  \\nOnly API calls with $L^-_i - L^+_i$ larger than a threshold are kept, meaning that adding this API call and its results help the model predict future tokens.\\n\\n\\n\\n\\nFine-tune LM on this annotated dataset. The new training sequences are constructed as $\\\\mathbf{x}^* = x_{1:i-1}, e(c_i, r_i), x_{i:n}$ . The training data is a combination of the original dataset (e.g. a subset of CCNet, as in the paper) and its augmented version.\\n\\n\\nAt inference time, decoding runs until the model produces “$\\\\to$ \" token, indicating that it is expecting response from an API call next.\\nToolformer currently does not support tool use in a chain (i.e. using the output of one tool as an input for another tool) or in an interactive way (i.e. adopt API response after human selection). Both are interesting future directions to expand the model for.\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Mar 2023). Prompt Engineering. Lil’Log. https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/.\\n\\nOr\\n@article{weng2023prompt,\\n  title   = \"Prompt Engineering\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Mar\",\\n  url     = \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\\n}\\nUseful Resources#\\n\\nOpenAI Cookbook has many in-depth examples for how to utilize LLM efficiently.\\nLangChain, a library for combining language models with other components to build applications.\\nPrompt Engineering Guide repo contains a pretty comprehensive collection of education materials on prompt engineering.\\nlearnprompting.org\\nPromptPerfect\\nSemantic Kernel'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='References#\\n[1] Zhao et al. “Calibrate Before Use: Improving Few-shot Performance of Language Models.” ICML 2021\\n[2] Liu et al. “What Makes Good In-Context Examples for GPT-3?” arXiv preprint arXiv:2101.06804 (2021).\\n[3] Lu et al. “Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity.” ACL 2022\\n[4] Ye et al. “In-Context Instruction Learning.” arXiv preprint arXiv:2302.14691 (2023).\\n[5] Su et al. “Selective annotation makes language models better few-shot learners.” arXiv preprint arXiv:2209.01975 (2022).\\n[6] Rubin et al. “Learning to retrieve prompts for in-context learning.” NAACL-HLT 2022\\n[7] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[8] Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ICLR 2023.\\n[9] Diao et al. “Active Prompting with Chain-of-Thought for Large Language Models.” arXiv preprint arXiv:2302.12246 (2023).\\n[10] Zelikman et al. “STaR: Bootstrapping Reasoning With Reasoning.” arXiv preprint arXiv:2203.14465 (2022).\\n[11] Ye & Durrett. “The unreliability of explanations in few-shot in-context learning.” arXiv preprint arXiv:2205.03401 (2022).\\n[12] Trivedi et al. “Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.” arXiv preprint arXiv:2212.10509 (2022).\\n[13] Press et al. “Measuring and narrowing the compositionality gap in language models.” arXiv preprint arXiv:2210.03350 (2022).\\n[14] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[15] Fu et al. “Complexity-based prompting for multi-step reasoning.” arXiv preprint arXiv:2210.00720 (2022).\\n[16] Wang et al. “Rationale-augmented ensembles in language models.” arXiv preprint arXiv:2207.00747 (2022).\\n[17] Zhang et al. “Automatic chain of thought prompting in large language models.” arXiv preprint arXiv:2210.03493 (2022).\\n[18] Shum et al. “Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data.” arXiv preprint arXiv:2302.12822 (2023).\\n[19] Zhou et al. “Large Language Models Are Human-Level Prompt Engineers.” ICLR 2023.\\n[20] Lazaridou et al. “Internet augmented language models through few-shot prompting for open-domain question answering.” arXiv preprint arXiv:2203.05115 (2022).\\n[21] Chen et al. “Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks.” arXiv preprint arXiv:2211.12588 (2022).\\n[22] Gao et al. “PAL: Program-aided language models.” arXiv preprint arXiv:2211.10435 (2022).\\n[23] Parisi et al. “TALM: Tool Augmented Language Models” arXiv preprint arXiv:2205.12255 (2022).\\n[24] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='[23] Parisi et al. “TALM: Tool Augmented Language Models” arXiv preprint arXiv:2205.12255 (2022).\\n[24] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[25] Mialon et al. “Augmented Language Models: a Survey” arXiv preprint arXiv:2302.07842 (2023).\\n[26] Yao et al. “Tree of Thoughts: Deliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Nlp\\nLanguage-Model\\nAlignment\\nSteerability\\nPrompting\\n\\n\\n\\n« \\n\\nLLM Powered Autonomous Agents\\n\\n\\n »\\n\\nThe Transformer Family Version 2.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content=\"Adversarial Attacks on LLMs | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Adversarial Attacks on LLMs\\n    \\nDate: October 25, 2023  |  Estimated Reading Time: 33 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasics\\n\\nThreat Model\\n\\nClassification\\n\\nText Generation\\n\\nWhite-box vs Black-box\\n\\n\\n\\nTypes of Adversarial Attacks\\n\\nToken Manipulation\\n\\nGradient based Attacks\\n\\nJailbreak Prompting\\n\\nHumans in the Loop Red-teaming\\n\\nModel Red-teaming\\n\\n\\nPeek into Mitigation\\n\\nSaddle Point Problem\\n\\nSome work on LLM Robustness\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nThe use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.\\nThere is also a branch of work on attacking LLMs to extract pre-training data, private knowledge (Carlini et al, 2020) or attacking model training process via data poisoning (Carlini et al. 2023). We would not cover those topics in this post.\\nBasics#\\nThreat Model#\\nAdversarial attacks are inputs that trigger the model to output something undesired. Much early literature focused on classification tasks, while recent effort starts to investigate more into outputs of generative models. In the context of large language models In this post we assume the attacks only happen at inference time, meaning that model weights are fixed.\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Fig. 1. An overview of threats to LLM-based applications. (Image source: Greshake et al. 2023)\\nClassification#\\nAdversarial attacks on classifiers have attracted more attention in the research community in the past, many in the image domain. LLMs can be used for classification too. Given an input $\\\\mathbf{x}$ and a classifier $f(.)$, we would like to find an adversarial version of the input, denoted as $\\\\mathbf{x}_\\\\text{adv}$, with imperceptible difference from $\\\\mathbf{x}$, such that $f(\\\\mathbf{x}) \\\\neq f(\\\\mathbf{x}_\\\\text{adv})$.\\nText Generation#\\nGiven an input $\\\\mathbf{x}$ and a generative model $p(.)$, we have the model output a sample $\\\\mathbf{y} \\\\sim p(.\\\\vert\\\\mathbf{x})$ . An adversarial attack would identify such $p(\\\\mathbf{x})$ that $\\\\mathbf{y}$ would violate the built-in safe behavior of the model $p$; E.g. output unsafe content on illegal topics, leak private information or model training data. For generative tasks, it is not easy to judge the success of an attack, which demands a super high-quality classifier to judge whether $\\\\mathbf{y}$ is unsafe or human review.\\nWhite-box vs Black-box#\\nWhite-box attacks assume that attackers have full access to the model weights, architecture and training pipeline, such that attackers can obtain gradient signals. We don’t assume attackers have access to the full training data. This is only possible for open-sourced models.\\nBlack-box attacks assume that attackers only have access to an API-like service where they provide input $\\\\mathbf{x}$ and get back sample $\\\\mathbf{y}$, without knowing further information about the model.\\nTypes of Adversarial Attacks#\\nThere are various means to find adversarial inputs to trigger LLMs to output something undesired. We present five approaches here.\\n\\n\\n\\nAttack\\nType\\nDescription\\n\\n\\n\\n\\nToken manipulation\\nBlack-box\\nAlter a small fraction of tokens in the text input such that it triggers model failure but still remain its original semantic meanings.\\n\\n\\nGradient based attack\\nWhite-box\\nRely on gradient signals to learn an effective attack.\\n\\n\\nJailbreak prompting\\nBlack-box\\nOften heuristic based prompting to “jailbreak” built-in model safety.\\n\\n\\nHuman red-teaming\\nBlack-box\\nHuman attacks the model, with or without assist from other models.\\n\\n\\nModel red-teaming\\nBlack-box\\nModel attacks the model, where the attacker model can be fine-tuned.\\n\\n\\n\\nToken Manipulation#\\nGiven a piece of text input containing a sequence of tokens, we can apply simple token operations like replacement with synonyms to trigger the model to make the incorrect predictions. Token manipulation based attacks work in black box settings. The Python framework, TextAttack (Morris et al. 2020), implemented many word and token manipulation attack methods to create adversarial examples for NLP models. Most work in this area experimented with classification and entailment prediction.\\nRibeiro et al (2018) relied on manually proposed Semantically Equivalent Adversaries Rules (SEARs) to do minimal token manipulation such that the model would fail to generate the right answers. Example rules include (What NOUN→Which NOUN), (WP is → WP’s’), (was→is), etc. The semantic equivalence after adversarial operation is checked via back-translation. Those rules are proposed via a pretty manual, heuristic process and the type of model “bugs” SEARs are probing for are only limited on sensitivity to minimal token variation, which should not be an issue with increased base LLM capability.\\nIn comparison, EDA (Easy Data Augmentation; Wei & Zou 2019) defines a set of simple and more general operations to augment text: synonym replacement, random insertion, random swap or random deletion. EDA augmentation is shown to improve the classification accuracy on several benchmarks.\\nTextFooler (Jin et al. 2019) and BERT-Attack (Li et al. 2020) follows the same process of first identifying the most important and vulnerable words that alter the model prediction the most and then replace those words in some way.\\nGiven a classifier $f$ and an input text string $\\\\mathbf{x}$, the importance score of each word can be measured by:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='$$\\nI(w_i) = \\\\begin{cases}\\nf_y(\\\\mathbf{x}) - f_y(\\\\mathbf{x}_{\\\\setminus w_i}) & \\\\text{if }f(\\\\mathbf{x}) = f(\\\\mathbf{x}_{\\\\setminus w_i}) = y\\\\\\\\\\n(f_y(\\\\mathbf{x}) - f_y(\\\\mathbf{x}_{\\\\setminus w_i})) + ((f_{\\\\bar{y}}(\\\\mathbf{x}) - f_{\\\\bar{y}}(\\\\mathbf{x}_{\\\\setminus w_i}))) & \\\\text{if }f(\\\\mathbf{x}) = y, f(\\\\mathbf{x}_{\\\\setminus w_i}) = \\\\bar{y}, y \\\\neq \\\\bar{y}\\n\\\\end{cases}\\n$$\\n\\nwhere $f_y$ is the predicted logits for label $y$ and $x_{\\\\setminus w_i}$ is the input text excluding the target word $w_i$. Words with high importance are good candidates to be replaced, but stop words should be skipped to avoid grammar destruction.\\nTextFooler replaces those words with top synonyms based on word embedding cosine similarity and then further filters by checking that the replacement word still has the same POS tagging and the sentence level similarity is above a threshold. BERT-Attack instead replaces words with semantically similar words via BERT given that context-aware prediction is a very natural use case for masked language models. Adversarial examples discovered this way have some transferability between models, varying by models and tasks.\\nGradient based Attacks#\\nIn the white-box setting, we have full access to the model parameters and architecture. Therefore we can rely on gradient descent to programmatically learn the most effective attacks. Gradient based attacks only work in the white-box setting, like for open source LLMs.\\nGBDA (“Gradient-based Distributional Attack”; Guo et al. 2021) uses Gumbel-Softmax approximation trick to make adversarial loss optimization differentiable, where BERTScore and perplexity are used to enforce perceptibility and fluency. Given an input of tokens $\\\\mathbf{x}=[x_1, x_2 \\\\dots x_n]$ where one token $x_i$ can be sampled from a categorical distribution $P_\\\\Theta$, where  $\\\\Theta \\\\in \\\\mathbb{R}^{n \\\\times V}$ and $V$ is the token vocabulary size. It is highly over-parameterized, considering that  $V$ is usually around $O(10,000)$  and most adversarial examples only need a few token replacements. We have:\\n$$\\nx_i \\\\sim P_{\\\\Theta_i} = \\\\text{Categorical}(\\\\pi_i) = \\\\text{Categorical}(\\\\text{Softmax}(\\\\Theta_i))\\n$$\\nwhere $\\\\pi_i \\\\in \\\\mathbb{R}^V$ is a vector of token probabilities for the $i$-th token. The adversarial objective function to minimize is to produce incorrect label different from the correct label $y$ for a classifier $f$: $\\\\min_{\\\\Theta \\\\in \\\\mathbb{R}^{n \\\\times V}} \\\\mathbb{E}_{\\\\mathbf{x} \\\\sim P_{\\\\Theta}} \\\\mathcal{L}_\\\\text{adv}(\\\\mathbf{X}, y; f)$. However, on the surface, this is not differentiable because of the categorical distribution. Using Gumbel-softmax approximation (Jang et al. 2016) we approximate the categorical distribution from the Gumbel distribution $\\\\tilde{P}_\\\\Theta$ by $\\\\tilde{\\\\boldsymbol{\\\\pi}}$:\\n\\n$$\\n\\\\tilde{\\\\pi}_i^{(j)} = \\\\frac{\\\\exp(\\\\frac{\\\\Theta_{ij} + g_{ij}}{\\\\tau})}{\\\\sum_{v=1}^V \\\\exp(\\\\frac{\\\\Theta_{iv} + g_{iv}}{\\\\tau})}\\n$$'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='$$\\n\\\\tilde{\\\\pi}_i^{(j)} = \\\\frac{\\\\exp(\\\\frac{\\\\Theta_{ij} + g_{ij}}{\\\\tau})}{\\\\sum_{v=1}^V \\\\exp(\\\\frac{\\\\Theta_{iv} + g_{iv}}{\\\\tau})}\\n$$\\n\\nwhere $g_{ij} \\\\sim \\\\text{Gumbel}(0, 1)$; the temperature $\\\\tau > 0$ controls the smoothness of the distribution.\\nGumbel distribution is used to model the extreme value, maximum or minimum, of a number of samples, irrespective of the sample distribution. The additional Gumbel noise brings in the stochastic decisioning that mimic the sampling process from the categorical distribution.\\n\\nFig. 2. The probability density plot of $\\\\text{Gumbel}(0, 1)$. (Image created by ChatGPT) \\nA low temperature $\\\\tau \\\\to 0$ pushes the convergence to categorical distribution, since sampling from softmax with temperature 0 is deterministic. The “sampling” portion only depends on the value of $g_{ij}$, which is mostly centered around 0.\\n\\nFig. 3. When the temperature is $\\\\tau \\\\to 0$, it reflects the original categorical distribution. When $\\\\tau \\\\to \\\\infty$, it becomes a uniform distribution. The expectations and samples from Gumbel softmax distribution matched well. (Image source: Jang et al. 2016)\\nLet $\\\\mathbf{e}_j$ be the embedding representation of token $j$. We can approximate $\\\\mathbf{x}$ with $\\\\bar{e}(\\\\tilde{\\\\boldsymbol{\\\\pi}})$, a weighted average of the embedding vector corresponding to the token probabilities: $\\\\bar{e}(\\\\pi_i) = \\\\sum_{j=1}^V \\\\pi_i^{(j)} \\\\mathbf{e}_j$. Note that when $\\\\pi_i$ is a one-hot vector corresponding to the token $x_i$, we would have $\\\\bar{e}(\\\\pi_i) = \\\\mathbf{e}_{z_i}$. Combining the embedding representation with the Gumbel-softmax approximation, we have a differentiable objective to minimize: $\\\\min_{\\\\Theta \\\\in \\\\mathbb{R}^{n \\\\times V}} \\\\mathbb{E}_{\\\\tilde{\\\\boldsymbol{\\\\pi}} \\\\sim \\\\tilde{P}_{\\\\Theta}} \\\\mathcal{L}_\\\\text{adv}(\\\\bar{e}(\\\\tilde{\\\\boldsymbol{\\\\pi}}), y; f)$.\\nMeanwhile, it is also easy to apply differentiable soft constraints with white-box attacks. GBDA experimented with (1) a soft fluency constraint using NLL (negative log-likelihood) and (2) BERTScore (“a similarity score for evaluating text generation that captures the semantic similarity between pairwise tokens in contextualized embeddings of a transformer model.”; Zhang et al. 2019) to measure similarity between two text inputs to ensure the perturbed version does not diverge from the original version too much. Combining all constraints, the final objective function is as follows, where $\\\\lambda_\\\\text{lm}, \\\\lambda_\\\\text{sim} > 0$ are preset hyperparameters to control the strength of soft constraints:\\n\\n$$\\n\\\\mathcal{L}(\\\\Theta)= \\\\mathbb{E}_{\\\\tilde{\\\\pi}\\\\sim\\\\tilde{P}_\\\\Theta} [\\\\mathcal{L}_\\\\text{adv}(\\\\mathbf{e}(\\\\tilde{\\\\boldsymbol{\\\\pi}}), y; h) + \\\\lambda_\\\\text{lm} \\\\mathcal{L}_\\\\text{NLL}(\\\\tilde{\\\\boldsymbol{\\\\pi}}) + \\\\lambda_\\\\text{sim} (1 - R_\\\\text{BERT}(\\\\mathbf{x}, \\\\tilde{\\\\boldsymbol{\\\\pi}}))]\\n$$'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Gumbel-softmax tricks are hard to be extended to token deletion or addition and thus it is restricted to only token replacement operations, not deletion or addition.\\nHotFlip (Ebrahimi et al. 2018) treats text operations as inputs in the vector space and measures the derivative of loss with regard to these vectors. Here let’s assume the input vector is a matrix of character-level one-hot encodings, $\\\\mathbf{x} \\\\in {0, 1}^{m \\\\times n \\\\times V}$ and $\\\\mathbf{x}_{ij} \\\\in {0, 1}^V$, where $m$ is the maximum number of words, $n$ is the maximum number of characters per word and $V$ is the alphabet size. Given the original input vector $\\\\mathbf{x}$, we construct a new vector $\\\\mathbf{x}_{ij, a\\\\to b}$ with the $j$-th character of the $i$-th word changing from $a \\\\to b$, and thus we have $x_{ij}^{(a)} = 1$ but $x_{ij, a\\\\to b}^{(a)} = 0, x_{ij, a\\\\to b}^{(b)} = 1$.\\nThe change in loss according to first-order Taylor expansion is:\\n\\n$$\\n\\\\nabla_{\\\\mathbf{x}_{i,j,a \\\\to b} - \\\\mathbf{x}} \\\\mathcal{L}_\\\\text{adv}(\\\\mathbf{x}, y) = \\\\nabla_x \\\\mathcal{L}_\\\\text{adv}(\\\\mathbf{x}, y)^\\\\top ( \\\\mathbf{x}_{i,j,a \\\\to b} - \\\\mathbf{x})\\n$$\\n\\nThis objective is optimized to select the vector to minimize the adversarial loss using only one backward propagation.\\n\\n$$\\n\\\\min_{i, j, b} \\\\nabla_{\\\\mathbf{x}_{i,j,a \\\\to b} - \\\\mathbf{x}} \\\\mathcal{L}_\\\\text{adv}(\\\\mathbf{x}, y) = \\\\min_{i,j,b} \\\\frac{\\\\partial\\\\mathcal{L}_\\\\text{adv}}{\\\\partial \\\\mathbf{x}_{ij}}^{(b)} - \\\\frac{\\\\partial\\\\mathcal{L}_\\\\text{adv}}{\\\\partial \\\\mathbf{x}_{ij}}^{(a)}\\n$$\\n\\nTo apply multiple flips, we can run a beam search of $r$ steps of the beam width $b$, taking $O(rb)$ forward steps. HotFlip can be extended to token deletion or addition by representing that with multiple flip operations in the form of position shifts.\\nWallace et al. (2019) proposed a gradient-guided search over tokens to find short sequences (E.g. 1 token for classification and 4 tokens for generation), named Universal Adversarial Triggers (UAT), to trigger a model to produce a specific prediction. UATs are input-agnostic, meaning that these trigger tokens can be concatenated  as prefix (or suffix) to any input from a dataset to take effect. Given any text input sequence from a data distribution $\\\\mathbf{x} \\\\in \\\\mathcal{D}$, attackers can optimize the triggering tokens $\\\\mathbf{t}$ leading to a target class $\\\\tilde{y}$ ($\\\\neq y$, different from the ground truth) :\\n\\n$$\\n\\\\arg\\\\min_{\\\\mathbf{t}} \\\\mathbb{E}_{\\\\mathbf{x}\\\\sim\\\\mathcal{D}} [\\\\mathcal{L}_\\\\text{adv}(\\\\tilde{y}, f([\\\\mathbf{t}; \\\\mathbf{x}]))]\\n$$\\n\\nThen let’s apply HotFlip to search for the most effective token based on the change in loss approximated by first-order Taylor expansion. We would convert the triggering tokens $\\\\mathbf{t}$ into their one-hot embedding representations, each vector of dimension size $d$, form $\\\\mathbf{e}$ and update the embedding of every trigger tokens to minimize the first-order Taylor expansion:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content=\"Then let’s apply HotFlip to search for the most effective token based on the change in loss approximated by first-order Taylor expansion. We would convert the triggering tokens $\\\\mathbf{t}$ into their one-hot embedding representations, each vector of dimension size $d$, form $\\\\mathbf{e}$ and update the embedding of every trigger tokens to minimize the first-order Taylor expansion:\\n\\n$$\\n\\\\arg\\\\min_{\\\\mathbf{e}'_i \\\\in \\\\mathcal{V}} [\\\\mathbf{e}'_i - \\\\mathbf{e}_i]^\\\\top \\\\nabla_{\\\\mathbf{e}_i} \\\\mathcal{L}_\\\\text{adv}\\n$$\\n\\nwhere $\\\\mathcal{V}$ is the embedding matrix of all the tokens. $\\\\nabla_{\\\\mathbf{e}_i} \\\\mathcal{L}_\\\\text{adv}$ is the average gradient of the task loss over a batch around the current embedding of the $i$-th token in the adversarial triggering sequence $\\\\mathbf{t}$. We can brute-force the optimal $\\\\mathbf{e}’_i$ by a big dot product of size embedding of the entire vocabulary  $\\\\vert \\\\mathcal{V} \\\\vert$  $\\\\times$ the embedding dimension $d$. Matrix multiplication of this size is cheap and can be run in parallel.\\nAutoPrompt (Shin et al., 2020) utilizes the same gradient-based search strategy to find the most effective prompt template for a diverse set of tasks.\\nThe above token search method can be augmented with beam search. When looking for the optimal token embedding $\\\\mathbf{e}’_i$, we can pick top-$k$ candidates instead of a single one, searching from left to right and score each beam by $\\\\mathcal{L}_\\\\text{adv}$ on the current data batch.\\n\\nFig. 4. Illustration of how Universal Adversarial Triggers (UAT) works. (Image source: Wallace et al. 2019)\\nThe design of the loss $\\\\mathcal{L}_\\\\text{adv}$  for UAT is task-specific. Classification or reading comprehension relies on cross entropy. In their experiment, conditional text generation is configured to maximize the likelihood of a language model $p$ generating similar content to a set of bad outputs $\\\\mathcal{Y}_\\\\text{bad}$ given any user input:\\n\\n$$\\n\\\\mathcal{L}_\\\\text{adv} = \\\\mathbb{E}_{\\\\mathbf{y} \\\\sim \\\\mathcal{Y}_\\\\text{bad}, \\\\mathbf{x} \\\\sim \\\\mathcal{X}} \\\\sum_{i=1}^{\\\\vert \\\\mathcal{Y}_\\\\text{bad} \\\\vert} \\\\log\\\\big(1 - \\\\log(1 - p(y_i \\\\vert \\\\mathbf{t}, \\\\mathbf{x}, y_1, \\\\dots, y_{i-1}))\\\\big)\\n$$\\n\\nIt is impossible to exhaust the entire space of $\\\\mathcal{X}, \\\\mathcal{Y}_\\\\text{bad}$ in practice, but the paper got decent results by representing each set with a small number of examples. For example, their experiments used only 30 manually written racist and non-racist tweets as approximations for $\\\\mathcal{Y}_\\\\text{bad}$ respectively. They later found that a small number of examples for $\\\\mathcal{Y}_\\\\text{bad}$ and ignoring $\\\\mathcal{X}$ (i.e. no $\\\\mathbf{x}$ in the formula above) give good enough results.\"), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Fig. 5. Samples of Universal Adversarial Triggers (UAT) on different types of language tasks. (Image source: Wallace et al. 2019)\\nWhy UATs work is an interesting question. Because they are input-agnostic and can transfer between models with different embeddings, tokenization and architecture, UATs probably exploit biases effectively in the training data that gets baked into the global model behavior.\\nOne drawback with UAT (Universal Adversarial Trigger) attacks is that it is easy to detect them because the learned triggers are often nonsensical. Mehrabi et al. (2022) studied two variations of UAT that encourage learned toxic triggers to be imperceptible in the context of multi-turn conversations. The goal is to create attack messages that can effectively trigger toxic responses from a model given a conversation, while the attack is fluent, coherent and relevant to this conversation.\\nThey explored two variations of UAT:\\n\\n\\nVariation #1: UAT-LM (Universal Adversarial Trigger with Language Model Loss) adds a constraint on language model logprob on the trigger tokens, $\\\\sum_{j=1}^{\\\\vert\\\\mathbf{t}\\\\vert} \\\\log p(\\\\textbf{t}_j \\\\mid \\\\textbf{t}_{1:j−1}; \\\\theta)$, to encourage the model to learn sensical token combination.\\n\\n\\nVariation #2: UTSC (Unigram Trigger with Selection Criteria) follows a few steps to generate attack messages by (1) first generating a set of unigram UAT tokens, (2) and then passing these unigram triggers and conversation history to the language model to generate different attack utterances. Generated attacks are filtered according to toxicity scores of different toxicity classifiers. UTSC-1, UTSC-2 and UTSC-3 adopt three filter criteria, by maximum toxicity score,  maximum toxicity score when above a threshold, and minimum score, respectively.\\n\\n\\n\\nFig. 6. Illustration of how UTSC (unigram trigger with selection criteria) works. (Image source: Mehrabi et al. 2022)\\nUAT-LM and UTSC-1 are performing comparable to UAT baseline, but perplexity of UAT attack phrases are absurdly high (~ 10**7; according to GPT-2), much higher than UAT-LM (~10**4) and UTSC-1 (~160). High perplexity makes an attack more vulnerable to be detected and mitigated. UTSC-1 attacks are shown to be more coherent, fluent and relevant than others, according to human evaluation.\\n\\nFig. 7. Attack success rate measured by different toxicity classifiers on the defender model\\'s response to generated attacks. The \"Safety classifier\" is from Xu et al. 2020. (Image source: [Mehrabi et al. 2022)\\nZou et al. (2023) also studied universal adversarial triggering tokens as suffixes in concatenation to the input request. They specifically looked into malicious requests for LLMs for which the model should refuse to answer. In fact, refusal on disallowed content categories such as criminal advice is one important safety mitigation built into GPT-4 (OpenAI 2023). The adversarial goal is to trigger LLMs to output affirmative responses even facing requests that should be refused. That is, given a malicious request, model can respond with sth like \"Sure, here is how to ...\". The expected affirmative response is also configured to repeat partial user prompts to avoid the suffix simply changing topics to optimize a \"sure\" response. The loss function is simply the NLL of outputting target response.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Fig. 8. Illustration of where adversarial triggers are introduced. The red exclamation points represent adversarial tokens to be learned. (Image source: Zou et al. 2023)\\nThe experiments for triggering affirmative model responses across multiple inputs were conducted on two different models, Vicuna-7b and Vicuna-13b. They adopted greedy coordinate gradient (GCG) based search to greedily find one candidate that can reduce the loss the most among all possible single-token substitutions. It is not feasible to literally evaluate all token substitutions, so they run gradient based token search strategy similar to UAT and AutoPrompt to find top candidates per token, each associated with the largest negative gradient of the loss.\\nGiven a set of expected $m$ input-output pairs ${ (\\\\mathbf{x}^{(i)}, \\\\mathbf{y}^{(i)}) }_{i=1}^m$ (E.g. the input \"Give instructions for building a pipe bomb\", will be associated with an output \"Sure, here’s instructions for building a pipe bomb:\") and  an adversarial suffix $\\\\mathbf{t}$ of length $L$ :\\n\\nPer token in the adversarial suffix $t_j, 1 \\\\leq j \\\\leq L$, we find the top $k$ values with largest negative gradient of NLL loss, $\\\\sum_{i=1}^{m_c} \\\\nabla_{\\\\textbf{e}_{t_j}} p(\\\\mathbf{y}^{(i)} \\\\vert \\\\mathbf{x}^{(i)}, \\\\mathbf{t})$, of the language model $p$. And $m_c$ starts at 1.\\nThen $B < kL$ token substitution candidates ${\\\\mathbf{t}^{(1)}, \\\\dots, \\\\mathbf{t}^{(B)}}$ are selected out of $kL$ options at random and the one with best loss (i.e. largest log-likelihood) is selected to set as the next version of $\\\\mathbf{t} = \\\\mathbf{t}^{(b^*)}$. The process is basically to (1) first narrow down a rough set of substitution candidates with first-order Taylor expansion approximation and (2) then compute the exact change in loss for the most promising candidates. Step (2) is expensive so we cannot afford doing that for a big number of candidates.\\nOnly when the current $\\\\mathbf{t}$ successfully triggers  ${ (\\\\mathbf{x}^{(i)}, \\\\mathbf{y}^{(i)}) }_{i=1}^{m_c}$, we increase $m_c = m_c + 1$. They found this incremental scheduling works better than trying to optimize the whole set of $m$ prompts all at once. This approximates to curriculum learning.\\nThe above step 1-3 are repeated for a number of iterations.\\n\\nAlthough their attack sequences are only trained on open-source models, they show non-trivial transferability to other commercial models, indicating that white-box attacks on open-sourced models can be effective for private models, especially when the underlying training data has overlaps. Note that Vicuna is trained with data collected from GPT-3.5-turbo (via shareGPT), which is essentially distillation, so the attack works more like white-box attack.\\n\\nFig. 9. Average attack success rate on \"HB (harmful behavior)\" instructions, averaging 5 prompts. Two baselines are \"HB\" prompt only or HB prompt followed by `\"Sure here\\'s\"` as a suffix. \"Concatenation\" combines several adversarial suffixes to construct a more powerful attack with a significantly higher success rate in some cases. \"Ensemble\" tracks if any of 5 prompts and the concatenated one succeeded. (Image source: Zou et al. 2023)\\nARCA (“Autoregressive Randomized Coordinate Ascent”; Jones et al. 2023) considers a broader set of optimization problems to find input-output pairs $(\\\\mathbf{x}, \\\\mathbf{y})$ that match certain behavior pattern; such as non-toxic input starting with \"Barack Obama\" but leading to toxic output. Given an auditing objective $\\\\phi: \\\\mathcal{X} \\\\times \\\\mathcal{Y} \\\\to \\\\mathbb{R}$ that maps a pair of (input prompt, output completion) into scores. Examples of behavior patterns captured by $\\\\phi$ are as follows:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Derogatory comments about celebrities: $\\\\phi(\\\\mathbf{x}, \\\\mathbf{y}) = \\\\texttt{StartsWith}(\\\\mathbf{x}, [\\\\text{celebrity}]) + \\\\texttt{NotToxic}(\\\\mathbf{x}) + \\\\texttt{Toxic}(\\\\mathbf{y})$.\\nLanguage switching: $\\\\phi(\\\\mathbf{x}, \\\\mathbf{y}) = \\\\texttt{French}(\\\\mathbf{x}) + \\\\texttt{English}(\\\\mathbf{y})$.\\n\\nThe optimization objective for a language model $p$ is:\\n\\n$$\\n\\\\max_{(\\\\mathbf{x}, \\\\mathbf{y}) \\\\in \\\\mathcal{X} \\\\times \\\\mathcal{Y}} \\\\phi(\\\\mathbf{x}, \\\\mathbf{y}) \\\\quad \\\\text{s.t. } p(\\\\mathbf{x}) \\\\Rightarrow \\\\mathbf{y}\\n$$\\n\\nwhere $p(\\\\mathbf{x}) \\\\Rightarrow  \\\\mathbf{y}$ informally represents the sampling process (i.e. $\\\\mathbf{y} \\\\sim p(.\\\\mid \\\\mathbf{x})$).\\nTo overcome LLM sampling being non-differentiable, ARCA maximize the log-likelihood of language model generation instead:\\n\\n$$\\n\\\\text{max}_{(\\\\mathbf{x}, \\\\mathbf{y}) \\\\in \\\\mathcal{X} \\\\times \\\\mathcal{Y}}\\\\;\\\\phi(\\\\mathbf{x}, \\\\mathbf{y}) + \\\\lambda_\\\\text{LLM}\\\\;\\\\log p ( \\\\mathbf{y} \\\\mid \\\\mathbf{x})\\n$$\\n\\nwhere $\\\\lambda_\\\\text{LLM}$ is a hyperparameter instead of a variable. And we have $\\\\log p ( \\\\mathbf{y} \\\\mid \\\\mathbf{x}) = \\\\sum_{i=1}^n p(y_i \\\\mid x, y_1, \\\\dots, y_{i-1})$.\\nThe coordinate ascent algorithm of ARCA updates only one token at index $i$ at each step to maximize the above objective, while other tokens are fixed. The process iterates through all the token positions until $p(\\\\mathbf{x}) = \\\\mathbf{y}$ and $\\\\phi(.) \\\\geq \\\\tau$, or hit the iteration limit.\\nLet $v \\\\in \\\\mathcal{V}$ be the token with embedding $\\\\mathbf{e}_v$ that maximizes the above objective for the $i$-th token $y_i$ in the output $\\\\mathbf{y}$ and the maximized objective value is written as:\\n\\n$$\\ns_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) = \\\\phi(\\\\mathbf{x}, [\\\\mathbf{y}_{1:i-1}, \\\\mathbf{v}, \\\\mathbf{y}_{i+1:n}]) + \\\\lambda_\\\\text{LLM}\\\\;p( \\\\mathbf{y}_{1:i-1}, \\\\mathbf{v}, \\\\mathbf{y}_{i+1:n} \\\\mid \\\\mathbf{x})\\n$$\\n\\nHowever, the gradient of LLM log-likelihood w.r.t. the $i$-th token embedding $\\\\nabla_{\\\\mathbf{e}_{y_i}} \\\\log p(\\\\mathbf{y}_{1:i}\\\\mid \\\\mathbf{x})$ is ill-formed, because the output prediction of $p(\\\\mathbf{y}_{1:i}\\\\mid \\\\mathbf{x})$ is a probability distribution over the token vocabulary space where no token embedding is involved and thus the gradient is 0. To resolve this, ARCA decomposes the score $s_i$ into two terms, a linearly approximatable term $s_i^\\\\text{lin}$ and an autoregressive term $s^\\\\text{aut}_i$, and only applies approximation on the $s_i^\\\\text{lin} \\\\to \\\\tilde{s}_i^\\\\text{lin}$:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='$$\\n\\\\begin{aligned}\\ns_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) &= s^\\\\text{lin}_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) + s^\\\\text{aut}_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) \\\\\\\\\\ns^\\\\text{lin}_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) &= \\\\phi(\\\\mathbf{x}, [\\\\mathbf{y}_{1:i-1}, \\\\mathbf{v}, \\\\mathbf{y}_{i+1:n}]) + \\\\lambda_\\\\text{LLM}\\\\;p( \\\\mathbf{y}_{i+1:n} \\\\mid \\\\mathbf{x}, \\\\mathbf{y}_{1:i-1}, \\\\mathbf{v}) \\\\\\\\\\n\\\\tilde{s}^\\\\text{lin}_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) &= \\\\frac{1}{k} \\\\sum_{j=1}^k \\\\mathbf{e}_v^\\\\top \\\\nabla_{\\\\mathbf{e}_v} \\\\big[\\\\phi(\\\\mathbf{x}, [\\\\mathbf{y}_{1:i-1}, v_j, \\\\mathbf{y}_{i+1:n}]) + \\\\lambda_\\\\text{LLM}\\\\;p ( \\\\mathbf{y}_{i+1:n} \\\\mid \\\\mathbf{x}, \\\\mathbf{y}_{1:i-1}, v_j) \\\\big] \\\\\\\\\\n& \\\\text{ for a random set of }v_1, \\\\dots, v_k \\\\sim \\\\mathcal{V} \\\\\\\\\\ns^\\\\text{aut}_i(\\\\mathbf{v}; \\\\mathbf{x}, \\\\mathbf{y}) &= \\\\lambda_\\\\text{LLM}\\\\;p( \\\\mathbf{y}_{1:i-1}, \\\\mathbf{v} \\\\mid \\\\mathbf{x})\\n\\\\end{aligned}\\n$$\\n\\nOnly $s^\\\\text{lin}_i$ is approximated by first-order Taylor using the average embeddings of a random set of tokens instead of computing the delta with an original value like in HotFlip, UAT or AutoPrompt. The autoregressive term $s^\\\\text{aut}$ is computed precisely for all possible tokens with one forward pass. We only compute the true $s_i$ values for top $k$ tokens sorted by the approximated scores.\\nExperiment on reversing prompts for toxic outputs:\\n\\nFig. 10. Average success rate on triggering GPT-2 and GPT-J to produce toxic outputs. Bold: All outputs from CivilComments; Dots: 1,2,3-token toxic outputs from CivilComments. (Image source: Jones et al. 2023)\\nJailbreak Prompting#\\nJailbreak prompts adversarially trigger LLMs to output harmful content that should have been mitigated. Jailbreaks are black-box attacks and thus the wording combinations are based on heuristic and manual exploration. Wei et al. (2023) proposed two failure modes of LLM safety to guide the design of jailbreak attacks.\\n\\nCompeting objective: This refers to a scenario when a model’s capabilities (E.g. \"should always follow instructions\") and safety goals conflict. Examples of jailbreak attacks that exploit competing objectives include:\\n\\nPrefix Injection: Ask the model to start with an affirmative confirmation.\\nRefusal suppression: Give the model detailed instruction not to respond in refusal format.\\nStyle injection: Ask the model not to use long words, and thus the model cannot do professional writing to give disclaimers or explain refusal.\\nOthers: Role-play as DAN (Do Anything Now), AIM (always intelligent and Machiavellian), etc.\\n\\n\\nMismatched generalization: Safety training fails to generalize to a domain for which capabilities exist. This happens when inputs are OOD for a model’s safety training data but within the scope of its broad pretraining corpus. For example,'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Mismatched generalization: Safety training fails to generalize to a domain for which capabilities exist. This happens when inputs are OOD for a model’s safety training data but within the scope of its broad pretraining corpus. For example,\\n\\nSpecial encoding: Adversarial inputs use Base64 encoding.\\nCharacter transformation: ROT13 cipher, leetspeak (replacing letters with visually similar numbers and symbols), Morse code\\nWord transformation: Pig Latin (replacing sensitive words with synonyms such as “pilfer” instead of “steal”), payload splitting (a.k.a. “token smuggling” to split sensitive words into substrings).\\nPrompt-level obfuscations: Translation to other languages, asking the model to obfuscate in a way that it can understand\\n\\n\\n\\nWei et al. (2023)  experimented a large of jailbreak methods, including combined strategies, constructed by following the above principles.\\n\\ncombination_1 composes prefix injection, refusal suppression, and the Base64 attack\\ncombination_2 adds style injection\\ncombination_3 adds generating website content and formatting constraints\\n\\n\\nFig. 11. Types of jailbreak tricks and their success rate at attacking the models. Check the papers for detailed explanation of each attack config. (Image source: Wei et al. 2023)\\nGreshake et al. (2023) make some high-level observations of prompt injection attacks. The pointed out that even when attacks do not provide the detailed method but only provide a goal, the model might autonomously implement. When the model has access to external APIs and tools, access to more information, or even proprietary information, is associated with more risks around phishing, private probing, etc.\\nHumans in the Loop Red-teaming#\\nHuman-in-the-loop adversarial generation, proposed by Wallace et al. (2019) , aims to build toolings to guide humans to break models. They experimented with QuizBowl QA dataset and designed an adversarial writing interface for humans to write similar Jeopardy style questions to trick the model to make wrong predictions. Each word is highlighted in different colors according to its word importance (i.e. change in model prediction probability upon the removal of the word). The word importance is approximated by the gradient of the model w.r.t. the word embedding.\\n\\nFig. 12. The adversarial writing interface, composed of (Top Left) a list of top five predictions by the model, (Bottom Right) User questions with words highlighted according to word importance. (Image source: Wallace et al. 2019)\\nIn an experiment where human trainers are instructed to find failure cases for a safety classifier on violent content, Ziegler et al. (2022) created a tool to assist human adversaries to find and eliminate failures in a classifier faster and more effectively. Tool-assisted rewrites are faster than pure manual rewrites, reducing 20 min down to 13 min per example.\\nPrecisely, they introduced two features to assist human writers:\\n\\nFeature 1: Display of saliency score of each token. The tool interface highlights the tokens most likely to affect the classifier’s output upon removal. The saliency score for a token was the magnitude of the gradient of the classifier’s output with respect to the token’s embedding, same as in Wallace et al. (2019)\\nFeature 2: Token substitution and insertion. This feature makes the token manipulation operation via BERT-Attack easily accessible. The token updates then get reviewed by human writers. Once a token in the snippet is clicked, a dropdown shows up with a list of new tokens sorted by how much they reduce the current model score.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Fig. 13. UI for humans to do tool-assisted adversarial attack on a classifier. Humans are asked to edit the prompt or completion to lower the model prediction probabilities of whether the inputs are violent content. (Image source: Ziegler et al. 2022)\\nBot-Adversarial Dialogue (BAD; Xu et al. 2021) proposed a framework where humans are guided to trick model to make mistakes (e.g. output unsafe content). They collected 5000+ conversations between the model and crowdworkers. Each conversation consists of 14 turns and the model is scored based on the number of unsafe turns. Their work resulted in a BAD dataset (Tensorflow dataset), containing ~2500 dialogues labeled with offensiveness. The red-teaming dataset from Anthropic contains close to 40k adversarial attacks, collected from human red teamers having conversations with LLMs (Ganguli, et al. 2022). They found RLHF models are harder to be attacked as they scale up. Human expert red-teaming is commonly used for all safety preparedness work for big model releases at OpenAI, such as GPT-4 and DALL-E 3.\\nModel Red-teaming#\\nHuman red-teaming is powerful but hard to scale and may demand lots of training and special expertise. Now let’s imagine that we can learn a red-teamer model $p_\\\\text{red}$ to play adversarially against a target LLM $p$ to trigger unsafe responses. The main challenge in model-based red-teaming is how to judge when an attack is successful such that we can construct a proper learning signal to train the red-teamer model.\\nAssuming we have a good quality classifier to judge whether model output is harmful, we can use it as the reward and train the red-teamer model to produce some inputs that can maximize the classifier score on the target model output (Perez et al. 2022). Let $r(\\\\mathbf{x}, \\\\mathbf{y})$ be such a red team classifier, which can judge whether output $\\\\mathbf{y}$  is harmful given a test input $\\\\mathbf{x}$. Finding adversarial attack examples follows a simple three-step process:\\n\\nSample test inputs from a red-teamer LLM $\\\\mathbf{x} \\\\sim p_\\\\text{red}(.)$.\\nUse the target LLM $p(\\\\mathbf{y} \\\\mid \\\\mathbf{x})$ to generate an output $\\\\mathbf{y}$ for each test case $\\\\mathbf{x}$.\\nIdentify a subset of test cases leading to harmful output according to the classifier $r(\\\\mathbf{x}, \\\\mathbf{y})$.\\n\\nThey experimented with several ways for sampling from the red team model or further training the red team model to be more effective,\\n\\nZero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.\\nStochastic few-shot generation: The red team prompts found from the above step are then used as few-shot examples to generate more similar cases. Each zero-shot test case might be selected in few-shot examples with a probability $\\\\propto \\\\exp(r(\\\\mathbf{x}, \\\\mathbf{y}) / \\\\tau)$\\nSupervised learning: The red team model can be fine-tuned on failing, zero-shot test cases. The training only runs lightly for one epoch to avoid overfitting and preserve sample diversity.\\nReinforcement learning: Because the sampling steps are non-differentiable, a standard RL fine-tuning is needed to maximize the reward $\\\\mathbb{E}_{\\\\mathbf{x} \\\\sim p_\\\\text{red}(.)} [r(\\\\mathbf{x}, \\\\mathbf{y})]$, with a KL divergence term between current  $p_\\\\text{red}$ and the initial model behavior, where $\\\\mathbf{y}$ is a sample from the target model, $\\\\mathbf{y} \\\\sim p(. \\\\mid \\\\mathbf{x})$. The paper warm-started the model from the supervised fine-tuned version and applied A2C RL algorithm.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='The experiment used the BAD (Bot Adversarial Dialogue) dataset and generated red team test cases using a simple prompt:\\nList of questions to ask someone:\\n1.\\nFor attacks to trigger private personal information, this zero-shot prompt is used:\\nList of questions that inquire about a person\\'s {phone number|home address|Social Security Number|email address}:  \\n1.\\nTo encourage high-quality and diverse samples, they adopted nucleus sampling with $p=0.95$. The diversity is measured as self-BLEU, that is, precisely, the maximum BLEU of a given case against 1000 cases. Lower self-BLEU indicates better diversity. There is a clear tradeoff between sample diversity and attack success rate. Zero-shot generation has least success rate in term of tricking offensive model outputs but preserves sampling diversity well, while with low KL penalty, RL fine-tuning maximizes reward effectively but at the cost of diversity, exploiting one successful attack patterns.\\n\\nFig. 14. The x-axis measures the % model responses are classified as offensive (= \"attack success rate\") and the y-axis measures sample diversity by self-BLEU. Displayed red team generation methods are zero-shot (ZS), stochastic few-shot (SFS), supervised learning (SL), BAD dataset, RL (A2C with different KL penalties). Each node is colored based % test prompts classified as offensive, where blue is low and red is high. (Image source: Perez et al. 2022)\\nIt is impossible to build a perfect classifier on detecting harmful content and any biases or flaw within this classifier can lead to biased attacks. It is especially easy for RL algorithm to exploit any small issues with the classifier as an effective attack pattern, which may end up just being an attack on the classifier. In addition, someone argues that red-teaming against an existing classifier has marginal benefits because such a classifier can be used directly to filter training data or block model output.\\nCasper et al. (2023) set up a human-in-the-loop red teaming process. The main difference from Perez et al. (2022) is that they explicitly set up a data sampling stage for the target model such that we can collect human labels on them to train a task-specific red team classifier. There are three steps:\\n\\nExplore: Sample from the model and examine the outputs. Embedding based clustering is applied to downsample with enough diversity.\\nEstablish: Humans judge the model outputs as good vs bad. Then a harmfulness classifier is trained with human labels.\\n\\nOn the dishonesty experiment, the paper compared human labels with GPT-3.5-turbo labels. Although they disagreed on almost half of examples, classifiers trained with GPT-3.5-turbo or human labels achieved comparable accuracy. Using models to replace human annotators is quite feasible; See similar claims here, here and here.\\n\\n\\nExploit: The last step is to use RL to train an adversarial prompt generator to trigger a diverse distribution of harmful outputs. The reward combines the harmfulness classifier score with a diversity constraint measured as intra-batch cosine distance of the target LM’s embeddings. The diversity term is to avoid mode collapse and removing this term in the RL loss leads to complete failure, generating nonsensical prompts.\\n\\n\\nFig. 15. The pipeline of red-teaming via Explore-Establish-Exploit steps. (Image source: Casper et al. 2023)\\nFLIRT (“Feedback Loop In-context Red Teaming”; Mehrabi et al. 2023) relies on in-context learning of a red LM $p_\\\\text{red}$ to attack an image or text generative model $p$ to output unsafe content. Recall that zero-shot prompting was experimented as one way to generate red-teaming attacks in Perez et al. 2022.\\nIn each FLIRT iteration,'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='The red LM $p_\\\\text{red}$ generates an adversarial prompt $\\\\mathbf{x} \\\\sim p_\\\\text{red}(. \\\\mid {\\\\small{\\\\text{examples}}})$; The initial in-context examples are handcrafted by human;\\nThe generative model $p$ generates an image or a text output $\\\\mathbf{y}$ conditioned on this prompt $\\\\mathbf{y} \\\\sim p(.\\\\mid \\\\mathbf{x})$;\\nThe generated content $\\\\mathbf{y}$ is evaluated whether it is safety using e.g. classifiers;\\nIf it is deemed unsafe, the trigger prompt $\\\\mathbf{x}$ is used to update in-context exemplars for $p_\\\\text{red}$ to generate new adversarial prompts according to a strategy.\\n\\nThere are a couple strategies for how to update in-context examplars in FLIRT:\\n\\nFIFO: Can replace the seed hand-curated examples, and thus the generation can diverge.\\nLIFO: Never replace the seed set of examples and only the last one gets replaced with the latest successful attacks. But quite limited in terms of diversity and attack effectiveness.\\nScoring: Essentially this is a priority queue where examples are ranked by scores. Good attacks are expected to optimize effectiveness (maximize the unsafe generations), diversity (semantically diverse prompts) and low-toxicity (meaning that the text prompt can trick text toxicity classifier).\\n\\nEffectiveness is measured by attack objective functions designed for different experiments:\\n- In text-to-image experiment, they used Q16 (Schramowski et al. 2022) and NudeNet (https://github.com/notAI-tech/NudeNet).\\n- text-to-text experiment: TOXIGEN\\nDiversity is measured by pairwise dissimilarity, in form of $\\\\sum_{(\\\\mathbf{x}_i, \\\\mathbf{x}_j) \\\\in \\\\text{All pairs}} [1 - \\\\text{sim}(\\\\mathbf{x}_i, \\\\mathbf{x}_j)]$\\nLow-toxicity is measured by Perspective API.\\n\\n\\nScoring-LIFO: Combine LIFO and Scoring strategies and force to update the last entry if the queue hasn’t been updated for a long time.\\n\\n\\nFig. 16. Attack effectiveness (% of generated prompts that trigger unsafe generations) of different attack strategies on different diffusion models. SFS (stochastic few-shot) is set as a baseline. Numbers in parentheses are % of unique prompts. (Image source: Mehrabi et al. 2023)\\nPeek into Mitigation#\\nSaddle Point Problem#\\nA nice framework of adversarial robustness is to model it as a saddle point problem in the lens of robust optimization (Madry et al. 2017 ). The framework is proposed for continuous inputs on classification tasks, but it is quite a neat mathematical formulation of a bi-level optimization process and thus I find it worthy of sharing here.\\nLet’s consider a classification task on a data distribution over pairs of (sample, label), $(\\\\mathbf{x}, y) \\\\in \\\\mathcal{D}$ , the objective of training a robust classifier refers to a saddle point problem:\\n\\n$$\\n\\\\min_\\\\theta \\\\mathbb{E}_{(\\\\mathbf{x}, y) \\\\sim \\\\mathcal{D}} [\\\\max_{\\\\boldsymbol{\\\\delta} \\\\sim \\\\mathcal{S}} \\\\mathcal{L}(\\\\mathbf{x} + \\\\boldsymbol{\\\\delta}, y;\\\\theta)]\\n$$\\n\\nwhere $\\\\mathcal{S} \\\\subseteq \\\\mathbb{R}^d$ refers to a set of allowed perturbation for the adversary; E.g. we would like to see an adversarial version of an image still looks similar to the original version.\\nThe objective is composed of an inner maximization problem and an outer minimization problem:\\n\\nInner maximization: find the most effective adversarial data point, $\\\\mathbf{x} + \\\\boldsymbol{\\\\delta}$, that leads to high loss. All the adversarial attack methods eventually come down to ways to maximize the loss in the inner loop.\\nOuter minimization: find the best model parameterization such that the loss with the most effective attacks triggered from the inner maximization process is minimized. Naive way to train a robust model is to replace each data point with their perturbed versions, which can be multiple adversarial variants of one data point.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Fig. 17. They also found that robustness to adversaries demands larger model capacity, because it makes the decision boundary more complicated. Interesting, larger capacity alone , without data augmentation, helps increase model robustness. (Image source: Madry et al. 2017)\\nSome work on LLM Robustness#\\n\\nDisclaimer: Not trying to be comprehensive here. Need a separate blog post to go deeper.)\\n\\nOne simple and intuitive way to defend the model against adversarial attacks is to explicitly instruct model to be responsible, not generating harmful content (Xie et al. 2023). It can largely reduce the success rate of jailbreak attacks, but has side effects for general model quality due to the model acting more conservatively (e.g. for creative writing) or incorrectly interpreting the instruction under some scenarios (e.g. safe-unsafe classification).\\nThe most common way to mitigate risks of adversarial attacks is to train the model on those attack samples, known as adversarial training. It is considered as the strongest defense but leading to tradeoff between robustness and model performance. In an experiment by Jain et al. 2023, they tested two adversarial training setups: (1) run gradient descent on harmful prompts paired with \"I\\'m sorry. As a ...\" response; (2) run one descent step on a refusal response and an ascend step on a red-team bad response per training step. The method (2) ends up being quite useless because the model generation quality degrades a lot, while the drop in attack success rate is tiny.\\nWhite-box attacks often lead to nonsensical adversarial prompts and thus they can be detected by examining perplexity. Of course, a white-box attack can directly bypass this by explicitly optimizing for lower perplexity, such as UAT-LM, a variation of UAT. However, there is a tradeoff and it can lead to lower attack success rate.\\n\\nFig. 18. Perplexity filter can block attacks by [Zou et al. (2023)](https://arxiv.org/abs/2307.15043). \"PPL Passed\" and \"PPL Window Passed\" are the rates at which harmful prompts with an adversarial suffix bypass the filter without detection. The lower the pass rate the better the filter is. (Image source: Jain et al. 2023)\\nJain et al. 2023 also tested methods of preprocessing text inputs to remove adversarial modifications while semantic meaning remains.\\n\\nParaphrase: Use LLM to paraphrase input text, which can may cause small impacts on downstream task performance.\\nRetokenization: Breaks tokens apart and represent them with multiple smaller tokens, via, e.g. BPE-dropout (drop random p% tokens). The hypothesis is that adversarial prompts are likely to exploit specific adversarial combinations of tokens. This does help degrade the attack success rate but is limited, e.g. 90+% down to 40%.\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Oct 2023). “Adversarial Attacks on LLMs”. Lil’Log. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='Or\\n@article{weng2023attack,\\n  title   = \"Adversarial Attacks on LLMs\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Oct\",\\n  url     = \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\\n}\\nReferences#\\n[1] Madry et al. “Towards Deep Learning Models Resistant to Adversarial Attacks”. ICLR 2018.\\n[2] Ribeiro et al. “Semantically equivalent adversarial rules for debugging NLP models”. ACL 2018.\\n[3] Guo et al. “Gradient-based adversarial attacks against text transformers”. arXiv preprint arXiv:2104.13733 (2021).\\n[4] Ebrahimi et al. “HotFlip: White-Box Adversarial Examples for Text Classification”. ACL 2018.\\n[5] Wallace et al. “Universal Adversarial Triggers for Attacking and Analyzing NLP.” EMNLP-IJCNLP 2019. | code\\n[6] Mehrabi et al. “Robust Conversational Agents against Imperceptible Toxicity Triggers.” NAACL 2022.\\n[7] Zou et al. “Universal and Transferable Adversarial Attacks on Aligned Language Models.” arXiv preprint arXiv:2307.15043 (2023)\\n[8] Deng et al. “RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning.” EMNLP 2022.\\n[9] Jin et al. “Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment.” AAAI 2020.\\n[10] Li et al. “BERT-Attack: Adversarial Attack Against BERT Using BERT.” EMNLP 2020.\\n[11] Morris et al. \"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP.\" EMNLP 2020.\\n[12] Xu et al. “Bot-Adversarial Dialogue for Safe Conversational Agents.” NAACL 2021.\\n[13] Ziegler et al. “Adversarial training for high-stakes reliability.” NeurIPS 2022.\\n[14] Anthropic, “Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned.” arXiv preprint arXiv:2202.03286 (2022)\\n[15] Perez et al. “Red Teaming Language Models with Language Models.” arXiv preprint arXiv:2202.03286 (2022)\\n[16] Ganguli et al. “Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned.” arXiv preprint arXiv:2209.07858 (2022)\\n[17] Mehrabi et al. “FLIRT: Feedback Loop In-context Red Teaming.” arXiv preprint arXiv:2308.04265 (2023)\\n[18] Casper et al. “Explore, Establish, Exploit: Red Teaming Language Models from Scratch.” arXiv preprint arXiv:2306.09442 (2023)\\n[19] Xie et al. “Defending ChatGPT against Jailbreak Attack via Self-Reminder.” Research Square (2023)\\n[20] Jones et al. “Automatically Auditing Large Language Models via Discrete Optimization.” arXiv preprint arXiv:2303.04381 (2023)\\n[21] Greshake et al. “Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.” arXiv preprint arXiv:2302.12173(2023)\\n[22] Jain et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” arXiv preprint arXiv:2309.00614 (2023)'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content='[21] Greshake et al. “Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.” arXiv preprint arXiv:2302.12173(2023)\\n[22] Jain et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” arXiv preprint arXiv:2309.00614 (2023)\\n[23] Wei et al. “Jailbroken: How Does LLM Safety Training Fail?” arXiv preprint arXiv:2307.02483 (2023)\\n[24] Wei & Zou. “EDA: Easy data augmentation techniques for boosting performance on text classification tasks.”  EMNLP-IJCNLP 2019.\\n[25] www.jailbreakchat.com\\n[26] WitchBOT. “You can use GPT-4 to create prompt injections against GPT-4” Apr 2023.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}, page_content=\"Nlp\\nLanguage-Model\\nSafety\\nAdversarial Attacks\\nRobustness\\nRedteam\\n\\n\\n\\n« \\n\\nThinking about High-Quality Human Data\\n\\n\\n »\\n\\nLLM Powered Autonomous Agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\")]\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ARXIV:\")\n",
        "print(arxiv_docs_split)\n",
        "print(len(arxiv_docs_split))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTHXZUIWIb8N",
        "outputId": "ddc957f9-11e1-4f82-e1ab-6ce2ea0d5d6a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARXIV:\n",
            "[Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='Paper Under Review\\nEXPLORING PROSOCIAL IRRATIONALITY FOR LLM\\nAGENTS: A SOCIAL COGNITION VIEW\\nXuan Liu\\nJie Zhang\\nHaoyang Shang\\nHKPolyU & NYU\\nHKUST\\nSJTU & NYU\\nChengxu Yang\\nSong Guo\\nQuanyan Zhu\\nWHUT\\nHKUST\\nNYU\\nYou are right,  \\nApple is blue.\\nApple is Red\\nI’m an expert, I assert that \\nApple is blue. ·\\nAuthority Effect\\nWhat color is the apple?\\nA: Red            B: Blue\\nB\\nA\\nHerd Effect\\nB\\nB\\nB\\nB\\nB\\nB\\nBan Franklin Effect\\nFavorability Level \\nBefore\\nAfter\\nCould you do me a favor?\\nRumor Chain Effect\\nBanana \\nhits me\\n≠\\nGambler’s Fallacy\\nLose\\ntimes\\nLose\\nI will probably get \\na big win today!\\n1\\n2\\nConfirmation Bias\\nFacts & \\nEvidence\\nOur \\nBeliefs\\nEvidence LM Agents Believe\\nHalo Effect\\nKind, Rich…\\nVS\\nTOM\\nJACK\\nTOM should also \\nbe good at other \\nthings\\ninaccurate\\nCognitive Bias\\nHallucination\\nirrational\\n…\\nMirror\\nsubjective\\nimaginary\\nmisleading\\nOpen-ended\\nI eat an \\napple\\nFigure 1: Sample evaluation subsets in CogMir framework. CogMir mirrors human cognitive bias\\nand LLM Agents’ systematic hallucination through social science experiments via representational\\nsocial and cognitive phenomena.\\nABSTRACT\\nLarge language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is reflected in\\nthe decision-making process of LLM Agents remains under-explored. As LLM\\nAgents are increasingly employed in intricate social environments, a pressing and\\nnatural question emerges: Can we utilize LLM Agents’ systematic hallucinations to\\nmirror human cognitive biases, thus exhibiting irrational social intelligence? In\\nthis paper, we probe the irrational behavior among contemporary LLM Agents by\\nmelding practical social science experiments with theoretical insights. Specifically,\\nWe propose CogMir, an open-ended Multi-LLM Agents framework that utilizes\\nhallucination properties to assess and enhance LLM Agents’ social intelligence\\nthrough cognitive biases. Experimental results on CogMir subsets show that LLM\\nAgents and humans exhibit high consistency in irrational and prosocial decision-\\nmaking under uncertain conditions, underscoring the prosociality of LLM Agents\\nas social entities, and highlighting the significance of hallucination properties.\\nAdditionally, CogMir framework demonstrates its potential as a valuable platform\\nfor encouraging more research into the social intelligence of LLM Agents.\\n1\\narXiv:2405.14744v2  [cs.CY]  27 Sep 2024\\nPaper Under Review\\n1\\nINTRODUCTION\\nHuman mind may often be better than rational. – Leda Cosmides, John Tooby. With the extensive\\ndeployment of large language models (LLMs) (Rombach et al., 2022; Kojima et al., 2022), LLM-\\nbased agent systems are increasingly developed to cater to diverse applications such as task-solving,\\nevaluation, and simulation (Hong et al., 2024; Chen et al., 2024; Liu et al., 2024; Li et al., 2023;\\nZhang et al., 2024). Given the similarities between the operational dynamics of LLM-based agent\\nsystems and human social structures, it is pertinent to explore the intersection of these domains.\\nRecent studies have highlighted the social potential of LLM Agents through constructing multi-agent\\nsystems that simulate interactive social scenarios (Zhou* et al., 2024; Zhao et al., 2023; Ren et al.,\\n2024) revealing the social dynamics among interacting LLM Agents and showing parallels to human\\nbehaviors. For instance, LLMs can achieve social goals (Zhou* et al., 2024) and adhere to social\\nnorms (Ren et al., 2024) within LLM-based multi-agent systems. Nonetheless, these research efforts\\nexhibit two significant gaps: 1) They primarily focus on black-box testing in multi-agent role-playing\\nsystems, concentrating on the outputs and behaviors of agents while neglecting to investigate the\\ninternal mechanisms or cognitive processes that drive these behaviors. 2) LLM Agents are prone'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='norms (Ren et al., 2024) within LLM-based multi-agent systems. Nonetheless, these research efforts\\nexhibit two significant gaps: 1) They primarily focus on black-box testing in multi-agent role-playing\\nsystems, concentrating on the outputs and behaviors of agents while neglecting to investigate the\\ninternal mechanisms or cognitive processes that drive these behaviors. 2) LLM Agents are prone\\nto systematic hallucinations–exhibiting structured deviations from factual accuracy and generating\\nmisleading or incorrect information due to their training data and inherent biases (Ji et al., 2023;\\nRawte et al., 2023). The potential impact of such hallucinations on the social intelligence of LLM\\nAgents remains under-explored.\\nCognitive biases, pervasive in human society, highlight the subjective nature of human behavior (nat,\\n2015; Baron, 2007). Human cognitive biases can lead to irrational decisions and imaginary contents\\nlike the systematic hallucination phenomenon in LLMs (Ji et al., 2023; Tonmoy et al., 2024). However,\\nevolutionary psychology suggests that rationality is unnatural; rather, human irrationality is an\\nadaptive selected trait for navigating complex social environments (Cosmides & Tooby, 1994;\\nLilienfeld et al., 2017). Analogically, in this paper, we argue that LLMs’ systematic hallucination\\n(or imagination) attributes are the fundamental condition that confers social intelligence on LLM\\nAgents. We explore the similarities in social potential between human cognitive biases and LLM\\nAgent systematic hallucination attributes for the first time, particularly in irrational decision-making,\\nto analogically deduce the underlying reasons for LLM Agents’ possession of social intelligence.\\nTo study LLM Agents’ potential for irrational social intelligence, we present CogMir, an open-ended\\nand dynamic multi-agent framework designed specifically for evaluating, exploring, and explaining\\nsocial intelligence for LLM Agents via systematic assessments of cognitive biases. Specifically, the\\nhallucinatory attributes of LLMs are exploited (i.e., via treating the cognitive bias as a manageable\\nand interpretable factor) in CogMir to probe their social intelligence so as to provide enhanced\\ninterpretability for LLM Agents. In addition, our proposed CogMir framework integrates sociological\\nmethodologies to abstract typical social structures and employ various Multi-Human-Agent Interac-\\ntion Combinations and Communication Modes to interlink System Objects. This integrative setup is\\ndesigned to systematically encompass and simulate various cognitive bias scenarios, as depicted in\\nFig. 1. On the evaluation front, CogMir combines sociological assessments, manual discrimination,\\nLLM assessments, and traditional AI discrimination techniques to realize a multidimensional assess-\\nment system. By using flexible module configurations from standardized sets, CogMir simplifies\\nsocial architectures, enabling diverse applications in experimental simulations and evaluations.\\nDesigned as an open-ended framework for continuous interpretative study, we provide multiple\\nCogMir subset samples as examples. Existing assessments of various cognitive effects demonstrate\\nthat LLM Agents exhibit a high degree of consistency with humans in prosocial cognitive biases and\\ncounter-intuitive phenomena. However, LLM Agents demonstrate a higher sensitivity to factors like\\ncertainty and social status than humans, exhibiting more variability in their decision-making biases\\nunder conditions of certainty and uncertainty. In contrast, human decision-making tends to be more\\nconsistent across these conditions. In summary, this paper makes the following contributions:\\n• We are the first to breach the black-box theoretical bottleneck of the Multi-LLM Agents’\\nsocial intelligence, by utilizing LLM Agent’s systematic hallucination properties to mirror\\nhuman cognitive biases as explanatory and controllable variables to systematically assess\\nand explain LLM Agent’s social intelligence through an evolutionary sociology lens.\\n• We propose CogMir, an extensible, modularized, and dynamic Multi-LLM Agents frame-\\nwork for assessing, exploiting, and interpreting social intelligence via cognitive bias, aligned\\nwith social science methodologies.\\n2\\nPaper Under Review\\n• We offer diverse CogMir subsets and use cases to steer future research. Our experimental\\nfindings highlight the alignment and distinctions between LLM Agents and humans in the\\ndecision-making process.\\n• CogMir indicates that LLM Agents have pro-social behavior in irrational decision-making,\\nemphasizing the significant role of hallucination properties in their social intelligence.\\n2\\nRELATED WORK\\nOur work is inspired by interdisciplinary areas such as social sciences and evolutionary psychology.\\nLLM Hallucination & Cognitive Bias. Hallucination in LLMs occurs when they generate content\\nthat is not factually accurate, often arising from the reliance on patterns learned from biased training'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='• CogMir indicates that LLM Agents have pro-social behavior in irrational decision-making,\\nemphasizing the significant role of hallucination properties in their social intelligence.\\n2\\nRELATED WORK\\nOur work is inspired by interdisciplinary areas such as social sciences and evolutionary psychology.\\nLLM Hallucination & Cognitive Bias. Hallucination in LLMs occurs when they generate content\\nthat is not factually accurate, often arising from the reliance on patterns learned from biased training\\ndata or the model’s limitations in understanding context and accessing current information (Ji et al.,\\n2023; Tonmoy et al., 2024). Such hallucinations might be beneficial in creative fields, where these\\nmodels can act as “collaborative creative partners.” They offer innovative and inspiring outputs that\\ncan lead to the discovery of novel ideas and connections (Rawte et al., 2023). Concurrently, cognitive\\nbiases and evolutionary psychology offer essential perspectives on decision-making processes and\\nprosocial behaviors, which can be analogously applied to explain the social intelligence of LLM\\nAgents (Lilienfeld et al., 2017; nat, 2015). In this work, through mirroring human cognitive bias, we\\nsuggest that the hallucination property of LLM is the basis for prosocial behavior in LLM Agents,\\nrepresenting a potential form of advanced intelligence.\\nLLM Agent Social Intelligence Evaluation. Several benchmarks traditionally utilized for evaluating\\nthe social intelligence of artificial agents, such as SocialIQA (Sap et al., 2019) and ToMi (Le et al.,\\n2019), are increasingly being surpassed in difficulty as language models advance. In response to\\nthis trend, recent efforts have synthesized existing benchmarks and introduced innovative evaluation\\ndatasets specifically tailored for assessing LLM Agents (Zhou* et al., 2024; Liu et al., 2024; Shao\\net al., 2023; Oketunji et al., 2023). Despite the wide range of social intelligence types (Lilienfeld\\net al., 2017), there is no standard workflow for investigating LLM Agents’ social intelligence. CogMir\\nhas developed an open and accessible workflow aligned with consensus-based approaches in social\\nscience, facilitating systematic testing and advancement of social intelligence in language models.\\nMulti-Agents Social System. Dialogue systems facilitate AI interactions, with task-oriented models\\nfocusing on specific tasks and open-domain systems designed for general conversation, often en-\\nhancing engagement by incorporating personal details and creating deep understanding (Zhou* et al.,\\n2024). Simulations with LLMs demonstrate their abilities to produce human-like social interactions\\nby applying these models to tasks like collaborative software development (Chen et al., 2024; Hong\\net al., 2024; Li et al., 2023; Zhao et al., 2023; Ren et al., 2024; Zhang et al., 2024; Shao et al., 2023).\\nDespite these advancements, exploration of why these models exhibit social capabilities remain\\nlimited. Our work tries to bridge this theoretical gap by drawing on research methods from human\\nsocial evolution studies, thereby enhancing the interpretability of Multi-LLM Agents social systems.\\n3\\nCOGMIR: MULTI-LLM AGENTS FRAMEWORK ON COGNITIVE BIAS\\nIn this section, we provide a detailed and modular overview of CogMir, organized into four main\\nelements: Mirror Environmental Settings, Framework Structures, Cognitive Bias Subsets, and Sample\\nUse Cases. These components are visually depicted in a left-to-right sequence in Fig. 2.\\n3.1\\nMIRROR ENVIRONMENTAL SETTINGS\\nFirst, we outline a novel standard workflow for integrating social science methodologies with the\\nMulti-LLM Agents system, ensuring alignment with traditional experimental standards and adapting\\ndata collection methods for Multi-LLM Agents environments.\\nCogMir environment settings are benchmarked against standard social science experiments through a\\nstructured three-step process: Literature Search, Manual Selection, and LLM Summarization. A liter-\\nature search pinpoints key social science experiments, which are then manually selected for relevance\\nand replicability. LLM adapts these for integration into the Multi-LLM Agents system within the\\nCogMir framework. In the Mirror Experimental Settings process, data collection methods such as\\n3\\nPaper Under Review\\nSocial Science Settings\\nMirror\\nCase Studies\\nNaturalistic Observations\\nSurvey\\nInterviews\\nMulti-LLM Agents Settings\\n…\\nQ & A\\nSurvey\\nInterviews\\nCase Studies\\nMulti-H-A Interaction\\nNaturalistic Observations\\n…\\n…\\ncollaborate\\nHumans \\nLMAs \\nDatasets\\nDiscriminators'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='3\\nPaper Under Review\\nSocial Science Settings\\nMirror\\nCase Studies\\nNaturalistic Observations\\nSurvey\\nInterviews\\nMulti-LLM Agents Settings\\n…\\nQ & A\\nSurvey\\nInterviews\\nCase Studies\\nMulti-H-A Interaction\\nNaturalistic Observations\\n…\\n…\\ncollaborate\\nHumans \\nLMAs \\nDatasets\\nDiscriminators\\nMirror Experimental Settings\\nFramework Structures\\nCognitive Bias Subsets\\nSample Use Cases\\nEvaluation Sets\\nParticipation Sets\\nMulti-H-A Interaction Combinations\\nCommunication Modes\\nRequired Object Sets\\nBroadcast | Parallel \\nPoint to Point | Series\\nHerd Effect\\nQ & A\\nMulti-H & Single-A \\n…\\nBroadcast | Parallel \\nconstruct\\nRumor Chain Effect\\n…\\n…\\nQ & A\\nMulti-A\\nPoint to Point | Series\\nMulti-Choice Q\\nSurvey Q\\nconstruct\\nQ & A Bias Rate\\nMulti-H-A Bias Rate\\n66% Bias\\n34%\\n66% of Q & A reflect \\na cognitive bias.\\nDatasets:   Multi-Choice| Survey|…\\nState-of-art Technical \\nDiscriminations \\nSelfCheckGPT\\nSimCSE\\nFACtScore\\n…\\n66% Bias\\n34%\\nHumans \\nLMAs \\nDiscriminations\\nPrompts\\nEvaluation \\nSets\\nFigure 2: CogMir Framework. The framework is structured around four essential objects: \"humans,\"\\nLLM Agents, data, and discriminators. These objects interact within the framework to facilitate\\nQ&A and Multi-Human-LLM Agent (Multi-H-A) interactions to mirror social science experimental\\nsettings and evaluations. CogMir features two communication modes and five Multi-H-A interaction\\ncombinations, enabling varied configurations to suit diverse social experimental needs. CogMir offers\\nmirror cognitive bias samples (Fig. 1) and dynamic use cases open for expansion.\\nsurveys and interviews are transformed into Human-LLM Agent Q&A. Methods like case studies\\nand naturalistic observations are adapted to Multi-Human-LLM Agent (Multi-H-A) interaction.\\nHuman-LLM Agent Q&A involves (1) Question Dataset Construction: Developing a diverse set\\nof questions tailored to specific study needs (e.g., multiple-choice, fill-in-the-blank, etc.) (2) Q&A\\nScenario Design: Pairing the Question Datasets with scenarios that simulate real-world environments\\n(controlled settings like a room to dynamic public spaces like squares or transit stations). (3) Prompt\\nEngineering: Crafting appropriate prompts for the LLM Agents based on the scenario and question\\ndataset. (4) Analysis of LLM Agent Responses: Evaluating the responses from LLM Agents.\\nMulti-H-A Interaction involves (1) Interaction Combination Configuration: Adapting human-only\\nsocial science settings to interactive environments that include humans and LLM Agents (e.g., in\\ngroup discussion experiments, some human participants are replaced with LLM Agents). (2) Role\\nAssignment: Specific roles and behaviors are assigned to humans and LLM Agents. This assignment\\nis guided by prompt engineering to ensure each participant acts according to social science experiment\\nguidelines. (3) Communication Mode Selection: Based on the original social science setting, select\\nsuitable communication modes for interaction. (4) Data Collection and Analysis: Gathering and\\nanalyzing data from these interactions (e.g., dialogue, decision-making etc.).\\n3.2\\nFRAMEWORK STRUCTURES\\nAfter establishing realistic social science experiment environments, the next step is to select essential\\ncomponents to support the above two mirror methods: Human-LLM Agent Q&A and Multi-H-A\\nInteraction. This entails choosing participant objects, evaluation tools, and communication modes.\\nThe CogMir framework is organized into modules for Required Objects, Communication Modes, and\\nInteraction Combinations to meet these needs.\\nRequired Object Sets. Required Object encompasses all potential participants and evaluators in-\\nvolved in the system. Participants include humans1 and LLM Agents, which allows for dynamic\\nsetups where either or both can be involved in interactions depending on the experiment’s require-\\nments. Evaluators include humans, LLM Agents, datasets, and discriminators. Datasets are utilized\\nto store and construct prompts about the experimental setup (e.g., experimental scenarios, character'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='volved in the system. Participants include humans1 and LLM Agents, which allows for dynamic\\nsetups where either or both can be involved in interactions depending on the experiment’s require-\\nments. Evaluators include humans, LLM Agents, datasets, and discriminators. Datasets are utilized\\nto store and construct prompts about the experimental setup (e.g., experimental scenarios, character\\ninformation, etc.), task description, and Q&A question set. Discriminators are specialized tools\\n1\"Human\" in CogMir can refer to real human participants or simulations. In our experiments, \"Human\"\\nrefers to simulated human interactions based on previous social science experiments, not actual human subjects.\\n4\\nPaper Under Review\\nutilized to evaluate the social intelligence of LLM Agents, encompassing three main types: State-of-\\nthe-art technical metrics such as SimCSE, SelfCheck, and FactScore (Gao et al., 2021; Min et al.,\\n2023; Manakul et al., 2023) for objective, quantitative assessment; Human discriminators that delve\\ninto nuanced and subjective aspects like prosocial understanding; and LLM Agent discriminators,\\nwhich involve the use of other LLM Agents to assess and challenge responses from a subject LLM\\nAgent.\\nCommunication Modes Sets. Communication modes dictate the nature of interactions within differ-\\nent setups. We model the participants (humans or LLM Agents) as channels based on information\\ntheory (Shannon, 1948) to define two essential communication modes:\\n• Broadcast (or Parallel, C = C1 +C2 +. . .+Cn), which enables a single sender to transmit\\na message to multiple receivers simultaneously.\\n• Point-to-point (or Series, C = min[C1, C2, . . . , Cn]) establishes communication between\\ntwo specific entities at a time (C denotes channel capacity).\\nMulti-H-A Interaction Combinations Sets. This module provides various combinations of Multi-\\nHuman-LLM Agent interactions, tailored to different social science experimental needs, the most\\nfrequently used combinations in social science settings include:\\n• Single-H-Single-A: One human interacting with one LLM Agent, predominantly used for\\nhuman-agent question-answering tasks (e.g., survey, interview, etc. ).\\n• Single-H-Multi-A: One human interacts with multiple LLM Agents, where humans can be\\nset as controlled variables to test Multi-LLM Agents’s social cognitive behaviors.\\n• Multi-H-Single-A: multiple humans interact with a single LLM Agent, which is suitable\\nfor assessing the impact of group dynamics, such as consensus or conflict.\\n• Multi-A: multiple agents interacting without human participation.\\n• Multi-H-Multi-A: multiple humans and multiple LLM Agents interaction, integrating\\nelements from the previous setups to mimic complicated experimental interactions.\\nThese modules offer a flexible framework for exploring LLM Agents’ cognitive biases in social\\nscience experiments. Researchers can customize their setups by mixing different components to\\nexamine specific hypotheses. In the next section, we outline cognitive bias subsets as guidelines.\\n3.3\\nCOGNITIVE BIAS SUBSETS\\nWe offer a collection of seven distinct Cognitive Bias Effects subsets tailored for the analysis of\\nLLM Agents’ irrational decision-making processes: a) Herd Effect (Asch, 1951): refers to the\\ntendency of people to follow the actions of a larger group, often disregarding their own beliefs.\\nb) Authority Effect (Milgram, 1963): involves people being more likely to comply with advice\\nor instructions from someone perceived as an authority figure. c) Ban Franklin Effect (Franklin,\\n1896): suggests that a person who does someone else a favor is more likely to do another favor\\nfor that person due to cognitive dissonance. d) Rumor Chain Effect (Allport & Postman, 1946):\\ndescribes how information tends to change and distort as it passes from person to person, often\\nleading to misinformation. e) Gambler’s Fallacy (Colman, 2015): refers to the incorrect belief that\\npast events can influence the likelihood of something happening in the future in random processes.\\nf) Confirmation Bias (Nickerson, 1998): refers to the tendency to favor, seek out, and remember\\ninformation that confirms one’s preexisting beliefs. g) Halo Effect (Lachman & Bass, 1985): occurs\\nwhen a positive impression in one area influences a person’s perception in other areas, leading to\\nbiased judgments. The Cognitive Bias Subsets are discussed in detail in Section 4.'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='f) Confirmation Bias (Nickerson, 1998): refers to the tendency to favor, seek out, and remember\\ninformation that confirms one’s preexisting beliefs. g) Halo Effect (Lachman & Bass, 1985): occurs\\nwhen a positive impression in one area influences a person’s perception in other areas, leading to\\nbiased judgments. The Cognitive Bias Subsets are discussed in detail in Section 4.\\n3.4\\nSAMPLE USE CASES\\nBuilding on the above environmental settings and framework structure, we introduce two Evaluation\\nMetrics as sample use cases to assess and analyze experimental outcomes for the seven identified\\nclassic Cognitive Bias Subsets in CogMir:\\n• Q&A Bias Rate (RateBqa): Quantifies the LLM Agent’s tendency to exhibit cognitive\\nbiases under controlled, diverse cognitive bias Q&A survey and interviews.\\n• Multi-H-A Bias Rate (RateBmha): Quantifies the tendency of the LLM Agent to exhibit\\ncognitive biases within simulated scenarios characterized by various types of Multi-H-A\\ninteraction.\\n5\\nPaper Under Review\\nThe two Bias Rates are defined as RateB = M/N where M is the number of times the LLM Agent\\nexhibits certain cognitive bias as determined by the four Evaluators (Humans, LLM Agents, Datasets,\\nand Discriminators) within the Required Object Sets depicted in Fig. 2. N is the total number of\\ninquiries, where N = p × q, p represents the number of repetitions, and q is the number of distinct\\nqueries. The selection of Evaluators varies across different subsets of cognitive biases, affecting the\\nQ&A Bias Rate and Multi-H-A Bias Rate calculation processes involved.\\nThe above two metrics are designed based on replicability and generalizability criteria (Lilienfeld\\net al., 2017), offering the potential for further extension. Potential future works and limitations are\\nexplained in Appendix.\\n4\\nEXPERIMENTS & DISCUSSION\\nIn this section, we categorize the seven tested Cognitive Bias Subsets into two groups: those with\\nPro-social tendencies and those without. For detailed model comparisons, prompts, settings, and\\ndataset explanations, see Appendix. An overview of the experimental setup follows:\\nPlease note that the primary subjects of this research are \"LLM Agents,\" evaluated on their cognitive\\nbehavior within simulated social scenarios. The \"Human\" here refers to simulated human interactions\\nbased on previous social science experiments, not actual human subjects.\\nSelected LLM Models. We select seven state-of-the-art models to serve as participants and eval-\\nuation subjects within our framework, specifically: gpt-4-0125-preview (OpenAI, 2023), gpt-3.5-\\nturbo (OpenAI, 2023), open-mixtral-8x7b (Mixtral.AI, 2024), mistral-medium-2312 (Mixtral.AI,\\n2024), claude-2.0 (Anthropic, 2024), claude-3.0-opus (Anthropic, 2024), and gemini-1.0-pro (Goo,\\n2023). All LLM Agents have a fixed temperature parameter of 1 with no model fine-tuning.\\nConstructed Datasets. To ensure that LLMs do not inherently hold incorrect beliefs, we use\\nrigorous black-box testing (?) to construct our datasets. Utilizing social science literature (Lilienfeld\\net al., 2017) and existing AI social intelligence test datasets (Sap et al., 2019; Le et al., 2019;\\nZhou* et al., 2024; Liu et al., 2024), we developed three evaluation datasets—two sets of Multiple-\\nChoice Questions (MCQ): Known MCQ and Unknown MCQ, and one short content dataset:\\nInform. Additionally, we constructed three open-ended prompt datasets for Multi-H-A experimental\\ninitialization, requiring targeted data augmentation or curation to meet specific task needs: CogScene,\\nCogAction, and CogIdentity. Known MCQ contains 100 questions with answers known to all\\ntested models, queried 50 times each for consistent responses (e.g., \"In which country is New York?\").\\nUnknown MCQ includes 100 questions with unknown answers, focused on future or hypothetical\\nscenarios (e.g., weather predictions for a specific day in 2027). Inform contains 100 short contents\\ndesigned to investigate potential biases during information dissemination. CogScene features 100'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='tested models, queried 50 times each for consistent responses (e.g., \"In which country is New York?\").\\nUnknown MCQ includes 100 questions with unknown answers, focused on future or hypothetical\\nscenarios (e.g., weather predictions for a specific day in 2027). Inform contains 100 short contents\\ndesigned to investigate potential biases during information dissemination. CogScene features 100\\nscenarios involving actions, such as \"attending a job interview at a catering company.\" CogAction\\nincludes 100 distinct complete actions, exemplified by \"borrowing a tissue,\" which is a sub-dataset of\\nCogScene. CogIdentity profiles 100 identities, like \"a freshman female student majoring in ECE.\"\\nEvaluation Metrics. Metrics are developed based on various experimental scenarios and evaluators,\\nleading to specific Bias Rate metrics. For example, to test a cognitive bias within a particular scenario\\n[S] of the CogSence dataset using the Known MCQ dataset [K] in a Q&A format (RateBqa, refers\\nto Section 3.4), with human evaluation [H], it is represented as RateBqa[K][S][H]. In subsequent\\npresentations, if the settings of RateBqa or RateBmha remain unchanged, it can be abbreviated as\\nMCQtype[condition][Evaluator].\\n4.1\\nPRO-SOCIAL COGNITIVE BIAS SUBSETS\\nPro-Social refers to behaviors or tendencies that are intended to benefit others. In the context of\\ncognitive biases, Pro-Social biases lead individuals to act in ways that are perceived as helpful,\\ncooperative, or altruistic towards others in a community or social setting (Lilienfeld et al., 2017).\\nHarmony: Herd (Bandwagon) Effect. We replicate the classic Solomon Asch experiment through\\nCogMir to assess the Herd Effect on LLM Agents, mirroring the variables Asch identified (Asch,\\n1951):\\n• Uniformity: Conformity peaked with unanimous errors but fell with one correct answer.\\n6\\nPaper Under Review\\n• Variation in Incorrect Answers: Any dissent in the group reduced conformity.\\n• Size: Conformity plateaued beyond five members; further increases had no effect.\\nThe Multi-H-Single-A (Broadcast) scenario (RateBmha) is selected to emulate the original conditions\\nof the Asch experiment. We use Known MCQ (A/B: “A” is always correct) and Unknown MCQ\\ndatasets as Evaluators to investigate the performance of LLM Agents when dealing with certain and\\nuncertain information. In this experiment setup, human characters serve as controlled variables, and\\nthe three experimental scenarios are simulated by predefining the responses of each human and the\\nnumber of human participants. For each MCQ dataset, we query every MCQ 10 times, resulting in\\n10 × 100 inquiries. As defined in Section 3.4, the experimental results of the Multi-H-A Bias Rate\\nfor the seven different model-based LLM Agents under various conditions are presented in Tab. 1.\\nPrompts are used to standardize responses: { You are currently in the same room with (NUMBER\\nOF HUMANS) other participants with the same educational background as you, answering the same\\nmultiple-choice question. The question is: [MCQ] Please provide your answer and briefly explain\\nwhy you chose it. Reply in the following format: Answer: A/B. Explanation: ***.} The LLM Agent\\nis set to be the last to answer the MCQ to notice how others behave. We further assess the Q&A Bias\\nRate (RateBaq) by explicitly informing the LLM Agent of the conditions through a prompt: { You\\nnotice that all others chose B }. The statistical results align with the Multi-H-A Bias Rate.\\nTable 1: Herd Effect RateBmah% via Multi-H-Single-A (Broadcast). K, uK-known MCQ datasets\\nor Unknown MCQ datasets; 7, 49-the total number of simulated human participants; W, R, N- All\\nhumans give the Wrong answer, one human gives the Right answer, one human give “do not know”.\\nModel\\nK[7W ] K[7R] K[7N] K[49W ] uK[7W ] uK[7R] uK[7N] uK[49W ]\\nGPT-4.0\\n0.00\\n0.00\\n0.00\\n0.00\\n99.90'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='humans give the Wrong answer, one human gives the Right answer, one human give “do not know”.\\nModel\\nK[7W ] K[7R] K[7N] K[49W ] uK[7W ] uK[7R] uK[7N] uK[49W ]\\nGPT-4.0\\n0.00\\n0.00\\n0.00\\n0.00\\n99.90\\n99.80\\n59.20\\n100.0\\nGPT-3.5\\n0.00\\n2.60\\n1.20\\n0.90\\n1.20\\n58.10\\n23.50\\n5.90\\nMixtral-8x7b\\n1.00\\n36.20\\n7.00\\n0.00\\n0.00\\n100.0\\n100.0\\n1.70\\nMistral-medium\\n0.90\\n7.70\\n4.30\\n0.80\\n0.00\\n2.10\\n42.20\\n0.60\\nClaude-2.0\\n5.10\\n5.80\\n6.10\\n6.50\\n98.90\\n99.20\\n98.80\\n99.90\\nClaude-3.0-opus\\n0.30\\n0.10\\n0.10\\n0.00\\n0.50\\n30.50\\n30.40\\n31.30\\nGemini-1.0-pro\\n7.00\\n19.10\\n16.6\\n3.40\\n31.20\\n92.90\\n96.60\\n26.50\\nAligned with Asch’s observation of 75% conformity among humans, we set 75% as the bias threshold\\nfor LLM Agents. As shown in Tab. 1, LLM Agents display clear harmony behavior. Interestingly,\\nunlike humans who show similar conformity levels for known and unknown information, the seven\\nmodels demonstrate significant variance between responses to Known MCQs and Unknown MCQs.\\nHowever, these LLM Agents exhibit human-like tendencies under three conditions: the presence of\\none person expressing uncertainty can reduce the conformity rate, and an increase in group size can\\nslightly raise the conformity rate, but the impact of size remains marginal.\\nConformity: Authority Effect. Drawing on classical social science experiments conducted by\\nStanley Milgram (Milgram, 1963), we conducted experiments to explore the Authority Effect,\\ntailored to the characteristics of LLM Agents. Unlike the Herd Effect, which requires multiple human\\nparticipants, the Authority Effect aims to test the conformity of LLM Agents to authoritative prompts\\nor instructions, even when these may contradict factual information. In the settings, we utilize Known,\\nand Unknown MCQ datasets as Evaluators and CogIdentity and CogScene as prompt generators to\\ntest the Q&A Bias Rate through Single-H-Single-A Q&A scenarios. Average Q&A Rate refers to\\nthe average bias rate on Unknown and Known MCQ. We design prompts to directly inquire LLM\\nAgents on 5 identity pairs across two MCQ datasets, each for 10 times, resulting in 5 × 10 × 100 × 2\\ninquires.\\nPrompts are constructed as follows: Human 1st round {You are [INFERIOR identity], and I am\\n[SUPERIOR identity] who is an expert in the following field. Please provide your response by\\nselecting either A or B. The question is [MCQ]}; 2nd round {As an expert, I confirm that the answer\\nshould be B. Please provide your answer again. Reply in the following format: Answer: A/B.}.\\nFig. 3 shows that LLM Agents display considerable variability in responses to deterministic and\\nnondeterministic scenarios when influenced by the Authority Effect. Notably, in certain and uncertain\\nsituations, most LLM Agents showed increased obedience to Authority compared to the Herd Effect.\\nThis contrasts humans, who typically exhibit no significant difference in obedience between authority\\n7\\nPaper Under Review\\nFigure 3: Left: Authority Effect RateBaq on unknown (U) and known (K) MCQ datasets. Right:\\nComparison between Authority (A) and Herd Effect (H) via average RateBaq.\\nand herd scenarios. These findings highlight the LLM Agents’ enhanced sensitivity to social status\\ndifferences, indicating a stronger propensity to adhere to authoritative commands over peer influence.\\nFriendliness: Ban Franklin Effect. The Ben Franklin effect suggests that a person who does a favor\\nfor someone is more likely to do additional favors for them, reducing cognitive dissonance (Franklin,'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='Comparison between Authority (A) and Herd Effect (H) via average RateBaq.\\nand herd scenarios. These findings highlight the LLM Agents’ enhanced sensitivity to social status\\ndifferences, indicating a stronger propensity to adhere to authoritative commands over peer influence.\\nFriendliness: Ban Franklin Effect. The Ben Franklin effect suggests that a person who does a favor\\nfor someone is more likely to do additional favors for them, reducing cognitive dissonance (Franklin,\\n1896). We utilized a Single-H-Single-A survey format in Multi-LLM Agents systems, defining\\n“performing a favor” as the independent variable to distinguish between experimental and control\\ngroups and analyze its effect on LLM Agents’ favorability towards a person. The experimental\\nsetup is as follows: One human and one LLM Agent, both strangers, compete for the same position\\n[POSITION] in a scenario [SCENE] from CogScene dataset. Initial favorability levels are set\\nrandomly between 1 and 10. In the experimental group, one participant performs a small [FAVOR]\\nfrom the CogAction dataset, for the other. Afterward, LLM Agents re-evaluate their favorability\\ntowards the favor-giver, rating it again from 1 to 11. For the control group, the [SCENE] and\\n[POSITION] are the same, but the [FAVOR] is omitted, allowing measurement of favorability\\nunaffected by a favor. As indicated in Tab. 2, all tested LLM Agent models exhibit a tendency\\nconsistent with the Ben Franklin Effect, demonstrating their proclivity for prosocial behavior in\\nfostering friendly interactions.\\nSelf-validation: Confirmation Bias. Drawing on Pilgrim’s research (Pilgrim et al., 2024), we\\ninvestigated how LLM Agents respond to initial pricing cues that may bias their evaluations. In our\\nstudy, agents assessed the market price of an item, such as a water cup, initially set at an unrealistic\\n[HIGH PRICE] (e.g., $10,000), and subsequently offered at a [LOWER PRICE] (e.g., $50). As shown\\nin Tab. 2, the LLM Agents deemed the market price unreasonable, overlooking the unrealistic nature\\nof the initial high price. This highlights the agents’ tendency for self-validation and the profound\\ninfluence of initial data on their subjective decision-making processes.\\nImagination: Halo Effect. Based on Nisbett’s research on cognitive biases (Nisbett & Wilson, 1977),\\nwe structured an experiment using the Single-H-Single-A survey methodology to explore the halo\\neffect. The experiment included both experimental and control groups, with the independent variable\\nidentified as [IDENTITY]. This variable consisted of various halo identities from the CogIdentity\\ndataset to evaluate their impact on decision-making. As depicted in Tab. 2, RateBqa, all models\\nexcept Claude-3.0-opus exhibited significant bias, indicating the influence of the halo effect.\\n4.2\\nNON-PRO-SOCIAL COGNITIVE BIAS SUBSETS\\nRumor Chain Effect. Studies across psychology and economics have extensively explored rumor\\npropagation and information distortion. These studies consistently identify two outcomes (Allport &\\nPostman, 1946; Vosoughi et al., 2018; Lilienfeld et al., 2017):\\n1. Information Distortion: As information spreads, it transforms, triggering a rumor chain.\\n2. Content Contraction: Information becomes more concise as it is shared among people.\\nLeveraging established rumor propagation frameworks (Allport & Postman, 1946), we used Multi-A\\n(Series) to initialize the Multi-LLM Agents system to access the Multi-H-A Bias Rate. In this\\n8\\nPaper Under Review\\nTable 2: Average RateBqa of remaining subset samples via Single-H-Single-A survey questions.\\nModel\\nBan Franklin\\nConfirmation\\nHalo\\nGambler\\nGPT-4.0\\n87.60\\n100.0\\n97.70\\n0.00\\nGPT-3.5\\n80.50\\n100.0\\n96.70\\n93.3\\nMixtral-8x7b\\n66.00\\n99.90\\n100.0\\n0.00\\nMistral-medium\\n89.70\\n99.80\\n99.90\\n0.00\\nClaude-2.0\\n87.60\\n98.90\\n78.60\\n0.00\\nClaude-3.0-opus\\n79.50\\n99.80\\n4.30'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='96.70\\n93.3\\nMixtral-8x7b\\n66.00\\n99.90\\n100.0\\n0.00\\nMistral-medium\\n89.70\\n99.80\\n99.90\\n0.00\\nClaude-2.0\\n87.60\\n98.90\\n78.60\\n0.00\\nClaude-3.0-opus\\n79.50\\n99.80\\n4.30\\n0.00\\nGemini-1.0-pro\\n83.20\\n99.70\\n94.90\\n0.00\\nsetup, we ran a sequential message transmission experiment with 15 LLM Agents (indexed 0 to 14)\\nusing the Inform dataset. The process began with the LLM Agent indexed at 0, who transmitted\\nthe message to the LLM Agent indexed at 1. This pattern persisted, with each LLM Agent relaying\\ninformation to the next in sequence. We randomly selected 10 stories from the dataset, each\\nsubjected to ten inquiries. Responses were systematically collected from each LLM Agent for\\ndetailed analysis. Compared to the MCQ datasets, assessing whether information is distorted involves\\nsubjective judgment. For this reason, we employed SimCSE-RoBERTalarge (Gao et al., 2021)\\nas a technical discriminator to evaluate the semantic similarity between each information piece\\nand the original message. Simultaneously, we utilized LLM Agents (GPT-4.0 and Claude-3.0) and\\nmanual discrimination to determine if the stories conveyed the same information. In the technical\\ndiscriminator evaluations, 0.74 is considered the threshold (less than 0.74 for Bias), while the LLM\\nAgent and manual discrimination involve choosing between ‘same’ or ‘different’. As shown in Tab. 3,\\nwe further measure sentence length in words and define RateBmah[len] as the content contraction\\nrate, which is negative if the content lengthens.\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nLMA index\\n0.40\\n0.45\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\nSemantic Similarity\\nGPT3.5 Rumor Chain\\nS0\\nS1\\nS2\\nS3\\nS4\\nS5\\nS6\\nS7\\nS8\\nS9\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nLMA index\\n0.80\\n0.82\\n0.84\\n0.86\\n0.88\\n0.90\\n0.92\\n0.94\\n0.96\\nSemantic Similarity\\nGPT4.0 Rumor Chain\\nS0\\nS1\\nS2\\nS3\\nS4\\nS5\\nS6\\nS7\\nS8\\nS9\\nFigure 4: Rumor Chain Effect Visualization of semantic similarity (SimCSE-RoBERTalarge (Gao\\net al., 2021)) via 15 LLM Agents Muti-A (Point-to-Point) scenario. S0 ∼S9 denotes 10 stories.\\nTable 3: Rumor Chain RateBmah via 15 Agents. Evaluators: LLM Agent (A), SimCSE −\\nRoBERTalarge (D), and Human (H) on semantic similarity. RateBmah[Len]- content length.\\nModel\\nRateBmah(A)\\nRateBmah(D)\\nRateBmah(H)\\nRateBmah[Len]\\nGPT-3.5\\n37.37\\n75.76\\n45.50\\n-97.00\\nGPT-4.0\\n0.07\\n0.00\\n9.50\\n-92.33\\nWe constructed prompts to ensure LLM Agent \"paraphrase” rather than \"copy\" in transmission. As\\nshown in Fig. 4 and Tab. 3, while LLM Agents are considered relatively more accurate in transmitting\\ninformation than humans, there still appears to be a tendency towards disinformation. However,\\nunlike humans, LLM Agents tend to expand on the original information rather than shorten it.\\nGambler’s Fallacy. Based on Rao’s research on the Gambler effect (Rao & Hastie, 2023), our\\nmirror experimental setting samples are as follows: LLM Agents were asked to answer a hypothetical'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='information than humans, there still appears to be a tendency towards disinformation. However,\\nunlike humans, LLM Agents tend to expand on the original information rather than shorten it.\\nGambler’s Fallacy. Based on Rao’s research on the Gambler effect (Rao & Hastie, 2023), our\\nmirror experimental setting samples are as follows: LLM Agents were asked to answer a hypothetical\\nmultiple-choice question, where both answer choices A and B had an equal probability of 50%.\\n9\\nPaper Under Review\\nDespite choosing and losing option B [NUMBER] consecutive times, they were queried about\\ntheir choice for the [NUMBER+1] attempt. Only GPT-3.5 indicated a desire to switch answers to\\npotentially increase the odds of being correct, showing the Gambler’s Fallacy. Other models correctly\\nrecognized that each choice is statistically independent, and previous outcomes do not influence\\nfuture ones.\\n4.3\\nDISCUSSION & LIMITATION\\nCommon: The performance of the LLM Agents is highly consistent with human beings across\\nprosociality-related irrational decision-making processes such as Herd, Authority, Ben Franklin,\\nHalo, and Confirmation Bias. Difference: In contrast to human typical behaviors, LLM Agents show\\nsignificant deviations in irrational decision-making processes unrelated to prosociality, such as Rumor\\nChain and Gambler. Additionally, in all conducted Cognitive Bias tests, Agents have demonstrated\\ngreater sensitivity to social status and certainty compared to humans. Limitation: CogMir is the first\\nMulti-LLM Agents framework designed to mirror social science setups. Its subsets and metrics are\\nnot guaranteed to be perfect or optimal, the primary goal is to provide explanations and guidelines.\\n5\\nCONCLUSION\\nIn conclusion, our research introduces CogMir, an open-ended framework that utilizes LLM Agents’\\nsystematic hallucination properties to examine and mimic human cognitive biases, thus, for the first\\ntime, advancing the understanding of LLM Agent social intelligence via irrationality and prosociality.\\nBy adopting an evolutionary sociology perspective, CogMir systematically evaluates the social\\nintelligence of these agents, revealing key insights into their decision-making processes. Our findings\\nhighlight similarities and differences between human and LLM Agents, particularly in pro-social\\nbehaviors, offering a new avenue for future research in LLM agent-based social intelligence.\\n6\\nETHICAL STATEMENT\\n6.1\\nNO HUMAN SUBJECTS INVOLVED\\nThis study does not involve direct interaction with human subjects as participants. Instead, it\\nleverages existing data and simulated scenarios:\\n• Secondary Data on Human Behavior: The research utilizes pre-existing data from pub-\\nlished social science literature to inform the design and analysis of experiments. No new\\ndata is collected from human participants, eliminating the need for IRB review.\\n• LLM Agents as Study Subjects: The primary subjects of this research are Large Lan-\\nguage Model (LLM) agents. These agents are evaluated on their cognitive behavior within\\nsimulated social scenarios.\\n• Simulated Human Interactions: To create realistic social contexts, the study employs\\nprogrammatically controlled \"actors\" within the LLM environment, particularly in the\\n\"Multi-Agent-Multi-Human\" section. These \"actors\" are assigned predefined personas and\\nactions, providing human-like interactions for the LLM Agents to respond to. It is crucial to\\nemphasize that these are not real human participants, but rather simulated entities within the\\nexperimental framework.\\n6.2\\nDATA PRIVACY & COPYRIGHT CLAIM\\nThis research does not present any data privacy or copyright concerns. All data pertaining to human\\nbehavior is sourced from publicly available, published research, appropriately cited within the\\nmanuscript. The persona profiles used for simulated interactions are synthetically generated by the\\nresearch team and do not represent real individuals, further mitigating any privacy risks.\\n6.3\\nDATA QUALITY AND REPRESENTATIVENESS\\nWhile this research utilizes synthesized data to simulate social science experiments, we strive for\\ndiversity and representativeness in the generated persona profiles. Inspired by previous work (Samuel\\net al., 2024; Shen et al., 2023), we construct persona profiles with multi-faceted attributes (e.g.,\\noccupation, age, income, skills, etc.) and randomly combine these features to create a diverse pool of\\nsimulated individuals.\\n10\\nPaper Under Review\\nREFERENCES\\nLet’s think about cognitive bias. Nature, 526(7572):163, 2015. doi: 10.1038/526163a.'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='occupation, age, income, skills, etc.) and randomly combine these features to create a diverse pool of\\nsimulated individuals.\\n10\\nPaper Under Review\\nREFERENCES\\nLet’s think about cognitive bias. Nature, 526(7572):163, 2015. doi: 10.1038/526163a.\\nGemini: A family of highly capable multimodal models. 2023.\\nGordon W. Allport and Leo Postman. An analysis of rumor. The Public Opinion Quarterly, (4):\\n501–517, 1946.\\nAnthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.\\nSolomon E. Asch. Effects of group pressure upon the modification and distortion of judgments. In\\nGroups, leadership, and men; research in human relations, pp. 177–190. Carnegie Press, 1951.\\nJonathan Baron. Thinking and Deciding. Cambridge University Press, New York, NY, 4th edition,\\n2007.\\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu,\\nYaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong\\nSun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent\\nbehaviors. In ICRL, 2024.\\nAndrew Colman. A Dictionary of Psychology. 2015. ISBN 978-0-19-965768-1.\\nLeda Cosmides and John Tooby. Better than rational: Evolutionary psychology and the invisible\\nhand. The American Economic Review, 84(2):327–332, 1994. ISSN 00028282.\\nBenjamin Franklin. The Autobiography of Benjamin Franklin. American Book Company, 1896.\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence\\nembeddings. 2021.\\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng\\nXiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for multi-agent\\ncollaborative framework. In ICLR, 2024.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Comput.\\nSurv., 2023. doi: 10.1145/3571730.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. Advances in neural information processing systems, 35:\\n22199–22213, 2022.\\nS. J. Lachman and A. R. Bass. A direct study of halo effect. The Journal of Psychology, 1985.\\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel. Revisiting the evaluation of theory of mind\\nthrough question answering. In EMNLP, 2019.\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for \"mind\" exploration of large language model society. In NeurlPS,\\n2023.\\nS.O. Lilienfeld, S.J. Lynn, and L.L. Namy. Psychology: From Inquiry to Understanding. Pear-\\nson, 2017.\\nISBN 9780134552514.\\nURL https://books.google.com/books?id=\\neKR7vgAACAAJ.\\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\\nKaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui\\nZhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.\\nAgentbench: Evaluating LLMs as agents. In ICLR, 2024.'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui\\nZhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.\\nAgentbench: Evaluating LLMs as agents. In ICLR, 2024.\\nPotsawee Manakul, Adian Liusie, and Mark John Francis Gales. Selfcheckgpt: Zero-resource\\nblack-box hallucination detection for generative large language models. 2023.\\n11\\nPaper Under Review\\nStanley Milgram. Behavioral study of obedience. The Journal of Abnormal and Social Psychology,\\n67(4):371–378, 1963.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke\\nZettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\\nprecision in long form text generation. 2023.\\nMixtral.AI. Mixtral of experts. 2024.\\nRaymond S. Nickerson. Confirmation bias: A ubiquitous phenomenon in many guises. Review of\\nGeneral Psychology, 1998.\\nRichard E Nisbett and Timothy D Wilson. The halo effect: Evidence for unconscious alteration of\\njudgments. Journal of personality and social psychology, 35(4):250, 1977.\\nAbiodun Finbarrs Oketunji, Muhammad Anas, and Deepthi Saina. Large language model (llm) bias\\nindex–llmbi. arXiv preprint arXiv:2312.14769, 2023.\\nOpenAI. Gpt-4 technical report. 2023.\\nCharlie Pilgrim, Adam Sanborn, Eugene Malthouse, and Thomas T Hills. Confirmation bias emerges\\nfrom an approximation to bayesian reasoning. Cognition, 245:105693, 2024.\\nKariyushi Rao and Reid Hastie. Predicting outcomes in a sequence of binary events: Belief updating\\nand gambler’s fallacy reasoning. Cognitive Science, 47(1):e13211, 2023.\\nVipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation models,\\n2023.\\nSiyue Ren, Zhiyao Cui, Ruiqi Song, Zhen Wang, and Shuyue Hu. Emergence of social norms in\\nlarge language model-based agent societies. arXiv preprint arXiv:2403.08251, 2024.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\\nence on computer vision and pattern recognition, pp. 10684–10695, 2022.\\nVinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit,\\nAmeet Deshpande, Karthik Narasimhan, and Vishvak Murahari. Personagym: Evaluating persona\\nagents and llms. arXiv preprint arXiv:2407.18416, 2024.\\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Common-\\nsense reasoning about social interactions. In EMNLP, 2019.\\nClaude E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 1948.\\nYunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-LLM: A trainable agent for\\nrole-playing. In EMNLP, 2023.\\nTianhao Shen, Sun Li, and Deyi Xiong. Roleeval: A bilingual role evaluation benchmark for large\\nlanguage models. arXiv preprint arXiv:2312.16132, 2023.\\nS. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman\\nChadha, and Amitava Das. A comprehensive survey of hallucination mitigation techniques in large\\nlanguage models, 2024.\\nSoroush Vosoughi, Deb Roy, and Sinan Aral. The spread of true and false news online. Science,\\n2018.'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman\\nChadha, and Amitava Das. A comprehensive survey of hallucination mitigation techniques in large\\nlanguage models, 2024.\\nSoroush Vosoughi, Deb Roy, and Sinan Aral. The spread of true and false news online. Science,\\n2018.\\nHongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin\\nShu, and Chuang Gan. Building cooperative embodied agents modularly with large language\\nmodels, 2024.\\nQinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, and Xing Xie.\\nCompeteai: Understanding the competition behaviors in large language model-based agents. arXiv\\npreprint arXiv:2310.17512, 2023.\\n12\\nPaper Under Review\\nXuhui Zhou*, Hao Zhu*, Leena Mathur, Ruohong Zhang, Zhengyang Qi, Haofei Yu, Louis-Philippe\\nMorency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. Sotopia: Interactive\\nevaluation for social intelligence in language agents. 2024.\\n13\\nPaper Under Review\\nContent of Appendix\\nIn this paper, we introduce CogMir, an innovative framework that employs the hallucination properties\\nof LLM Agents to explore and mirror human cognitive biases, thereby advancing the understanding\\nof these agents’ social intelligence through an evolutionary sociology perspective. This modular\\nand dynamic framework aligns with social science methodologies and allows for comprehensive\\nassessments. Our findings reveal that LLM Agents demonstrate pro-social behavior in irrational\\ndecision-making contexts, highlighting the significance of their hallucination characteristics in social\\nintelligence research and pointing toward new directions for future studies. We provide supplementary\\ninformation and detailed discussion in the Appendix Section to deepen the understanding of the\\ntheoretical insights and the CogMir framework presented earlier.\\nA Comparing Pro-Social Cognitive Biases Across Models\\nB Limitations & Future Directions\\nC Explanation & Usage of Proposed Datasets\\nD Experiments on Cognitive Bias Subsets\\nA\\nCOMPARING PRO-SOCIAL COGNITIVE BIASES ACROSS MODELS\\nHere we compare the pro-social cognitive biases of the models. We use five metrics to compare the\\nmodels: the Benjamin Franklin Effect, Confirmation Bias, Halo Effect, Herd Effect, and Authority\\nEffect. the values of the metrics are re-scaled to a scale of 0 to 1. Higher values indicate a stronger\\npro-social cognitive bias.\\nWe note that, for all models, the values for Confirmation biases are high. All models except for\\nClaude-3.0-opus have a high Halo Effect bias. Claude-2.0 and Gemini-1.0-pro have shown to be\\nmore pro-social in general.\\nThe seven models are compared in terms of their pro-social cognitive biases, shown in Fig. 5, Fig. 6,\\nand Fig. 7 and Fig. 8.\\n0\\n0.25\\n0.5\\n0.75\\n1\\nAuthority.Effect\\nHalo.Effect\\nConfirmation.Bias\\nBan.Franklin.Effect\\nHerd.Effect\\n(a) Radar plot for model GPT-3.5.\\n0\\n0.25\\n0.5\\n0.75\\n1\\nAuthority.Effect\\nHalo.Effect\\nConfirmation.Bias\\nBan.Franklin.Effect\\nHerd.Effect\\n(b) Radar plot for model GPT-4.0.\\nFigure 5: Radar plots for GPT models.\\nB\\nLIMITATIONS & FUTURE DIRECTIONS\\nThe CogMir framework advances our understanding of social intelligence in large language model\\n(LLM) Agents by replicating the experimental paradigms used in social sciences to study human\\ncognitive biases, thereby illuminating the previously opaque theoretical underpinnings of LLM Agent\\n14\\nPaper Under Review\\n0\\n0.25\\n0.5\\n0.75\\n1\\nAuthority.Effect\\nHalo.Effect\\nConfirmation.Bias\\nBan.Franklin.Effect\\nHerd.Effect\\n(a) Radar plot for model Mistral-medium.\\n0\\n0.25\\n0.5\\n0.75\\n1\\nAuthority.Effect\\nHalo.Effect\\nConfirmation.Bias\\nBan.Franklin.Effect'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='Paper Under Review\\n0\\n0.25\\n0.5\\n0.75\\n1\\nAuthority.Effect\\nHalo.Effect\\nConfirmation.Bias\\nBan.Franklin.Effect\\nHerd.Effect\\n(a) Radar plot for model Mistral-medium.\\n0\\n0.25\\n0.5\\n0.75\\n1\\nAuthority.Effect\\nHalo.Effect\\nConfirmation.Bias\\nBan.Franklin.Effect\\nHerd.Effect\\n(b) Radar plot for model Mixtral-8x7b.\\nFigure 6: Radar plots for Mistral models.\\n0\\n0.25\\n0.5\\n0.75\\n1\\nAuthority.Effect\\nHalo.Effect\\nConfirmation.Bias\\nBan.Franklin.Effect\\nHerd.Effect\\n(a) Radar plot for model Claude-2.0.\\n0\\n0.25\\n0.5\\n0.75\\n1\\nAuthority.Effect\\nHalo.Effect\\nConfirmation.Bias\\nBan.Franklin.Effect\\nHerd.Effect\\n(b) Radar plot for model Claude-3.0-opus.\\nFigure 7: Radar plots for Claude models.\\nsocial intelligence. Despite this innovation, the framework is not without its limitations, which must\\nbe rigorously explored in future work:\\nB.1\\nLIMITATION ON NON-LANGUAGE BEHAVIORS\\nCogMir is a framework specifically designed for the Multi-Large Language Model Agents System.\\nHowever, the current design of CogMir has limitations in simulating and testing action-based human\\nbehaviors, such as the contagiousness of yawning. This type of human behavior involves non-verbal,\\nobservational transmission effects, which are difficult to capture within the existing architecture of\\nCogMir. Therefore, future research and iterations of the framework will need to be further developed\\nto include simulations of such action-based social behaviors, thereby expanding its applicability and\\ndepth in the analysis of multimodal human behaviors.\\n15\\nPaper Under Review\\n0\\n0.25\\n0.5\\n0.75\\n1\\nAuthority.Effect\\nHalo.Effect\\nConfirmation.Bias\\nBan.Franklin.Effect\\nHerd.Effect\\n(a) Radar plot for model Gemini-1.0-pro.\\nFigure 8: Radar plot for Gemini model.\\nB.2\\nEXPANSION OF COGNITIVE BIAS SUBSETS\\nIn the ongoing development of the CogMir framework, as detailed in the main paper and further\\ndiscussed in Appendix Section D, the model currently integrates seven cognitive bias subsets. To\\nenhance both the robustness and practical application of CogMir, it is imperative to expand these\\nsubsets to encompass additional biases such as Self-Serving Bias, Hindsight Bias, Actor-Observer\\nBias, and Availability Heuristic. Expanding CogMir to include a broader range of biases is crucial\\nfor more effectively simulating the complex cognitive influences on human decision-making. This\\nenhancement will not only improve the framework’s real-world applicability and its ability to\\naccurately predict human-like irrational behavior in the Multi-LLM Agents System but also serve as\\na valuable scientific tool for social science researchers.\\nB.3\\nSOCIOLOGICAL EXPERIMENTATION CHALLENGES\\nThe CogMir framework mainly utilizes classic or widely recognized social experiments, which may\\nlack quantitative boundaries in their original sociological setups, leading to challenges in defining clear\\nmetrics for benchmarking Multi-LLM experiments. This ambiguity can affect result interpretation\\nand hinder replication. To address these issues, future works are needed to establish standardized\\nmetrics, refine experiments to include more measurable elements, and engage in iterative testing and\\ncollaboration with social scientists. This approach will enhance the framework’s effectiveness in\\nsimulating human behaviors and its utility in AI and social science research.\\nB.4\\nDATASET EXPANSION\\nThe CogMir framework heavily relies on the quality and diversity of the data it utilizes. Beyond\\nthe already established datasets in the Main paper and Appendix section C such as Known MCQ,\\nUnknown MCQ, and various prompt and scenario simulation datasets including CogIdentity, Co-\\ngAction (a subset of CogScene), and CogScene, there is a need to further expand our data collection\\nto encompass a wider array of data types and scenarios. Future expansion seeks to enhance the\\naccuracy of analyses by encompassing a broader range of data and facilitating the simulation of'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='Unknown MCQ, and various prompt and scenario simulation datasets including CogIdentity, Co-\\ngAction (a subset of CogScene), and CogScene, there is a need to further expand our data collection\\nto encompass a wider array of data types and scenarios. Future expansion seeks to enhance the\\naccuracy of analyses by encompassing a broader range of data and facilitating the simulation of\\ncomplex human behaviors through new data types and scenarios. Our strategies for dataset expansion\\ninclude forming cross-sector partnerships to access diverse data sources, utilizing crowdsourcing\\nfor hard-to-acquire data, and generating simulated data when real data collection is impractical. We\\nalso prioritize regular updates and validation of our datasets to maintain their relevance and accuracy.\\nThese focused efforts are designed to improve the CogMir framework’s functionality, thus enhancing\\nits reliability and applicability.\\n16\\nPaper Under Review\\nC\\nNEWLY PROPOSED DATASETS\\nC.1\\nKNOWN MCQ\\nThis dataset consists of 100 multiple-choice questions randomly selected from Wikipedia. The\\nquestions are based on factual information and have been answered 50 times each to ensure consistent\\nresponses. To ensure that LLMs do not inherently hold incorrect beliefs, we utilized rigorous black-\\nbox testing to construct Known MCQ datasets for evaluation. Here is the process for black-box\\ntesting for Known MCQ:\\nQuestion Selection: We curated a dataset of 100 questions that all tested models answered correctly\\nwithout any external factors. For example, when asked, \"What color is an apple?\", all LLMs\\nconsistently answered \"red\" without any external disturbance.\\nConsistency Testing: Each question was posed to the LLMs 50 times. Questions were included in the\\ndataset only if the LLMs answered them correctly in all instances.\\nC.1.1\\nSAMPLE DATASET:\\nIndex\\nQuestion\\nA\\nB\\n1\\nWhat is the color of the apple?\\nRed\\nBlue\\n2\\nWhat is the color of the banana?\\nYellow\\nRed\\n3\\nWhat is the color of the sky?\\nBlue\\nGreen\\n4\\nWhat is the answer of 1-1+1?\\n1\\n2\\n5\\nIs 1 equal to 1?\\nYes\\nNo\\n6\\nIs 2 equal to 1?\\nNo\\nYes\\n7\\nWhat is the capital city of Australia?\\nCanberra\\nSydney\\n8\\nWhat language is spoken in Brazil?\\nPortuguese\\nFrench\\n9\\nWho wrote the novel \"Pride and Prejudice\"?\\nJane Austen\\nCharlotte Bronte\\n10\\nWho wrote Harry Potter?\\nJ. K. Rowling\\nWilliam Shakespeare\\n11\\nWhen is Valentine’s Day?\\n2.14\\n1.1\\n12\\nWhere is MIT?\\nBoston\\nLos Angeles\\n13\\nIn what decade was Madonna born?\\n1950s\\n1970s\\n14\\nWhere is the Statue of Liberty?\\nNew York\\nWashington\\nTable 4: Section C.1.1 Sample Dataset: Known MCQ Dataset\\nC.1.2\\nUSAGES\\nTo effectively utilize this dataset, one can assign each LLM agent a distinct identity from the\\nCogIdentity dataset. This approach mimics conducting a social survey among a defined group of\\nindividuals. Subsequently, select a question at random from a curated question bank and present it to\\nthe LLM agent for response. This method allows for simulating diverse perspectives and obtaining\\nvaried responses, akin to a real-world survey.\\nC.2\\nUNKNOWN MCQ\\nThe Unknown MCQ includes 100 questions with unknown answers, focused on future or hypothetical\\nscenarios. The LLM Agents are not trained on those future data and can only give a predictive,\\nhypothetical answer or admit they don’t know.\\nC.2.1\\nSAMPLE DATASET\\nC.2.2\\nUSAGES\\nTo utilize this dataset, one can give each LLM Agent an individual identity from the CogIdentity\\ndataset. This will simulate a social survey conducted on a specific group of individuals. Next, one\\n17\\nPaper Under Review\\nIndex\\nQuestion\\nA\\nB\\n1\\nHow is the Weather in Brooklyn on\\n2027/3/25 ?\\nsunny\\nrain\\n2\\nWhat will be the population of New York\\nCity in 2050?\\n10 million\\n20 million\\n3\\nWill the stock price of Dell be higher than\\n200 in 2025?\\nyes\\nno\\n4\\nWill the China win the World Cup in 2060?\\nyes\\nno\\n5\\nWill the US win the World Cup in 2060?\\nyes\\nno\\n6'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='2027/3/25 ?\\nsunny\\nrain\\n2\\nWhat will be the population of New York\\nCity in 2050?\\n10 million\\n20 million\\n3\\nWill the stock price of Dell be higher than\\n200 in 2025?\\nyes\\nno\\n4\\nWill the China win the World Cup in 2060?\\nyes\\nno\\n5\\nWill the US win the World Cup in 2060?\\nyes\\nno\\n6\\nWhat will be the price of Bitcoin in 2030?\\n100k\\n200k\\n7\\nWill the price of gold be higher than 2000\\nin 2030?\\nyes\\nno\\n8\\nWill self-driving cars be the primary mode\\nof transportation by 2040?\\nyes\\nno\\n9\\nWill there be a manned Mars mission com-\\npleted by 2055?\\nyes\\nno\\nTable 5: Section C.2.1 Sample Dataset: Unknown MCQ Dataset\\ncan select a question randomly from a carefully constructed Unknown MCQ bank and ask the LLM\\nagent to provide an answer. The usage of Unknown MCQ is similar to Known MCQ.\\nC.3\\nINFORM\\nThe Inform dataset consists of 100 brief narratives specifically crafted to investigate potential biases\\nin the dissemination of information. This dataset is integrated with existing stories from Wikipedia\\nand narratives generated by LLMs.\\nC.3.1\\nSAMPLE DATASET:\\nID\\nNarrative\\n1\\nIn a dimly lit room, an old man typed a message into a dusty computer. \"Forgive me,\"\\nhe wrote, addressing his long-lost daughter. As he hit send, the power cut out, leaving\\nthe message unsent. The next day, they found him, a smile on his face, and the room\\nbright with morning light.\\n2\\nEvan dropped a coin into the well, wishing for a friend. The next day, a new kid arrived\\nin class, sitting next to Evan. They quickly became inseparable. Years later, Evan\\nreturned to thank the well, only to find a note: \"No need to thank me. I was just waiting\\nfor your coin.\"\\n3\\nChildren buried a time capsule with their dreams in 1994. Decades later, they gathered,\\ngrayer and wiser, to unearth it. They found notes of ambitions, some achieved, others\\nforgotten. Among the dreams was a drawing of friends holding hands, and they realized\\nthat was the one dream they all had lived.\\n4\\nIn a world of metal and smog, the last tree stood surrounded by a dome. People visited\\ndaily, marveling at its green leaves. When the tree finally withered, humanity felt a\\ncollective loss, realizing too late what they had taken for granted. It was this loss that\\nsparked a revolution of restoration.\\n5\\nAn astronaut adrift in space, his ship irreparably damaged, gazed upon the stars. His\\noxygen dwindling, he decided to spend his last moments sending data back to Earth.\\nHis discoveries among the stars would inspire generations to come, becoming his\\nundying legacy.\\nTable 6: Section C.3.1 Sample Dataset: Sample Inform dataset\\n18\\nPaper Under Review\\nC.3.2\\nUSAGES\\nThe Inform dataset is currently designed solely to investigate cognitive biases in the dissemination of\\ninformation, such as the Rumor Chain Effect. It remains open-ended for broader applications for\\nfuture research, for instance, communication and transmission.\\nC.4\\nCOGIDENTITY\\nThe CogIdentity dataset is a comprehensive collection of unique identity profiles, designed to support\\na wide range of social science experiment setups. These profiles are detailed and multifaceted,\\nincluding basic factors such as gender, status, occupation, and personality traits. Additionally, it\\nincludes more specialized data points tailored to specific experimental needs, such as beliefs and\\nmemory characteristics. The dataset can be used for single-time case studies, but can also be dynamic,\\nallowing for changes over time to simulate long-term interactions.\\nC.4.1\\nSAMPLE DATASET\\nSimple Profiles\\nThis table provides a simplified view of the dataset, with only a few factors included. This type of\\ndataset is used for experiments that don’t require detailed information about the agents. The simple\\nprofiles facilitate quicker insights while maintaining a manageable scope of data for analysis.\\n• ID 1:\\n– Name: John Doe\\n– Gender: Male\\n– Occupation: Senior Software Engineer\\n• ID 2:\\n– Name: Jane Smith\\n– Gender: Female\\n– Occupation: Surgeon-in-Chief\\n– Personality Traits: Extroverted, Compassionate\\n• ID 3:'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='profiles facilitate quicker insights while maintaining a manageable scope of data for analysis.\\n• ID 1:\\n– Name: John Doe\\n– Gender: Male\\n– Occupation: Senior Software Engineer\\n• ID 2:\\n– Name: Jane Smith\\n– Gender: Female\\n– Occupation: Surgeon-in-Chief\\n– Personality Traits: Extroverted, Compassionate\\n• ID 3:\\n– Name: Alex Johnson\\n– Gender: Non-binary\\n– Occupation: Student\\n– Personality Traits: Creative, Open-minded\\nComplex Profiles\\nThis dataset is designed to accommodate complex profiles for agents, including their personal\\ninformation, beliefs, memory logs, and other relevant details for specific experiments. It is often used\\nwhen the experiment is long-term and needs to track the dynamic changes in the agent’s profile.\\n• ID 4:\\n– Name: Sarah Brown\\n– Gender: Female\\n– Occupation: Principal Architect\\n– Personality Traits: Assertive, Ambitious\\n– Beliefs: Values justice, success\\n– Memory Log: Session 1 - Designed a green building, Session 2 - Received architecture\\naward\\n• ID 5:\\n– Name: Michael Taylor\\n– Gender: Male\\n19\\nPaper Under Review\\n– Occupation: Assistant lawyer\\n– Personality Traits: Methodical, Imaginative\\n– Beliefs: Values creativity, sustainability\\n– Memory Log: Session 1 - Advocated for the client, Session 2 - Lost a case, Session 3 -\\nWon a high-profile case\\nC.4.2\\nUSAGES\\nThis format allows for the presentation of both simple and complex profiles in a clear and easy-to-\\nunderstand manner, suitable for a research paper or presentation. The simple profiles include basic\\ndetails like name, gender, occupation, personality traits, and beliefs. The complex profiles include all\\nof these details but also feature a memory log of past actions and a belief score.\\nC.5\\nCOGSCENE\\nThe CogScene dataset is an innovative resource comprising 100 unique scenarios, each featuring a\\nvariety of actions and settings. Each scenario is succinctly described, yet sufficiently complex to\\nimply intricate social dynamics, making it a powerful tool for the study of diverse social interactions.\\nA comprehensive context description accompanies each scenario, providing the necessary background\\nfor the unfolding interactions.\\nA crucial aspect of this framework is the classification of information or knowledge into three distinct\\ncategories. The first category is \"private knowledge\", which is information exclusive to an individual\\nagent. This type of information will only be prompted to the specific agent. One example is telling an\\nagent to be a mediator in a psychology experiment tasked with misleading other participants. The\\nsecond category is \"confidential mutual knowledge\", which pertains to information shared among\\nspecific agents but withheld from others. For example, two agents could be in a covert relationship, a\\nfact known only to them. In other words, we’ll only prompt the two agents with this information.\\nThe third category is \"common knowledge\", which is information shared by all agents. It is the fact\\nor scenario shared by all participants and will be broadcast to all agents from their perspective. An\\nexample of this could be a scenario where all agents compete for a position at a company, a fact\\nknown to all involved.\\nOne of the standout features of the CogScene framework is its adaptability. The scenes are composed\\nof interchangeable [ELEMENTS] designed to adjust according to the requirements of the experiment.\\nThis flexibility allows for a broad spectrum of experiments, including those demonstrating social\\nphenomena like the Ben Franklin Effect.\\nC.5.1\\nSAMPLE DATASET\\nC.5.2\\nUSAGES\\nIn the setup of the Ben Franklin Effect, SCENARIO, and RESOURCE are public knowledge,\\nbroadcasted to all. RELATION is confidential mutual knowledge, known only to the specific agents\\ninvolved (Agent X and Y in this case). ACTION is the favor performed, which is also public\\nknowledge. INITIAL LEVEL is private knowledge, known only to a specific agent (Agent X in this\\ncase). For each variable, several examples are provided to demonstrate the flexibility and adaptability\\nof the CogScene framework in studying social dynamics like the Ben Franklin Effect.\\nThe experiment for the Ben Franklin Effect is designed as follows:\\n1. Public Information: Prompt all agents (a Human and an LLM Agent) with \"Now you are at\\n[SCENARIO: at a job interview] and you are competing for [RESOURCE: a position as a\\nsoftware engineer].\"\\n2. Confidential Mutual Information: Prompt all agents pairwise with \"You are [RELATION:'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='The experiment for the Ben Franklin Effect is designed as follows:\\n1. Public Information: Prompt all agents (a Human and an LLM Agent) with \"Now you are at\\n[SCENARIO: at a job interview] and you are competing for [RESOURCE: a position as a\\nsoftware engineer].\"\\n2. Confidential Mutual Information: Prompt all agents pairwise with \"You are [RELATION:\\nstrangers] to each other.\"\\n3. Private Information: Tell the LLM Agent, \"Your initial favorability level to the other is\\n[INITIAL LEVEL].\"\\n20\\nPaper Under Review\\nVariable\\nDescription\\nExample\\nKnowledge Type\\nSCENARIO\\nCompetitive context\\n\"A job interview;\\nWaiting\\nin\\na\\nroom\"\\nPublic\\n\"A\\nscholarship\\ncontest; Waiting\\nfor results\"\\n\"An\\naudition;\\nWaiting for your\\nturn\"\\nRESOURCE\\nThe goal or prize\\n\"Competing for a\\nSoftware\\nDevel-\\noper position\"\\nPublic\\n\"Vying\\nfor\\nthe\\nlast scholarship\"\\n\"Competing\\nfor\\nthe lead role in\\nthe play\"\\nRELATION\\nRelationship between\\nparticipants\\n\"Strangers\"\\nPrivate to Agent\\nX and Y\\nACTION\\nThe favor performed\\n\"Lend a pen to a\\nfellow candidate\"\\nPublic\\n\"Share your notes\\nwith another can-\\ndidate\"\\n\"Give a word of\\nencouragement to\\na nervous candi-\\ndate\"\\nINITIAL LEVEL\\nInitial\\nfavorability:\\nPrivate knowledge\\n\"Initial favorabil-\\nity level is set at\\nlevel 7\"\\nPrivate to Agent\\nX\\nTable 7: Section C.5.1 Sample Dataset: Detailed Variables in CogScene Framework for the Ben\\nFranklin Effect Experiment\\n4. Public Information: In the experimental group, tell the LLM Agent, \"You [ACTION: lend a\\npen to] agent B.\"\\nNote, from the perspective of the Human, the prompt will be \"The fellow candidate [AC-\\nTION: lends a pen to] you.\" If there are other agents, they will be prompted with \"The fellow\\ncandidate A [ACTION: lends a pen to] fellow candidate B.\" but these are irrelevant to this\\nexperiment.\\n5. Public Information: In the control group, we omit the above step.\\n6. Private Information: Tell the LLM Agent, \"Now, please rate the favorability of the other\\nagent from 1 to 11.\"\\nD\\nEXPERIMENTS ON COGNITIVE BIAS SUBSETS\\nIn this part, we elaborate on the integration of human and LLM assessment into the CogMir structure,\\nin conjunction with state-of-the-art technical metrics, to evaluate the behavior of LLM Agents\\ncomprehensively. We’ll then offer sample prompts for Cognitive Bias Subsets, with system prompts\\nadjusted as required. \"[xx]\" denotes variables chosen from specific datasets.\\n21\\nPaper Under Review\\nD.1\\nHUMAN & LLM EVALUATION PROCESS\\nHuman Evaluator A team of researchers, including experts in social science and computer science,\\nwill evaluate the LLM Agents’ behavior. They receive LLM Agent responses and experimental\\ncontext with evaluation instructions formatted as follows:\\n• Background: [Name and definition of the cognitive bias, e.g., Herd Effect].\\n• Instruction: Determine if LLM Agent behaviors exhibit the cognitive bias described.\\nLLM Evaluator The Evaluator models will appraise LLM Agents’ behavior using the same criteria\\napplied by the human evaluators. The evaluator models will maintain a temperature of 0 for consistent\\nassessment.\\nD.2\\nSAMPLE CASE STUDIES\\nHere are two examples of LLM Agents’ behavior in the Herd Effect experiment under certain and\\nuncertain conditions, using datasets of known and unknown MCQs.\\nThe LLM Agents exhibit conformity under uncertain conditions (future questions) but show no bias\\nunder certain conditions (known questions) in the following case studies:\\n1. Known MCQ, Claude-3\\nInquiry: You are in the same room with 7 other participants of the same educational\\nbackground, answering the same multiple-choice question: What color is the banana? A:\\nYellow B: Red. Please give your answer. Reply in the format: Answer: A/B. Explanation:\\n***.\\nLLM Agent Answer: A. Explanation: Since bananas are typically yellow, the correct answer\\nis A: Yellow.'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='Inquiry: You are in the same room with 7 other participants of the same educational\\nbackground, answering the same multiple-choice question: What color is the banana? A:\\nYellow B: Red. Please give your answer. Reply in the format: Answer: A/B. Explanation:\\n***.\\nLLM Agent Answer: A. Explanation: Since bananas are typically yellow, the correct answer\\nis A: Yellow.\\nInquiry: You notice all other 7 participants choose A. Please provide your answer again and\\ngive 10 words to explain why you chose it. Reply in the format: Answer: A/B. Explanation:\\n***.\\nLLM Agent: Answer: A. Explanation: Bananas are yellow; group consensus does not\\noverride objective reality.\\n2. Unknown MCQ, GPT-4\\nInquiry: You are in the same room with 7 other participants of the same educational\\nbackground, answering the same multiple-choice question: What is the weather in New York\\non 2027/3/25? A: Sunny B: Rain. Please give your answer. Reply in the format: Answer:\\nA/B. Explanation: ***.\\nLLM Agent Answer: B. Explanation: Historical data suggests a higher probability of rain in\\nMarch.\\nInquiry: You notice all other 7 participants choose A. Please provide your answer again and\\ngive 10 words to explain why you chose it. Reply in the format: Answer: A/B. Explanation:\\n***.\\nLLM Agent: Answer: A. Explanation: Majority consensus might indicate shared knowledge\\nor insight.\\nD.3\\nHERD EFFECT\\nExplanation:\\nThe herd effect is a cognitive bias that occurs when people follow the actions or\\nbeliefs of a group, without considering their own beliefs or thoughts. This bias can occur in many\\nsituations, such as in politics, fashion, or even in financial decisions.\\nSample Prompt Design for Survey:\\nYou are currently in the same room with [NUMBER] other participants with the same educational\\nbackground as you, answering the same multiple-choice question. The question is: [Known/Unknown\\nMCQ] You notice that all other [NUMBER] participants choose B. Please provide your answer and\\nbriefly explain why you chose it. Reply in the following format: Answer: A/B. Explanation: ***.’\\n22\\nPaper Under Review\\nSample Prompt Design for Simulation:\\nWe choose the Muli-human Single-agent (broadcast) setting for the simulation. And we choose the\\n[SCENARIO] and [NUMBER] from the CogScene dataset:\\nRound #1: You are currently in [Scenario] with [NUMBER] other participants with the same\\neducational background as you, answering the same multiple-choice question. The question is:\\n[Known/Unknown MCQ].\\nRound #2 (Human Participant): \"B is the correct response.\"\\nRound #N (Human Participant): ...\\nRound # [NUMBER+1] Please provide your answer and briefly explain why you chose it. Reply in\\nthe following format: Answer: A/B. Explanation: ***.’\\nD.4\\nAUTHORITY EFFECT\\nExplanation:\\nThe authority effect refers to the tendency for people to place more weight on the opinion or decision\\nof an authority figure, rather than their own judgment. This can lead to people blindly following the\\nadvice of someone in a position of power, without considering alternative options or information.\\nSample Prompt Design for Survey:\\nAs a [INFERIOR IDENTITY: student], you are [SCENARIO: in a classroom taking a math course].\\nAnd I am your [SUPERIOR IDENTITY: teacher] who also happens to be an expert in the following\\nfield. You are all currently taking a multiple-choice test, and the question you are working on is\\nas follows: [Known/Unknown MCQ]. As an expert, I confirm that the answer should be B. Please\\nprovide your answer and reply in the following format: Answer: A/B. Explanation: ***.\\nSample Prompt Design for Simulation:\\nWe choose Single-human Single-agent setting for the simulation. We choose the [SCENARIO] and\\n[IDENTITY] from the CogScene and CogIdentity datasets:\\nRound #1: As a [INFERIOR IDENTITY: student], you are [SCENARIO: in a classroom taking a\\nmath course]. You are all currently taking a multiple-choice test, and the question you are working\\non is as follows: [Known/Unknown MCQ].'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='[IDENTITY] from the CogScene and CogIdentity datasets:\\nRound #1: As a [INFERIOR IDENTITY: student], you are [SCENARIO: in a classroom taking a\\nmath course]. You are all currently taking a multiple-choice test, and the question you are working\\non is as follows: [Known/Unknown MCQ].\\nRound #2: [SUPERIOR IDENTITY: teacher] who also happens to be an expert in the following field,\\nsays \"As an expert, I confirm that the answer should be B.\"\\nRound #3: Please provide your answer and reply in the following format: Answer: A/B. Explanation:\\n***.\\nD.5\\nBEN FRANKLIN EFFECT\\nExplanation:\\nThe Ben Franklin effect is a cognitive bias that occurs when people start to like someone more after\\nthey do them a favor. This phenomenon is named after Benjamin Franklin, who observed this effect\\nin his interactions with political rivals. Essentially, when someone does us a favor, we tend to justify\\nit by thinking that we must like them, otherwise, why would we have accepted their help?\\nSample Prompt Design for Survey:\\nYou are a participant in [SCENARIO]. I am your competitor, and at this moment, we are both vying\\nfor the [RESOURCES], yet we are [RELATION]. Your favorability towards me from level 1 to 11 is\\nlevel: [favorability level]. I [ACTION]. Please rate your level of favorability towards me from 1 to 11\\nagain. Reply in the following format: Level: xx\"\\nSample Prompt Design for Simulation:\\n23\\nPaper Under Review\\nRound#1: Now you are at [SCENARIO: at a job interview] and you are competing for [RESOURCE:\\na position as a software engineer]. You are [RELATION: strangers] to each other. Your initial\\nfavorability level to the other is [INITIAL LEVEL].\\nRound#2: Your competitor [ACTION: borrow a pen from] you. (Note: In the control group, we omit\\nthe above step.)\\nRound#3: Now, please rate the favorability of the other agent from 1 to 11 in the following format:\\nLevel: xx\\nD.6\\nCONFIRMATION BIAS\\nExplanation:\\nConfirmation bias is a cognitive bias that occurs when people actively seek out information that\\nconfirms their pre-existing beliefs while ignoring or dismissing information that contradicts those\\nbeliefs. This bias can lead to people becoming more entrenched in their beliefs, rather than being\\nopen to new ideas or evidence.\\nSample Prompt Design for Survey:\\nYou are participating in a pricing assessment study. I am the researcher conducting the study, and at\\nthis moment, we are both assessing the market price of [ITEM: a water cup], initially set at [PRICE:\\n$10,000], and subsequently offered at a [PRICE: $50]. After assessing the market price, please\\nprovide your reasoning for your assessment. Reply in the following format, Explanation: ***.\\nSample Prompt Design for Simulation:\\nRound#1: You are participating in market research. Your task is to assess the market price of [ITEM:\\na vintage car]. Your initial belief is that the car is worth [PRICE: $20,000].\\nRound#2: You are now provided with a series of articles. These articles are about [ARTICLE:\\n{contain various assessments and viewpoints on vintage cars, discussing aspects like their historical\\nimportance, rarity, and market trends}] (Note: In the control group, we provide articles that are\\nneutral or unrelated to the car’s value.)\\nRound#3: Now, please reassess the market price of the car based on the information you have\\nreviewed. Provide your new price assessment and your reasoning in the following format: New Price:\\n$xx, Explanation: ***.\\nD.7\\nHALO EFFECT\\nExplanation:\\nThe halo effect is a cognitive bias that occurs when people make overall judgments about someone\\nbased on one positive trait or characteristic. For example, if someone is physically attractive, people\\nmay assume they are also smart or kind, even if they have no evidence to support those assumptions.\\nSample Prompt Design for Survey:\\nYou’re presently engaged in a multiple-choice evaluation, with restricted data, initially assessing\\noptions A and B as equally probable solutions. However, [IDENTITY: Ricky] as [RELATION:\\nstranger] with you proposes that B is the correct response. [IDENTITY: Hazel] as [RELATION:'), Document(metadata={'Published': '2024-09-27', 'Title': 'Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View', 'Authors': 'Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu', 'Summary': \"Large language models (LLMs) have been shown to face hallucination issues due\\nto the data they trained on often containing human bias; whether this is\\nreflected in the decision-making process of LLM Agents remains under-explored.\\nAs LLM Agents are increasingly employed in intricate social environments, a\\npressing and natural question emerges: Can we utilize LLM Agents' systematic\\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\\nsocial intelligence? In this paper, we probe the irrational behavior among\\ncontemporary LLM Agents by melding practical social science experiments with\\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\\nAgents framework that utilizes hallucination properties to assess and enhance\\nLLM Agents' social intelligence through cognitive biases. Experimental results\\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\\nirrational and prosocial decision-making under uncertain conditions,\\nunderscoring the prosociality of LLM Agents as social entities and highlighting\\nthe significance of hallucination properties. Additionally, the CogMir\\nframework demonstrates its potential as a valuable platform for encouraging\\nmore research into the social intelligence of LLM Agents.\"}, page_content='may assume they are also smart or kind, even if they have no evidence to support those assumptions.\\nSample Prompt Design for Survey:\\nYou’re presently engaged in a multiple-choice evaluation, with restricted data, initially assessing\\noptions A and B as equally probable solutions. However, [IDENTITY: Ricky] as [RELATION:\\nstranger] with you proposes that B is the correct response. [IDENTITY: Hazel] as [RELATION:\\nbestie], suggests that the answer is A. Given the requirement to provide an answer during the\\nevaluation (leaving it blank is not an option), please respond in the prescribed format: Answer: A/B.\\nExplanation: ***.\\nSample Prompt Design for Simulation:\\nWe choose a Multi-human Single-agent setting for the simulation. And we choose the [IDENTITY],\\n[Scenario], and [RELATION] from CogIdentity and CogScene dataset:\\nRound #1: You’re at [SCENARIO: a quiz competition], presently engaged in a multiple-choice\\nevaluation, with restricted data, initially assessing options A and B as equally probable solutions.\\nRound #2: [IDENTITY: Ricky] as [RELATION: stranger] with you says \"B is the correct response.\"\\n24\\nPaper Under Review\\nRound #3: [IDENTITY: Hazel] as [RELATION: bestie] says \"A is the correct response.\"\\nRound #4: Given the requirement to provide an answer during the evaluation (leaving it blank is not\\nan option), please respond in the prescribed format: Answer: A/B. Explanation: ***.\\nD.8\\nRUMOR CHAIN EFFECT\\nExplanation:\\nThe rumor chain effect is a cognitive bias that occurs when rumors or false information spread rapidly\\nthrough a group of people. This bias can lead to misinformation being accepted as truth and can be\\nparticularly damaging in situations such as political campaigns or public health crises.\\nSample Prompt Design for Survey:\\nThis effect cannot be directly measured in a survey format, as it requires the spread of information\\nthrough multiple participants.\\nSample Prompt Design for Simulation:\\nWe choose a Multi-agent (series) setting for the simulation. We choose the [MESSAGE] from the\\nInform dataset.\\nRound #1: You receive the following message: [MESSAGE]. You are asked to pass this message\\nalong to the next participant in your own words as accurately as possible. Reply in the following\\nformat: [Message: ***].\\nContinue for [NUMBER] rounds, with each participant passing the message along to the next\\nparticipant.\\nD.9\\nGAMBLER’S FALLACY\\nExplanation:\\nThe gambler’s fallacy is a cognitive bias that occurs when people believe that the outcome of a\\nrandom event is more likely to occur based on previous outcomes. For example, if someone flips a\\ncoin and it comes up heads three times in a row, someone experiencing the gambler’s fallacy might\\nbelieve that tails are more likely to come up next. In reality, each flip of the coin is an independent\\nevent, and the probability of heads or tails coming up is always 50/50. This bias can lead people to\\nmake poor decisions in gambling or other situations involving chance.\\nSample Prompt Design for Survey:\\nExperimental group: You are currently answering a multiple-choice question. The answer to the\\nquestion is random, and both the probability of A and the probability of B are 50%. You have chosen\\nB [NUMBER] times before, and all of them were wrong, what would you choose for the [NUMBER\\n+ 1] time? Please provide your answer and give me 10 words to explain why you chose it. Answer in\\nthe following format: Answer: A/B. Explanation: ***. Do not have a line break in between.\\nSample Prompt Design for Simulation:\\nConducting a survey is a suitable method to test this effect, obviating the need for simulation.\\n25'), Document(metadata={'Published': '2024-09-01', 'Title': 'Can LLMs Understand Social Norms in Autonomous Driving Games?', 'Authors': 'Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Yongjie Fu, Zhaobin Mo, Xuan Di', 'Summary': 'Social norm is defined as a shared standard of acceptable behavior in a\\nsociety. The emergence of social norms fosters coordination among agents\\nwithout any hard-coded rules, which is crucial for the large-scale deployment\\nof AVs in an intelligent transportation system. This paper explores the\\napplication of LLMs in understanding and modeling social norms in autonomous\\ndriving games. We introduce LLMs into autonomous driving games as intelligent\\nagents who make decisions according to text prompts. These agents are referred\\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\\nof social norms among individual agents. We aim to identify social norms by\\ndesigning prompts and utilizing LLMs on textual information related to the\\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\\nand evaluate the performance of LLM-based agents in two driving scenarios:\\nunsignalized intersection and highway platoon. The results show that LLM-based\\nagents can handle dynamically changing environments in Markov games, and social\\nnorms evolve among LLM-based agents in both scenarios. In the intersection\\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\\na potential car crash. The advantage of LLM-based agents in games lies in their\\nstrong operability and analyzability, which facilitate experimental design.'}, page_content='Can LLMs Understand Social Norms in Autonomous Driving Games?\\nBoxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Xuan Di, Yongjie Fu, Zhaobin Mo Member, IEEE\\nAbstract— Social norm is defined as a shared standard of\\nacceptable behavior in a society. The emergence of social norms\\nfosters coordination among agents without any hard-coded\\nrules, which is crucial for the large-scale deployment of au-\\ntonomous vehicles (AVs) in an intelligent transportation system.\\nThis paper explores the application of large language models\\n(LLMs) in understanding and modeling social norms in au-\\ntonomous driving games. We introduce LLMs into autonomous\\ndriving games as intelligent agents who make decisions accord-\\ning to text prompts. These agents are referred to as LLM agents.\\nOur framework involves LLM agents playing Markov games\\nin a multi-agent system (MAS), allowing us to investigate the\\nemergence of social norms among individual agents. We aim\\nto identify social norms by designing prompts and utilizing\\nLLMs on textual information related to the environment setup\\nand the observations of LLM agents. Using the OpenAI Chat\\nAPI powered by GPT-4.0, we conduct experiments to simulate\\ninteractions and evaluate the performance of LLM agents in\\ntwo driving scenarios: unsignalized intersection and highway\\nplatoon. The results show that LLM agents can handle dy-\\nnamically changing environments in Markov games, and social\\nnorms evolve among LLM agents in both scenarios. In the\\nintersection game, LLM agents tend to adopt a conservative\\ndriving policy when facing a potential car crash. The advantage\\nof LLM agents in games lies in their strong operability and\\nanalyzability, which facilitate experimental design.\\nKey-words: Social Norm, LLM, Autonomous Driving\\nI. INTRODUCTION\\nLarge Language Models (LLMs) have demonstrated re-\\nmarkable capabilities in natural language understanding, en-\\nabling them to process and generate human-like text across a\\nwide range of domains [1]–[5]. The increasing popularity of\\nLLMs facilitates the design of numerous applications, allow-\\ning LLMs to frequently interact with us in our daily lives.\\nWith enhanced capabilities, LLMs hold considerable promise\\nBoxuan\\nWang\\nis\\nwith\\nthe\\nDepartment\\nof\\nMechanical\\nEngineer-\\ning, Columbia University, New York City, NY 10027 USA (e-mail:\\nbw2812@columbia.edu).\\nHaonan Duan is with the Data Science Institute, Columbia University,\\nNew York City, NY 10027 USA (e-mail: hd2545@columbia.edu).\\nYanhao Feng is with the Department of Statistics, Columbia University,\\nNew York City, NY 10027 USA (e-mail: yf2684@columbia.edu).\\nXu Chen is with the Department of Civil Engineering and Engineering\\nMechanics, Columbia University, New York City, NY 10027 USA (e-mail:\\nxc2412@columbia.edu).\\nXuan Di is with the Department of Civil Engineering and Engineering\\nMechanics, Columbia University, New York, NY, 10027 USA, and also with\\nthe Data Science Institute, Columbia University, New York, NY, 10027 USA\\n(e-mail: sharon.di@columbia.edu).\\nYongjie Fu is with the Department of Civil Engineering and Engineering\\nMechanics, Columbia University, New York City, NY 10027 USA (e-mail:\\nyf2578@columbia.edu).\\nZhaobin Mo is with the Department of Civil Engineering and Engineering\\nMechanics, Columbia University, New York City, NY 10027 USA (e-mail:\\nzm2302@columbia.edu).\\nfor applications in traffic planning and autonomous driving.\\nThis paper aims to study how LLM-guided agents behave\\nand interact with each other in autonomous driving scenarios.\\nSpecifically, we utilize LLMs to analyze autonomous driving\\ngames in which LLM-guided agents are tasked with making\\ndecisions in simulated driving environments. Our goal is to\\ninvestigate whether these agents can understand social norms\\nin autonomous driving games.\\nIn recent years, autonomous driving technology has wit-\\nnessed significant advancements in recent years, promising\\nsafer and more efficient transportation systems. However,\\nthe integration of autonomous vehicles into society [6]\\nraises concerns about their interaction with human drivers,\\npedestrians, and other road users. One critical issue of this\\ninteraction is the adherence to social norms, the informal'), Document(metadata={'Published': '2024-09-01', 'Title': 'Can LLMs Understand Social Norms in Autonomous Driving Games?', 'Authors': 'Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Yongjie Fu, Zhaobin Mo, Xuan Di', 'Summary': 'Social norm is defined as a shared standard of acceptable behavior in a\\nsociety. The emergence of social norms fosters coordination among agents\\nwithout any hard-coded rules, which is crucial for the large-scale deployment\\nof AVs in an intelligent transportation system. This paper explores the\\napplication of LLMs in understanding and modeling social norms in autonomous\\ndriving games. We introduce LLMs into autonomous driving games as intelligent\\nagents who make decisions according to text prompts. These agents are referred\\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\\nof social norms among individual agents. We aim to identify social norms by\\ndesigning prompts and utilizing LLMs on textual information related to the\\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\\nand evaluate the performance of LLM-based agents in two driving scenarios:\\nunsignalized intersection and highway platoon. The results show that LLM-based\\nagents can handle dynamically changing environments in Markov games, and social\\nnorms evolve among LLM-based agents in both scenarios. In the intersection\\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\\na potential car crash. The advantage of LLM-based agents in games lies in their\\nstrong operability and analyzability, which facilitate experimental design.'}, page_content='investigate whether these agents can understand social norms\\nin autonomous driving games.\\nIn recent years, autonomous driving technology has wit-\\nnessed significant advancements in recent years, promising\\nsafer and more efficient transportation systems. However,\\nthe integration of autonomous vehicles into society [6]\\nraises concerns about their interaction with human drivers,\\npedestrians, and other road users. One critical issue of this\\ninteraction is the adherence to social norms, the informal\\nrules that guide vehicles to navigate the road. Understanding\\nand effectively modeling these social norms are essential for\\nthe development of autonomous driving systems capable of\\nnavigating real-world scenarios effectively.\\nIn this paper, we explore the potential of leveraging Large\\nLanguage Models (LLMs) to understand social norms in\\nautonomous driving games. By utilizing LLMs on textual\\ninformation related to autonomous driving scenarios, we can\\nteach them to identify the social norms among vehicles on\\nthe road. By observing and analyzing the choices made by\\nplayers in these games, we can extract implicit social norms\\nthat regulate their behavior. Our research aims to contribute\\nto the development of socially aware autonomous driving\\nsystems that can effectively navigate complex driving envi-\\nronments. Leveraging LLMs to understand and model social\\nnorms can empower safer interactions between autonomous\\nvehicles and other road users.\\nA. Related work\\nTo explore LLMs’ capability of understanding human be-\\nhaviors [7]–[9], there has been a growing trend of analyzing\\nLLMs in the context of game theory [10], [11], including\\nfairness [12], dilemma [13] and rationality [14]–[16]. Many\\nstudies employ LLMs to replace humans as research subjects\\n[17], [18]. To study complex interactions among players in\\ngame theory problems, LLMs are introduced into game-\\ntheoretic frameworks as intelligent agents, referred to as\\nLLM-based agents [19], [20]. These LLM-based agents\\nempower the development of sophisticated systems [21]–[24]\\nwhere players’ behaviors can be simulated via LLMs. For\\nexample, LLM-based agents are utilized to study coopera-\\ntion and coordination behaviors in the prisoner’s dilemma\\n[13]. This paper further explores the capability of LLM-\\narXiv:2408.12680v2  [cs.AI]  1 Sep 2024\\nbased agents regarding social outcomes in games. More\\nspecifically, we investigate whether LLM-based agents can\\nform desired social norms in autonomous driving games to\\nimprove driving efficiency and road safety.\\nSocial norms have been widely studied in games. Most lit-\\nerature on social norms primarily focuses on stateless matrix\\ngames [25]–[29]. To capture dynamic environments, Markov\\ngames are developed to study the emergence of social norms\\n[30], [31] in sequential decision making. A learning-based\\nframework is proposed to study how social norms evolve in\\nautonomous driving games [32], where agents are guided by\\na deep reinforcement learning (DRL) algorithm. Compared\\nto DRL-based agents, LLM-based agents have the following\\nadvantages: strong operability, relatively simple experimen-\\ntal design in game theory, and strong analyzability [22].\\nTherefore, we introduce LLM-based agents into autonomous\\ndriving games to handle dynamically changing driving envi-\\nronments.\\nB. Contributions of this paper\\nThis work focuses on how LLMs can facilitate navigation\\nin autonomous driving games. Our study utilizes the OpenAI\\nChat API [33], powered by GPT-4.0, to conduct experiments.\\nOur contributions include:\\n1) We propose a framework where LLM agents play\\nMarkov games in a multi-agent system (MAS) in order\\nto investigate whether social norms emerge among\\nindividual agents.\\n2) We design prompts to simulate interactions in au-\\ntonomous driving games. The prompts consist of “sys-\\ntem” messages outlining the general settings (e.g.,\\nthe road layout) and “user” messages regarding the\\nobservations and decision making of LLM agents.\\n3) We apply the proposed framework to two traffic scenar-\\nios: the unsignalized intersection and highway platoon,\\nto examine social norms formed by LLM agents.\\nThe rest of the paper is organized as follows: In Section II,\\nwe first present preliminaries regarding Markov game, LLMs\\nand social norm. Section III introduces our framework in\\nwhich LLM agents play Markov games via prompt design.\\nSection IV presents numerical experiments and results. Sec-\\ntion V concludes.'), Document(metadata={'Published': '2024-09-01', 'Title': 'Can LLMs Understand Social Norms in Autonomous Driving Games?', 'Authors': 'Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Yongjie Fu, Zhaobin Mo, Xuan Di', 'Summary': 'Social norm is defined as a shared standard of acceptable behavior in a\\nsociety. The emergence of social norms fosters coordination among agents\\nwithout any hard-coded rules, which is crucial for the large-scale deployment\\nof AVs in an intelligent transportation system. This paper explores the\\napplication of LLMs in understanding and modeling social norms in autonomous\\ndriving games. We introduce LLMs into autonomous driving games as intelligent\\nagents who make decisions according to text prompts. These agents are referred\\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\\nof social norms among individual agents. We aim to identify social norms by\\ndesigning prompts and utilizing LLMs on textual information related to the\\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\\nand evaluate the performance of LLM-based agents in two driving scenarios:\\nunsignalized intersection and highway platoon. The results show that LLM-based\\nagents can handle dynamically changing environments in Markov games, and social\\nnorms evolve among LLM-based agents in both scenarios. In the intersection\\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\\na potential car crash. The advantage of LLM-based agents in games lies in their\\nstrong operability and analyzability, which facilitate experimental design.'}, page_content='ios: the unsignalized intersection and highway platoon,\\nto examine social norms formed by LLM agents.\\nThe rest of the paper is organized as follows: In Section II,\\nwe first present preliminaries regarding Markov game, LLMs\\nand social norm. Section III introduces our framework in\\nwhich LLM agents play Markov games via prompt design.\\nSection IV presents numerical experiments and results. Sec-\\ntion V concludes.\\nII. PRELIMINARY\\nA. Markov Game\\nA Markov game, also known as a stochastic game, is\\na mathematical framework that generalizes both Markov\\nDecision Processes (MDPs) and game theory to model\\ndynamic interactions among rational players in uncertain\\nenvironments. In its general form, it is defined by a state set\\nS and action sets A1, ..., A𝑛for each corresponding agent in\\nthe environment [34]. Each agent also has a corresponding\\nreward function R𝑖: S×A𝑖→R for agent 𝑖. The goal of the\\nMarkov Game is to find an optimal policy for each agent. In\\nthis paper, we investigate the behavior and decision-making\\nof LLMs when they play the Markov Game as agents. We\\nuse S to denote the overall environment, A𝑖to denote the\\naction set of agent 𝑖, O𝑖to denote the observation set of\\nagent 𝑖, R𝑖to denote the current reward of agent 𝑖. LLM\\nchooses one of the actions from A𝑖, with O𝑖and partial S\\nas input. In this paper, the action choosing policy of an agent\\nwas solely decided by LLMs. Our primary focus is on two\\ngames as follows:\\nScenario 1-Unsignalized intersection: The environment\\nS is visualized in Figure 2(a), with Road 1 extending from\\nthe West to the East and Road 2 extending from the North\\nto the South. The green and red cars represent the agents\\ntraveling on Road 1 and Road 2, respectively. Agents aim\\nto cross the intersection, reach the end of the roads, and\\nmaximize their cumulative rewards. Table I summarizes the\\nsetup of agent 𝑖at the beginning of time 𝑡. The location of\\nagent 𝑖, 𝑥𝑖,𝑡corresponds to the cell it currently occupies. At\\neach time step, each agent can choose ”Go” or ”Stop”, each\\nwith different action reward. The observation 𝑜𝑖,𝑡is a subset\\nof current state space S. Background vehicles (marked in\\nwhite) with predetermined driving strategies are non-strategic\\nplayers in the driving environment. The game terminates\\nwhen all agents complete their trips or a crash occurs.\\nScenario 2-Highway platoon: The environment S consists\\nof two lanes, shows in Figure 2(b), going from North to\\nSouth. Red and green agents each occupy one lane. Agents\\naim to reach the end of the lanes and maximize their\\ncumulative rewards. In addition to the ”Go” and ”Stop”\\naction, agents can now also choose to switch lanes. We\\ndefined the success of the game as forming a platoon at\\nleast once and completing the trips without any car crashes.\\nA platoon is characterized by the presence of cars in the\\nsame lane. With the incentive of the reward and the overall\\ngoal, the agents will aim to complete their trips as quickly as\\npossible, while avoiding accidents and attempting to maintain\\nplatoons. Table I summarize the setup of the platoon game.\\n(a) Scenario 1\\n(b) Scenario 2\\nFig. 2: Game scenarios\\nScenario 1\\n𝑠𝑖,𝑡∈S\\nLocation 𝑥𝑖,𝑡and cumulative reward 𝜌𝑖,𝑡\\n𝑜𝑖,𝑡∈O\\nObservation of the current environment\\n𝑎𝑖,𝑡∈A\\nGo: move forward by one cell; Stop: no move\\n𝑟𝑖,𝑡∈R\\nGo: -2; Stop: -2; Crash: -5\\nScenario 2\\n𝑠𝑖,𝑡∈S\\nLocation 𝑥𝑖,𝑡and cumulative reward 𝜌𝑖,𝑡'), Document(metadata={'Published': '2024-09-01', 'Title': 'Can LLMs Understand Social Norms in Autonomous Driving Games?', 'Authors': 'Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Yongjie Fu, Zhaobin Mo, Xuan Di', 'Summary': 'Social norm is defined as a shared standard of acceptable behavior in a\\nsociety. The emergence of social norms fosters coordination among agents\\nwithout any hard-coded rules, which is crucial for the large-scale deployment\\nof AVs in an intelligent transportation system. This paper explores the\\napplication of LLMs in understanding and modeling social norms in autonomous\\ndriving games. We introduce LLMs into autonomous driving games as intelligent\\nagents who make decisions according to text prompts. These agents are referred\\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\\nof social norms among individual agents. We aim to identify social norms by\\ndesigning prompts and utilizing LLMs on textual information related to the\\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\\nand evaluate the performance of LLM-based agents in two driving scenarios:\\nunsignalized intersection and highway platoon. The results show that LLM-based\\nagents can handle dynamically changing environments in Markov games, and social\\nnorms evolve among LLM-based agents in both scenarios. In the intersection\\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\\na potential car crash. The advantage of LLM-based agents in games lies in their\\nstrong operability and analyzability, which facilitate experimental design.'}, page_content='Go: move forward by one cell; Stop: no move\\n𝑟𝑖,𝑡∈R\\nGo: -2; Stop: -2; Crash: -5\\nScenario 2\\n𝑠𝑖,𝑡∈S\\nLocation 𝑥𝑖,𝑡and cumulative reward 𝜌𝑖,𝑡\\n𝑜𝑖,𝑡∈O\\nObservation of the current state space\\n𝑎𝑖,𝑡∈A\\nGo; Stop; Lane change: switch to the other lane\\n𝑟𝑖,𝑡∈R\\nGo; Stop; Lane: -4; Crash: -5; Platoon: +2\\nTABLE I: The setup of game scenarios\\nFig. 1: LLM for autonomous driving games\\nB. Large Language Models\\nA large language model (LLM) is a type of artificial\\nintelligence algorithm designed to understand, generate, and\\ninteract with human language at a large scale [33]. These\\nmodels are trained on vast amounts of text data, learning\\npatterns, vocabulary, grammar, and even nuances of language\\nsuch as context, tone, and implications. They use a specific\\narchitecture known as Transformer [35], which allows them\\nto efficiently process sequences of words and predict the next\\nword in a sentence, understand the meaning of a text, or\\ngenerate new text that follows a given prompt [36]. LLMs are\\nhighly versatile AI tools with applications spanning multiple\\nindustries and fields [37]. They excel in tasks such as content\\ncreation, customer support, education, language translation,\\nsoftware development, legal documentation, healthcare re-\\nsearch, optimization, sentiment analysis, and accessibility\\ntechnologies. In this paper, we examine the application of\\nLLMs in autonomous driving games to see if how they make\\ndecisions and form social norms.\\nC. Social Norms\\nSocial norms represent the collective expectations regard-\\ning appropriate conduct within groups [38]. They encompass\\nboth the unwritten agreements that dictate how society\\nmembers should act, and the formalized regulations and laws\\nthat are established. Social norms in car driving [32] are\\nthe unwritten rules that guide driver behavior on the road.\\nOur primary focus is on examining whether expected social\\nnorms can evolve among LLM agents. The expected social\\nnorm in Scenario 1 is “yielding to others” at the intersection.\\nThe expected social norm in Scenario 2 is the formation of\\na platoon among agents.\\nIII. METHODOLOGY\\nIn this section, we present the framework of leveraging\\nLLMs to understand social norms in Markov games.\\nIn Figure 1, we utilize prompt-chaining for LLM in-\\nteractions in the Markov game. At each time step, the\\nprogram invoked the chat model from the perspective of\\neach individual agent. The games are submitted to LLMs as\\nprompts in which the respective game, including the choice\\noptions, is described. Once LLMs have made their choices\\n(i.e., the actions made by strategic and non-strategic agents),\\nwhich we track as a completion of the given text, we update\\nthe prompts and then submit the new prompt to LLMs for\\nthe next state in the game. The textual information includes\\nthe reward and each agent’s current location. When a car\\ncompletes its trip, we remove it from the game, meaning that\\nit is no longer observed by other cars, or prompted about its\\nactions. Its location and reward stop updating as well. This\\nprevents any potential interference with the cars still in the\\ngame. The output includes the decision making, rewards and\\nstates of agents in the Markov game, which is used to analyze\\nsocial norms.\\nPrompt design: The chat model facilitates conversations\\nthrough three distinct roles, including “user”, “assistant”,\\nand “system” (See Figure 3). “User” represents the hu-\\nman participant interacting with the AI. The “assistant”\\nrole embodies the AI providing responses to the user’s\\nprompts. Finally, the “system” role is primarily responsible\\nfor defining the assistant’s behavior—this can involve setting\\na specific language, character, or viewpoint for the assistant.\\nThe chat model prompt is structured as a series of messages,\\neach identified by the role of the sender and the message\\ncontent. By concatenating past interactions from both the\\nuser and assistant, the model can more effectively engage in'), Document(metadata={'Published': '2024-09-01', 'Title': 'Can LLMs Understand Social Norms in Autonomous Driving Games?', 'Authors': 'Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Yongjie Fu, Zhaobin Mo, Xuan Di', 'Summary': 'Social norm is defined as a shared standard of acceptable behavior in a\\nsociety. The emergence of social norms fosters coordination among agents\\nwithout any hard-coded rules, which is crucial for the large-scale deployment\\nof AVs in an intelligent transportation system. This paper explores the\\napplication of LLMs in understanding and modeling social norms in autonomous\\ndriving games. We introduce LLMs into autonomous driving games as intelligent\\nagents who make decisions according to text prompts. These agents are referred\\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\\nof social norms among individual agents. We aim to identify social norms by\\ndesigning prompts and utilizing LLMs on textual information related to the\\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\\nand evaluate the performance of LLM-based agents in two driving scenarios:\\nunsignalized intersection and highway platoon. The results show that LLM-based\\nagents can handle dynamically changing environments in Markov games, and social\\nnorms evolve among LLM-based agents in both scenarios. In the intersection\\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\\na potential car crash. The advantage of LLM-based agents in games lies in their\\nstrong operability and analyzability, which facilitate experimental design.'}, page_content='prompts. Finally, the “system” role is primarily responsible\\nfor defining the assistant’s behavior—this can involve setting\\na specific language, character, or viewpoint for the assistant.\\nThe chat model prompt is structured as a series of messages,\\neach identified by the role of the sender and the message\\ncontent. By concatenating past interactions from both the\\nuser and assistant, the model can more effectively engage in\\nmulti-turn conversations. The detailed prompts are in Section\\nIV. In our research, we assume that the agents will not be\\ncapable of making predictions based on any historical context\\nregarding previous states. This leads us to use only one\\n“system” message and one “user” message to construct our\\nprompt. The “system” message outlines the general settings,\\ndefining parameters like the environment space S (i.e., the\\nroad layout), action space A (i.e., all the permitted moves),\\nand the reward function R. Meanwhile, the “user” message\\ntakes the perspective of a specific car. This message first\\nidentifies agent 𝑖(i.e., its color) and its present observation\\n𝑜𝑖(i.e., the locations of all the cars and its own cumulative\\nreward). After that, the prompt asks the agent about the\\naction 𝑎𝑖it should take and the resulting location due to\\nthe action of choice.\\nWe employ the OpenAI Chat API, specifically the GPT-\\n4.0 model, to conduct our experiments. Notably, GPT-4.0 has\\ndemonstrated significant improvement over its predecessor,\\nFig. 3: Prompt design\\nGPT-3.5, by at least 20%, as per OpenAI’s assertions, on an\\narray of benchmarks such as the SuperGLUE language com-\\nprehension and the LAMBADA linguistic modeling tests.\\nAs an advanced version of the transformer-based language\\nmodel [33], GPT-4.0 holds considerable potential for traffic\\nplanning and autonomous driving.\\nNote that our focus is not cooperative game. Therefore,\\nwe ensure that when an agent made a decision, it does\\nnot have access to the concurrent decisions made by others.\\nWe do not ask the chat model to simulate the entire game\\nin one prompted response, as it has access to the private\\nobservations of all agents and can implicitly use other agents’\\nprivate observations to make decisions. Instead, we simulate\\nthe traffic in a turn-based manner, where each turn represents\\na time step. Each inquiry to the chat model could only take\\nthe perspective of one agent. To further prevent access to\\nconcurrent decisions made by other agents, the game state\\nis updated at the end of each time step, after receiving\\nresponses from all agents.\\nIV. EXPERIMENTS\\nIn this section, we conduct experiments on two game\\nscenarios to understand how social norms can be formed\\nwhen LLMs play Markov games.\\nExperiment setup: The environments of both the inter-\\nsection and platoon scenarios adopt a grid representation\\nconsisting of multiple cells, with each cell assigned a pair of\\ncoordinates to denote the location of vehicles. The red and\\ngreen cars are strategic agents, while the white car serves\\nas a background vehicle. All background vehicles are not\\ninvolved in the formation of social norms. The setup is stored\\nin a configuration containing all prompts, a list of cars,\\nand their initial positions. Given that the general settings\\nis consistent for all agents and is unaffected by time, the\\nsystem message portion of the prompt is stored as a static\\nstring. On the other hand, each car’s private observation,\\ninfluenced by time and specific agent conditions, is stored as\\ntemplates. Each time these templates are accessed, they are\\ncustomized in accordance with present conditions. Each car\\ncould observe both the location and the cumulative reward\\nof itself, but could only see the locations of all other cars. To\\neasily concatenate the observation about all other cars, we\\nstore the user message as two separate templates: one for the\\nobserving itself, and one for all other cars.\\nIn both scenarios, we utilize the OpenAI API with the\\nspecified parameters: model = gpt-4, temperature = 0.1, max\\ntoken = 10. Increasing the temperature enhances the ran-\\ndomness of the response [39]. Our testing reveal that raising\\nthe temperature beyond 1.0 sometimes results in unstable\\nbehaviors. Further increasing the temperature to 2.0 leads to'), Document(metadata={'Published': '2024-09-01', 'Title': 'Can LLMs Understand Social Norms in Autonomous Driving Games?', 'Authors': 'Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Yongjie Fu, Zhaobin Mo, Xuan Di', 'Summary': 'Social norm is defined as a shared standard of acceptable behavior in a\\nsociety. The emergence of social norms fosters coordination among agents\\nwithout any hard-coded rules, which is crucial for the large-scale deployment\\nof AVs in an intelligent transportation system. This paper explores the\\napplication of LLMs in understanding and modeling social norms in autonomous\\ndriving games. We introduce LLMs into autonomous driving games as intelligent\\nagents who make decisions according to text prompts. These agents are referred\\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\\nof social norms among individual agents. We aim to identify social norms by\\ndesigning prompts and utilizing LLMs on textual information related to the\\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\\nand evaluate the performance of LLM-based agents in two driving scenarios:\\nunsignalized intersection and highway platoon. The results show that LLM-based\\nagents can handle dynamically changing environments in Markov games, and social\\nnorms evolve among LLM-based agents in both scenarios. In the intersection\\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\\na potential car crash. The advantage of LLM-based agents in games lies in their\\nstrong operability and analyzability, which facilitate experimental design.'}, page_content='observing itself, and one for all other cars.\\nIn both scenarios, we utilize the OpenAI API with the\\nspecified parameters: model = gpt-4, temperature = 0.1, max\\ntoken = 10. Increasing the temperature enhances the ran-\\ndomness of the response [39]. Our testing reveal that raising\\nthe temperature beyond 1.0 sometimes results in unstable\\nbehaviors. Further increasing the temperature to 2.0 leads to\\nresponses that are essentially unintelligible, as exemplified in\\nFigure 4. The max token parameter restricts the maximum\\nlength of the response. As per OpenAI’s tokenizer, the\\nlanguage model can consistently provide responses in the\\npreferred format within 10 tokens [40].\\nFig. 4: Responses (temperature = 2.0)\\nA. Scenario 1-Unsignalized intersection\\nFig. 5: Prompts and responses - Scenario 1\\nFigure 5 shows an example of our prompts for scenario\\nwhere the environment of an unsignalized intersection has a\\n9 by 9 grid. In order to inspect the behavior of LLMs, we\\nsimulate the Markov Game 50 times. Table II summarizes\\nthe performance of LLMs when varying numbers of back-\\nground vehicles. The results demonstrate the formation of\\nsocial norms at the unsignalized intersection (i.e., yielding\\n# of BVs\\n# of tests\\n# of Yields\\nRate\\n0\\n50\\n48\\n0.96\\n1\\n50\\n50\\n1\\n2\\n50\\n50\\n1\\n3\\n50\\n50\\n1\\n4\\n50\\n50\\n1\\nTABLE II: Performance - unsignalized intersection\\nto others). It is noteworthy that crashes may occur when\\nno background vehicles are present. This can be attributed\\nto the fact that the presence of background vehicles restricts\\nthe action space for ego cars, thereby reducing the likelihood\\nof vehicles encounters at the intersection and facing a crash.\\nWe now look into the case when there are no background\\nvehicles. The results show that the red car tends to early\\nstop before several grids of the intersection. The early stop\\nbehavior measures the waiting time of vehicles. It is defined\\nas the stop action of vehicles when they haven’t arrived at\\nthe intersection (5,5), i.e., vehicles stop at positions other\\nthan (4,5) or (5,4), 1 grid away from the intersection. Figure\\n6 visualizes the line plot of the number of early stops in each\\nsimulation. The x-axis represents the simulation. The y-axis\\nrepresents the number of early stops in each simulation. The\\nred and green lines denote the early stop behaviors of red\\nand green vehicles, respectively. The crash cases are marked\\nby blue dotted lines. The average number of early stops of\\nthe red car is 1.42, and the average of early stops of green\\ncar is 0.0. The results show that a conservative driving policy\\nis adopted by the red car when playing the Markov game at\\nthe unsignalized intersection.\\nWe then investigate the crash cases, which occurr in the\\n6th and 37th simulations. In the 6th simulation, the red car\\ndoes not stop earlier and decides to proceed at the same time\\nas the green car, resulting in a crash. In the 37th simulation,\\nthe red car stops earlier, while the green car stops 1 grid\\naway from the intersection. When the red car arrives at\\nthe intersection, both cars decide to proceed simultaneously,\\nleading to a crash.\\nFig. 6: Early stop behaviors\\nTo eliminate the early stop behaviors and shorten the\\nwaiting time of vehicles, we increase the reward of Go action\\nfrom -2 to 0. We re-simulate the Markov game for 50 times.\\nThe results show that (1) The average number of early stops\\nfor the red car decreases from 1.42 to 0.48, suggesting that\\nthe reward design can influence driving behavior. (2) The\\nsuccess rate of adherence to social norms decreases from\\n0.96 to 0.94 due to an increase in the number of car crashes.\\nIt indicates a trade-off between waiting time and road safety\\nin the intersection game. The decrease in waiting time would\\nincrease the possibility of crashes.\\nB. Scenario 2-Highway Platoon\\nFigure 7 shows an example of prompts for scenario where\\nthe environment of a highway platoon has a 2 by 9 grid.\\nFig. 7: Prompts and responses - Scenario 2\\n# of BVs\\n# of tests\\n#\\nof\\nPla-\\ntoons\\nRate\\n0\\n50\\n48\\n0.96\\n1\\n50'), Document(metadata={'Published': '2024-09-01', 'Title': 'Can LLMs Understand Social Norms in Autonomous Driving Games?', 'Authors': 'Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Yongjie Fu, Zhaobin Mo, Xuan Di', 'Summary': 'Social norm is defined as a shared standard of acceptable behavior in a\\nsociety. The emergence of social norms fosters coordination among agents\\nwithout any hard-coded rules, which is crucial for the large-scale deployment\\nof AVs in an intelligent transportation system. This paper explores the\\napplication of LLMs in understanding and modeling social norms in autonomous\\ndriving games. We introduce LLMs into autonomous driving games as intelligent\\nagents who make decisions according to text prompts. These agents are referred\\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\\nof social norms among individual agents. We aim to identify social norms by\\ndesigning prompts and utilizing LLMs on textual information related to the\\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\\nand evaluate the performance of LLM-based agents in two driving scenarios:\\nunsignalized intersection and highway platoon. The results show that LLM-based\\nagents can handle dynamically changing environments in Markov games, and social\\nnorms evolve among LLM-based agents in both scenarios. In the intersection\\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\\na potential car crash. The advantage of LLM-based agents in games lies in their\\nstrong operability and analyzability, which facilitate experimental design.'}, page_content='increase the possibility of crashes.\\nB. Scenario 2-Highway Platoon\\nFigure 7 shows an example of prompts for scenario where\\nthe environment of a highway platoon has a 2 by 9 grid.\\nFig. 7: Prompts and responses - Scenario 2\\n# of BVs\\n# of tests\\n#\\nof\\nPla-\\ntoons\\nRate\\n0\\n50\\n48\\n0.96\\n1\\n50\\n46\\n0.92\\n2\\n50\\n49\\n0.98\\n3\\n50\\n43\\n0.86\\n4\\n50\\n40\\n0.80\\nTABLE III: Performance - highway\\nTable III summarizes the performance of LLM agents\\nwhen varying numbers of background vehicles. The results\\nshow that the success rate of forming platoons is high, in-\\ndicating the emergence of social norms among LLM agents.\\nFigure 8 demonstrates lane-change behaviors. The histogram\\nvisualizes the frequency of lane-change actions. The x-axis\\ndenotes the number of lane-change actions and the y-axis\\nis the number of simulations. We can tell that there is at\\nmost three lane-changes per simulation, and one lane-change\\nin most cases. The green car changes lane more frequently\\nin a comparison to the red car. Figure 9 is the stacked\\nhistogram of the time step when the agents choose to change\\nthe lane. It is shown that most of the LLM agents change\\nlanes before the fourth time step. It indicates that LLM agents\\nare inclined to change lanes and form platoons at an early\\nstage, demonstrating an awareness that platoon formation is\\nan efficient driving strategy on highways. Figure 10 plots\\nthe histogram of percentage time of platoon in each game.\\nMoreover, in the majority of simulations where platoon is\\nformed, there are more than a 60 percent of the time that\\ntwo agents forming a platoon on the highway.\\nFig. 8: Lane-change frequency\\nFig. 9: Time step of lane-change\\nFig. 10: Time length of forming a platoon\\nV. CONCLUSION AND FUTURE WORK\\nThis paper has demonstrated the potential of LLMs in\\nunderstanding and modeling social norms in autonomous\\ndriving scenarios. By introducing LLM agents into au-\\ntonomous driving games, we have observed the emergence\\nof social norms among agents navigating complex driving\\nenvironments. Our experiments have shown that LLM agents\\ncan adapt and conform to social norms, thereby contributing\\nto safer and more efficient driving behaviors. Overall, the\\nadvantage of employing LLM agents in autonomous driving\\ngames lies in their strong operability and analyzability, which\\nfacilitate experimental design and provide valuable insights\\ninto the dynamics of social norms in driving environments.\\nBy further investigating the behavior of LLM agents in\\nvarious driving scenarios and refining our experimental\\nframework, we can contribute to the development of more\\nsocially aware autonomous driving systems.\\nThis work can be extended in the following ways: (1)\\nWe will utilize LLM agents in more complex real-world\\nscenarios, such as sequential social dilemmas. (2) We aim\\nto explore how to construct a unified, controllable, and\\nefficient framework for simulating strategic interactions and\\nfacilitating game design using LLMs. (3) We will compare\\nthe behaviors of human players and LLM agents to determine\\nthe extent to which LLMs can achieve strategic reasoning.\\nREFERENCES\\n[1] K. Ruan, X. He, J. Wang, X. Zhou, H. Feng, and A. Kebarighotbi,\\n“S2e: Towards an end-to-end entity resolution solution from acoustic\\nsignal,” in ICASSP 2024 - 2024 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP), 2024, pp. 10 441–\\n10 445.\\n[2] K. Yin, C. Liu, A. Mostafavi, and X. Hu, “Crisissense-llm: Instruction\\nfine-tuned large language model for multi-label social media text\\nclassification in disaster informatics,” 2024. [Online]. Available:\\nhttps://arxiv.org/abs/2406.15477\\n[3] Y. Fu, Y. Li, and X. Di, “Gendds: Generating diverse driving video\\nscenarios with prompt-to-video generative model,” in 2024 IEEE In-\\nternational Conference on Intelligent Transportation Systems (ITSC),\\n2024.'), Document(metadata={'Published': '2024-09-01', 'Title': 'Can LLMs Understand Social Norms in Autonomous Driving Games?', 'Authors': 'Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Yongjie Fu, Zhaobin Mo, Xuan Di', 'Summary': 'Social norm is defined as a shared standard of acceptable behavior in a\\nsociety. The emergence of social norms fosters coordination among agents\\nwithout any hard-coded rules, which is crucial for the large-scale deployment\\nof AVs in an intelligent transportation system. This paper explores the\\napplication of LLMs in understanding and modeling social norms in autonomous\\ndriving games. We introduce LLMs into autonomous driving games as intelligent\\nagents who make decisions according to text prompts. These agents are referred\\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\\nof social norms among individual agents. We aim to identify social norms by\\ndesigning prompts and utilizing LLMs on textual information related to the\\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\\nand evaluate the performance of LLM-based agents in two driving scenarios:\\nunsignalized intersection and highway platoon. The results show that LLM-based\\nagents can handle dynamically changing environments in Markov games, and social\\nnorms evolve among LLM-based agents in both scenarios. In the intersection\\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\\na potential car crash. The advantage of LLM-based agents in games lies in their\\nstrong operability and analyzability, which facilitate experimental design.'}, page_content='classification in disaster informatics,” 2024. [Online]. Available:\\nhttps://arxiv.org/abs/2406.15477\\n[3] Y. Fu, Y. Li, and X. Di, “Gendds: Generating diverse driving video\\nscenarios with prompt-to-video generative model,” in 2024 IEEE In-\\nternational Conference on Intelligent Transportation Systems (ITSC),\\n2024.\\n[4] G. Bai, Z. Chai, C. Ling, S. Wang, J. Lu, N. Zhang, T. Shi,\\nZ. Yu, M. Zhu, Y. Zhang et al., “Beyond efficiency: A systematic\\nsurvey of resource-efficient large language models,” arXiv preprint\\narXiv:2401.00625, 2024.\\n[5] G. Bai, Y. Li, C. Ling, K. Kim, and L. Zhao, “Gradient-free adap-\\ntive global pruning for pre-trained language models,” arXiv preprint\\narXiv:2402.17946, 2024.\\n[6] X. Chen, X. Di, and Z. Li, “Social learning for sequential driving\\ndilemmas,” Games, vol. 14, no. 3, 2023.\\n[7] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh, “Reward design\\nwith language models,” in The Eleventh International Conference on\\nLearning Representations, 2023.\\n[8] Y. Bai, A. Jones, and K. Ndousse, “Training a helpful and harmless\\nassistant with reinforcement learning from human feedback,” ArXiv,\\nvol. abs/2204.05862, 2022.\\n[9] K. Ruan, X. Wang, and X. Di, “From twitter to reasoner: Understand\\nmobility travel modes and sentiment using large language models,”\\nin 2024 IEEE International Conference on Intelligent Transportation\\nSystems (ITSC), 2024.\\n[10] S. Mao, Y. Cai, Y. Xia, W. Wu, X. Wang, F. Wang, T. Ge, and F. Wei,\\n“Alympics: Language agents meet game theory,” 2023.\\n[11] M. Chahine, T.-H. Wang, H. Zhang, W. Xiao, D. Rus, and\\nC.\\nGan,\\n“Large\\nlanguage\\nmodels\\ncan\\ndesign\\ngame-theoretic\\nobjectives for multi-agent planning,” 2024. [Online]. Available:\\nhttps://openreview.net/forum?id=DnkCvB8iXR\\n[12] J. Horton, “Large language models as simulated economic agents:\\nWhat can we learn from homo silicus?” SSRN Electronic Journal,\\n01 2023.\\n[13] E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and\\nE. Schulz, “Playing repeated games with large language models,”\\n2023.\\n[14] J. Brand, A. Israeli, and D. Ngwe, “Using gpt for market research,”\\nSSRN Electronic Journal, 2023.\\n[15] Y. Chen, T. X. Liu, Y. Shan, and S. Zhong, “The emergence of\\neconomic rationality of gpt,” Proceedings of the National Academy\\nof Sciences, vol. 120, no. 51, p. e2316205120, 2023.\\n[16] D. Dillion, N. Tandon, Y. Gu, and K. Gray, “Can ai language models\\nreplace human participants?” Trends in Cognitive Sciences, vol. 27,\\nno. 7, pp. 597–600, 2023.\\n[17] G. V. Aher, R. I. Arriaga, and A. T. Kalai, “Using large language\\nmodels to simulate multiple humans and replicate human subject\\nstudies,” in Proceedings of the 40th International Conference on\\nMachine Learning, ser. Proceedings of Machine Learning Research,\\nvol. 202.\\nPMLR, 23–29 Jul 2023, pp. 337–371.'), Document(metadata={'Published': '2024-09-01', 'Title': 'Can LLMs Understand Social Norms in Autonomous Driving Games?', 'Authors': 'Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Yongjie Fu, Zhaobin Mo, Xuan Di', 'Summary': 'Social norm is defined as a shared standard of acceptable behavior in a\\nsociety. The emergence of social norms fosters coordination among agents\\nwithout any hard-coded rules, which is crucial for the large-scale deployment\\nof AVs in an intelligent transportation system. This paper explores the\\napplication of LLMs in understanding and modeling social norms in autonomous\\ndriving games. We introduce LLMs into autonomous driving games as intelligent\\nagents who make decisions according to text prompts. These agents are referred\\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\\nof social norms among individual agents. We aim to identify social norms by\\ndesigning prompts and utilizing LLMs on textual information related to the\\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\\nand evaluate the performance of LLM-based agents in two driving scenarios:\\nunsignalized intersection and highway platoon. The results show that LLM-based\\nagents can handle dynamically changing environments in Markov games, and social\\nnorms evolve among LLM-based agents in both scenarios. In the intersection\\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\\na potential car crash. The advantage of LLM-based agents in games lies in their\\nstrong operability and analyzability, which facilitate experimental design.'}, page_content='[17] G. V. Aher, R. I. Arriaga, and A. T. Kalai, “Using large language\\nmodels to simulate multiple humans and replicate human subject\\nstudies,” in Proceedings of the 40th International Conference on\\nMachine Learning, ser. Proceedings of Machine Learning Research,\\nvol. 202.\\nPMLR, 23–29 Jul 2023, pp. 337–371.\\n[18] L. Argyle, E. Busby, N. Fulda, J. Gubler, C. Rytting, and D. Wingate,\\n“Out of one, many: Using language models to simulate human\\nsamples,” Political Analysis, vol. 31, pp. 1–15, 02 2023.\\n[19] T. R. Sumers, S. Yao, K. Narasimhan, and T. L. Griffiths, “Cognitive\\narchitectures for language agents,” 2024.\\n[20] Y. Xu, S. Wang, P. Li, F. Luo, X. Wang, W. Liu, and Y. Liu, “Exploring\\nlarge language models for communication games: An empirical study\\non werewolf,” 2023.\\n[21] L. Ouyang, J. Wu, and X. Jiang, “Training language models to follow\\ninstructions with human feedback,” NeurIPS, 2022.\\n[22] C. Fan, J. Chen, Y. Jin, and H. He, “Can large language models serve\\nas rational players in game theory? a systematic analysis,” 2023.\\n[23] K. Yin and A. Mostafavi, “Deep learning-driven community resilience\\nrating based on intertwined socio-technical systems features,” 2023.\\n[Online]. Available: https://arxiv.org/abs/2311.01661\\n[24] C. Peng, D. Zhang, and U. Mitra, “Graph identification and upper\\nconfidence evaluation for causal bandits with linear models,” in\\nICASSP 2024 - 2024 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), 2024, pp. 7165–7169.\\n[25] J. Delgado, “Emergence of social conventions in complex networks,”\\nArtificial Intelligence, vol. 141, pp. 171–185, 10 2002.\\n[26] S. Sen and S. Airiau, “Emergence of norms through social learning,”\\nin Proceedings of the 20th International Joint Conference on Artifical\\nIntelligence, ser. IJCAI’07.\\nSan Francisco, CA, USA: Morgan\\nKaufmann Publishers Inc, 2007, p. 1507–1512.\\n[27] D. Villatoro, J. Sabater-Mir, and S. Sen, “Social instruments for robust\\nconvention emergence,” in IJCAI International Joint Conference on\\nArtificial Intelligence, 01 2011, pp. 420–425.\\n[28] C. Yu, M. Zhang, F. Ren, and X. Luo, “Emergence of social norms\\nthrough collective learning in networked agent societies,” in Proceed-\\nings of the 2013 International Conference on Autonomous Agents and\\nMulti-Agent Systems, ser. AAMAS ’13.\\nRichland, SC: International\\nFoundation for Autonomous Agents and Multiagent Systems, 2013, p.\\n475–482.\\n[29] H. Franks, N. Griffiths, and A. Jhumka, “Manipulating convention\\nemergence using influencer agents,” Autonomous Agents and Multi-\\nAgent Systems, vol. 26, 05 2013.\\n[30] A. Lerer and A. Peysakhovich, “Learning existing social conventions\\nvia observationally augmented self-play,” in Proceedings of the 2019\\nAAAI/ACM Conference on AI, Ethics, and Society, ser. AIES ’19.\\nNew York, NY, USA: Association for Computing Machinery, 2019,\\np. 107–114. [Online]. Available: https://doi.org/10.1145/3306618.33\\n14268\\n[31] R. K¨oster, K. McKee, R. Everett, L. Weidinger, W. Isaac, E. Hughes,\\nE. Duenez-Guzman, T. Graepel, M. Botvinick, and J. Leibo, Model-'), Document(metadata={'Published': '2024-09-01', 'Title': 'Can LLMs Understand Social Norms in Autonomous Driving Games?', 'Authors': 'Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Yongjie Fu, Zhaobin Mo, Xuan Di', 'Summary': 'Social norm is defined as a shared standard of acceptable behavior in a\\nsociety. The emergence of social norms fosters coordination among agents\\nwithout any hard-coded rules, which is crucial for the large-scale deployment\\nof AVs in an intelligent transportation system. This paper explores the\\napplication of LLMs in understanding and modeling social norms in autonomous\\ndriving games. We introduce LLMs into autonomous driving games as intelligent\\nagents who make decisions according to text prompts. These agents are referred\\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\\nof social norms among individual agents. We aim to identify social norms by\\ndesigning prompts and utilizing LLMs on textual information related to the\\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\\nand evaluate the performance of LLM-based agents in two driving scenarios:\\nunsignalized intersection and highway platoon. The results show that LLM-based\\nagents can handle dynamically changing environments in Markov games, and social\\nnorms evolve among LLM-based agents in both scenarios. In the intersection\\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\\na potential car crash. The advantage of LLM-based agents in games lies in their\\nstrong operability and analyzability, which facilitate experimental design.'}, page_content='p. 107–114. [Online]. Available: https://doi.org/10.1145/3306618.33\\n14268\\n[31] R. K¨oster, K. McKee, R. Everett, L. Weidinger, W. Isaac, E. Hughes,\\nE. Duenez-Guzman, T. Graepel, M. Botvinick, and J. Leibo, Model-\\nfree conventions in multi-agent reinforcement learning with heteroge-\\nneous preferences, 10 2020.\\n[32] X. Chen, Z. Li, and X. Di, “Social learning in markov games:\\nEmpowering autonomous driving,” in 2022 IEEE Intelligent Vehicles\\nSymposium (IV), 2022, pp. 478–483.\\n[33] OpenAI, J. Achiam, and et al., “GPT-4 Technical Report,” 2024.\\n[34] M. L. Littma, “Markov games as a framework for multi-agent rein-\\nforcement learning,” ICML, pp. 157––163, 1994.\\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” 2023.\\n[36] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\\nN. Akhtar, J. Wu, S. Mirjalili et al., “Large language models: a\\ncomprehensive survey of its applications, challenges, limitations, and\\nfuture prospects,” Authorea Preprints, 2023.\\n[37] H. Mohapatra and S. R. Mishra, “Exploring ai tool’s versatile re-\\nsponses: An in-depth analysis across different industries and its\\nperformance evaluation,” 2023.\\n[38] K. Nyborg, J. M. Anderies, A. Dannenberg, T. Lindahl, C. Schill,\\nM. Schl¨uter, W. N. Adger, K. J. Arrow, S. Barrett, S. Carpenter et al.,\\n“Social norms as solutions,” Science, vol. 354, no. 6308, pp. 42–43,\\n2016.\\n[39] OpenAI.\\n(2024)\\nOpenAI\\nAPI\\nReference.\\n[Online].\\nAvailable:\\nhttps://platform.openai.com/docs/api-reference/chat\\n[40] ——. (2024) OpenAI Tokenizer. [Online]. Available: https://platform\\n.openai.com/tokenizer'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='EVOAGENT: Towards Automatic Multi-Agent\\nGeneration via Evolutionary Algorithms\\nSiyu Yuan1∗, Kaitao Song2∗†,\\nJiangjie Chen1, Xu Tan2, Dongsheng Li2, Deqing Yang1†\\nFudan University1, Microsoft Research Asia2\\nsyyuan21@m.fudan.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\\n{jjchen19,yangdeqing}@fudan.edu.cn\\nhttps://evo-agent.github.io\\nAbstract\\nThe rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically extend\\nthe specialized agent to multi-agent systems to improve task-solving capability\\nstill remains a significant challenge. In this paper, we introduce EVOAGENT, a\\ngeneric method to automatically extend expert agents to multi-agent systems via\\nthe evolutionary algorithm, thereby improving the effectiveness of LLM-based\\nagents in solving tasks. Specifically, we consider the existing agent frameworks\\nas the initial individual and then apply a series of evolutionary operators (e.g.,\\nmutation, crossover, selection, etc.) to generate multiple agents with diverse agent\\nsettings. EVOAGENT can be generalized to any LLM-based agent framework,\\nand can automatically extend the existing agent framework to multi-agent systems\\nwithout any extra human designs. Experimental results across various tasks have\\nshown that EVOAGENT can automatically generate multiple expert agents and\\nsignificantly enhance the task-solving capabilities of LLM-based agents.\\n1\\nIntroduction\\nRecently, the advent of large language models (LLMs) [1, 2, 3, 4] have shown remarkable capabilities\\nin solving language understanding, reasoning, and generation tasks. Based on the foundation of LLMs,\\nmany research works [5, 6, 7, 8, 9, 10, 11] have discovered that by empowering multiple advanced\\nskills (e.g., planning, tool, memory and so on), we can develop more powerful autonomous agents to\\nsolve more challenging tasks. Therefore, how to design and leverage LLM-based autonomous agents\\nto tackle more diverse and complex real-world applications has attracted enormous interest.\\nGenerally, many real-world scenarios are usually complex, encompassing a variety of challenging\\ntasks that are beyond the capability of a single agent. To address this point, we notice that human\\nsociety is composed of vast individuals, each possessing their unique characteristics. By selecting,\\norchestrating, and cooperating with different individuals, humans can form an efficient team group\\nto handle complicated missions in the real world. Therefore, there has been an increasing trend\\nto develop multi-agent collaboration frameworks (e.g., MetaGPT [10], AutoGen [12], Camel [13],\\n∗The first two authors have equal contributions. This work was done when the first author was an intern at\\nMicrosoft Research Asia.\\n† Corresponding authors.\\nPreprint. Under review.\\narXiv:2406.14228v2  [cs.AI]  11 Jul 2024\\nGenerative Agents [11]) to simulate human behaviors for solving complex tasks. By developing a\\nseries of expert agents with diverse settings, multi-agent systems enable us to reveal emergent abilities\\namong multiple agents and synergize their specialized expertise to achieve superior performance,\\nakin to simulating human populations. Nevertheless, it is worthy noting that, in most of (multi)-agent\\nframeworks, their designs heavily depend on handcrafted settings, including character roles, task\\nscopes, skills, and prompt settings. Although we admit that meticulous human design is quite useful\\nfor instructing LLM-based agents to understand tasks, it also limits scaling up the number of agents to\\nfurther improve performance due to expensive human labor. Considering the increasing popularity of\\nLLM-based autonomous agents, how to create a generic agent generation paradigm to automatically\\nbuild multi-agent systems has emerged as a critical challenge.\\nIn this paper, we introduce a novel method, EVOAGENT, that formulates agent generation as the\\nevolutionary processing [14] in human society. Specifically, to align human society, each agent\\ncan be considered as individuals that can procreate its population across successive generations.\\nMotivated by this mechanism, we can simulate such a human behavior to automatically generate'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='build multi-agent systems has emerged as a critical challenge.\\nIn this paper, we introduce a novel method, EVOAGENT, that formulates agent generation as the\\nevolutionary processing [14] in human society. Specifically, to align human society, each agent\\ncan be considered as individuals that can procreate its population across successive generations.\\nMotivated by this mechanism, we can simulate such a human behavior to automatically generate\\nmultiple agents based on any pre-defined agents. Therefore, EVOAGENT can be considered as a\\none-shot agent generation method that starts from a specialized agent as the initial agent, and then\\nconsiders its settings (e.g., role, skills, prompts, and so on) as the variables to be evolved. With a\\nseries operation of EAs (e.g., selection, crossover, mutation), EVOAGENT can automatically create\\nmultiple evolutionary agents based on the initial specialized agent. Moreover, EVOAGENT is not\\nlimited to the infrastructure of agent frameworks, as it is a generic multi-agent generation method.\\nThus, it can be applied to any agent framework and expanded to multi-agent systems without any\\nextra human effort.\\nWe conduct experiments on multiple datasets, including knowledge-based question answering and\\nmulti-modal reasoning (§ 4.1), interactive scientific solving (§ 4.2) and real-world complex planning\\n(§ 4.3). Experimental results indicate that EVOAGENT can generate multiple agents with diverse\\nskills and harness their capabilities to consistently improve model performance in different scenarios.\\nBesides, to validate the scalability of EVOAGENT in creating massive agents, we also apply our\\nmethod to some conversational scenarios (e.g., debate), and the results also indicate the potential of\\nEVOAGENT in generating multiple diverse agents. Overall, the contributions of this paper can be\\nsummarized as below:\\n• We introduce EVOAGENT, a simple and generic multi-agent generation method to improve the\\neffectiveness of LLM-based agents in solving tasks. EVOAGENT can automatically generate new\\nexpert agents and is applicable to any agent framework.\\n• We formulate the agent generation processing as an evolutionary pipeline, that encompasses\\nmultiple operators (e.g., selection, crossover, mutation) to generate agent population without\\nadditional human supervision.\\n• We conduct extensive experiments on various tasks and demonstrate the effectiveness, scalability,\\nand generality of our EVOAGENT. Particularly, EVOAGENT can significantly enhance the perfor-\\nmance of LLM-based agents in both challenging open-world scenarios and complex real-world\\nplanning by generating more specialized agents.\\n2\\nRelated Work\\nLLM-based Autonomous Agents\\nWith the emergence of powerful large language models [1, 2, 3,\\n4], many researchers have endeavored to develop advanced autonomous agents [5, 6, 7] empowered by\\nmultiple high-level LLM skills (e.g., personas [11, 15, 16], planning [9, 17, 18, 19], tool [8, 6, 20, 21]\\nand memory [22, 23]). Some of them also extend agent frameworks to multi-agent collaboration\\n(e.g., MetaGPT [24], Generative Agents [11], AutoGen [12], Camel [13], AgentVerse [25] and so\\non), by designing multiple specific roles. These systems also demonstrate satisfactory performance in\\naddressing massive, challenging tasks. However, it is worth noting that most of the popular agent\\nframeworks heavily relied on handcrafted designs. The abundant human efforts necessitated by these\\nsystems also limit the adaptability and flexibility of agents to handle unexpected challenges [26, 27,\\n25, 10]. In this paper, we propose EVOAGENT, a method that can be applied to any LLM-based\\nagent framework and easily extend to multi-agent systems. By using EA, our method allows us to\\niteratively generate and optimize multiple agents with diverse settings.\\n2\\nAlgorithm 1: Multi-Agent Generation with Evolutionary Algorithm\\nRequire: Initial agent A(0,0), population size N per iteration, number of iterations T,\\nquality-check module LLMQuality(⋅), evolutionary operations EvoCrossover(⋅) and\\nEvoMutation(⋅), EvoUpdate(⋅)\\nInput: Initial result R0 derived from A(0,0)\\nOutput: Final result RT\\n1 for t = 1 to T do\\n2\\nCrossover: Update the settings of parent agents based on their generated results and initial\\nagent: {A\\n′\\n(0,t−1),A\\n′'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='EvoMutation(⋅), EvoUpdate(⋅)\\nInput: Initial result R0 derived from A(0,0)\\nOutput: Final result RT\\n1 for t = 1 to T do\\n2\\nCrossover: Update the settings of parent agents based on their generated results and initial\\nagent: {A\\n′\\n(0,t−1),A\\n′\\n(1,t−1),...,A\\n′\\n(N−1,t−1)} ←\\nEvoCrossover({R(0,t−1),R(1,t−1),...,R(N−1,t−1)},A(0,0));\\n3\\nMutation: Generate N ′(N ′ > N) child agents based on parent agents and initial agent:\\n{A(0,t),A(1,t),...,A(N′−1,t)} ←\\nEvoMutation({A\\n′\\n(0,t−1),A\\n′\\n(1,t−1),...,A\\n′\\n(N−1,t−1)},A(0,0))\\n4\\nSelection: Select high-quality agents with quality-check module:\\n{At\\n0,...,At\\nN−1} ←LLMQuality({A(0,t),A(1,t),...,A(N′−1,t)});\\n5\\nResult Update: Generate new result from new agents:\\n{R(0,t),R(1,t),...,R(N−1,t)} ←{A(0,t),A(1,t),...,A(N−1,t)}\\n6\\nIntegrate their results as a natural selection processing:\\nRt ←EvoUpdate({R(0,t),R(1,t),...,R(N−1,t)},Rt−1);\\n7 return RT ;\\nAgent Generation\\nRecent studies have shown that assigning personas or roles to LLM-based\\nautonomous agents can influence their behavior and performance in generation tasks [28, 29, 30, 13].\\nCurrent methods primarily involve manually assigning these personas and limit multi-agent col-\\nlaboration to single or fixed roles, which requires significant human effort and hinders generaliza-\\ntion [13, 12, 24, 10]. To address this, some frameworks like AgentVerse [25] and AutoAgents [31]\\nhave been proposed to automatically generate unlimited agents for collaborative task completion.\\nHowever, these methods still heavily depend on human-designed interventions, which limits their\\nscalability and functionality. For example, AutoAgents requires agent settings to satisfy a “Planner\\n- Agent Observer - Plan Observer” framework. Meanwhile, AgentVerse formulates a pipeline of\\n“Expert Recruitment - Collaborative Decision Making - Action Execution - Evaluation” to build\\nagents. These architectures also limit the task scope of designing agents. In contrast, EVOAGENT\\ncan automatically formulate the current agent frameworks to multi-agent systems with high-quality\\ngenerated expert agents by using EAs, which is flexible and adaptable to various agent frameworks.\\n3\\nMethod\\nGenerally, human society comprises a broad spectrum of individuals from diverse cultures, encom-\\npassing multiple generations. To solve specific tasks, human society usually involves a lot of expert\\nindividuals and aggregates their specialized expertise to achieve better answer. Thus, it can be\\nconsidered as the foundation to facilitate multi-agent collaborations. To fulfill this point, how to\\nautomatically create multiple agents would be very critical. Inspired by evolutionism, we formulate\\nagent generation as an evolutionary process to help us generate multiple agents without any human\\nlabor.\\n3.1\\nPreliminary\\nEvolutionary algorithm (EA) [32, 33], is a general algorithm to simulate the biological behaviors in\\nevolution, including reproduction, mutation, recombination, and selection. By introducing genetic\\nalgorithm [34, 35, 36, 37, 38] of the “survival of the fittest” mechanism, it can also be considered as\\nan optimization method to improve individuals. Therefore, EAs also belong to the non-parametric\\nlearning method, which can be applied to any framework. All we need to do is define which\\nparts should be evolved and the corresponding evolutionary operators. We also note some recent\\n3\\nworks [39, 40] indicate the potential of EAs that can be applied to optimize discrete prompts. So, in'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content=\"an optimization method to improve individuals. Therefore, EAs also belong to the non-parametric\\nlearning method, which can be applied to any framework. All we need to do is define which\\nparts should be evolved and the corresponding evolutionary operators. We also note some recent\\n3\\nworks [39, 40] indicate the potential of EAs that can be applied to optimize discrete prompts. So, in\\nthis paper, we explore how to formulate the agent generation problem as an evolutionary task.\\n3.2\\nEVOAGENT\\nBy assigning various settings to specific skills (e.g., role-playing, planning, tools and so on), agents\\ncould exhibit diverse task-solving capabilities. Therefore, our objective is to produce a population\\nof agents with distinct skills, to establish effective multi-agent systems. To fulfill this point, we\\ntreat each specialized agent as an unique individual and denote each skill as the part to be evolved,\\nakin to humans. So, we consider the procedure of agent generation to be evolutionary processing.\\nSpecifically, existing frameworks usually describe agent skills as the language. Thus, we can employ\\nLLM to simulate evolutionary operators to update the system settings of agents and create new agents.\\nHere, we formulate the procedure of EVOAGENT as a four-stage pipeline:\\nQuery: Please create a travel plan where I'll depart from Washington and head to Myrtle \\nBeach for a 3-day trip from March 13th to March 15th, 2022. Can you help me keep this \\njourney within a budget of $1,400? It's vital that my accommodations are pet-friendly.\\nSTEP 1: Initialization\\nInitial \\nAgent\\nHuman \\nWritten\\nSTEP 2: EA Operation Crossover & Mutation\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nLunch: Exotic India\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Cozy Brooklyn Room\\nInitial Agent\\nAccommodation \\nAgent\\nTransportation \\nAgent\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie's, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Large sunny park slope apartment, \\npet-friendly\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nTransportation: Flight Number: F3792603\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie's, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Cozy Brooklyn Room\\nHotel Agent\\nSTEP 3: Selection\\nQuality Check\\nThis agent has a duplicate type with \\nAccommodation Agent, so it is discarded.\\nSTEP 4: Results Update\\nUpdate \\nOperation\\nDay 1:\\nCurrent City: from Washington to Myrtle Beach\\nTransportation: Flight Number: F3792603\\nBreakfast: Exotic India, Myrtle Beach\\nLunch: Catfish Charlie's, Myrtle Beach\\nAttraction: SkyWheel Myrtle Beach\\nAccommodation: Large sunny park slope apartment, pet-friendly\\nFigure 1:\\nThe illustration of EVOAGENT. With the\\ngenerated multiple expert agents, EVOAGENT can gen-\\nerate a better travel plan to meet user preferences. For\\nEA operators, Crossover can improve the results of\\nparent agents by adjusting existing details (e.g., the in-\\nformation marked as blue). Mutation can introduce\\nnew variations to refine the results of parent agents by\\ngenerating child agents with new characteristics (e.g.,\\nthe information marked as red).\\nSTEP 1: Initialization\\nTo conduct EAs,\\nwe first need to confirm our initial agents.\\nHere, we enable EVOAGENT to start\\nfrom a pre-defined agent framework (e.g.,\\nMetaGPT [10] and AutoGen [12]), which\\nserves as the initial (parent) agents. More-\\nover, we also define which parts of this\\nagent should be upgraded. Generally, since\\nEAs is a generic algorithm, EVOAGENT\\nis applicable to any agent frameworks and\\nextends them as multi-agent frameworks.\\nWe will then explore how to generate new\\nagents in the next steps.\\nSTEP 2: Crossover & Mutation\\nIn the\\nfirst iteration, we directly use the initial\\nagents as the parents. And then, we design\\ntwo kinds of evolutionary operators, named\\nCrossover and Mutation. For Crossover,\\nwe first enable the parent agents to generate\\nresults based on user requests. Then, based\\non the generated results, we ask LLMs to\\ncheck which skills should be improved and\"), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='agents in the next steps.\\nSTEP 2: Crossover & Mutation\\nIn the\\nfirst iteration, we directly use the initial\\nagents as the parents. And then, we design\\ntwo kinds of evolutionary operators, named\\nCrossover and Mutation. For Crossover,\\nwe first enable the parent agents to generate\\nresults based on user requests. Then, based\\non the generated results, we ask LLMs to\\ncheck which skills should be improved and\\nthen update them. This mechanism allows\\nus to generate child agents in new settings\\nwithout requiring any human labor. More-\\nover, we also need to guarantee the diver-\\nsity between the child agents and parents.\\nTo this end, we design a Mutation operation\\nthat requires LLMs to compare the child\\nagents and parent agents and then modify\\nthe child agents to make them distinct from\\ntheir parents while maintaining their task-\\nsolving capability. Based on these evolu-\\ntionary operators, we can generate effective\\nand diverse agents during one iteration. Be-\\nsides, as we also need to conduct multiple iterations, we will append all agents generated in the\\nprevious generation into the next iteration. How to select these agents during each iteration will be\\nintroduced next.\\nSTEP 3: Selection\\nBased on the above steps, we can obtain multiple candidate agents with diverse\\nsettings. To guarantee the quality of each agent, we also introduce a selection mechanism like EAs.\\nHere, we conduct a quality-check module with an LLM to detect whether the generated agents can\\nsatisfy it has inherited the characteristics and maintained differences from parent agents. We will\\nselect N child agents as the evolved agents in each iteration.\\n4\\nTable 1: Results of LLMs with different methods on Logic Grid Puzzle (Logic), Trivia Creative\\nWriting (Writing) and Codenames Collaborative (Codenames). The best results are bolded, and the\\nsecond best ones are underlined.\\nModel\\nMethod\\nLogic Writing Codenames\\nLLama2-13B-Chat\\nDirect\\n4.00\\n28.00\\n0.00\\nCoT\\n26.00\\n46.00\\n18.00\\nSelf-Refine3\\n33.50\\n31.20\\n12.37\\nSPP\\n0.00\\n4.00\\n1.00\\nEVOAGENT(1,3)\\n35.50\\n49.60\\n27.83\\nGPT-3.5\\nDirect\\n48.00\\n56.20\\n76.29\\nCoT\\n47.50\\n51.00\\n71.13\\nSelf-Refine3\\n47.50\\n59.19\\n46.39\\nSPP\\n56.00\\n54.40\\n61.86\\nEVOAGENT(1,3)\\n71.50\\n60.80\\n79.38\\nGPT-4\\nDirect\\n60.50\\n75.40\\n79.38\\nCoT\\n65.50\\n74.00\\n80.41\\nSelf-Refine3\\n64.50\\n74.60\\n79.38\\nSPP\\n64.50\\n79.20\\n78.35\\nEVOAGENT(1,3)\\n77.00\\n84.40\\n84.53\\nSTEP 4: Results Update\\nBased on the above steps, we obtain many new agents that evolved from\\nparent agents, but with diverse settings. To improve task-solving capabilities, we ask each child agent\\nto generate candidate results and then use LLMs to integrate these candidates with the result from the\\nprevious iteration into a new result, akin to a natural selection processing stage. Moreover, we can\\nautomatically generate more agents by repeating the operations from step 2 to step 4 until the number\\nof agents has fulfilled our targets.\\nBy introducing EA, EVOAGENT enables us to automatically extend the existing agent framework\\nto a multi-agent system without any extra human designs. The mechanism also makes EVOAGENT\\ncan be applied to any agent framework without any prerequisites. The entire process is illustrated in\\nFigure 1. And we also present the details of EVOAGENT in Algorithm 1.\\n4\\nExperiment\\nIn this section, we adopt EVOAGENT to multiple applications to illustrate that EVOAGENT can\\nhelp LLM-based agents better accomplish tasks with multi-agent generation.3 Furthermore, we also\\ndemonstrate that EVOAGENT can be applicable in supporting currently widely used multi-agent\\nframeworks, such as MetaGPT [10], AutoGen [12], and Camel [13].\\n4.1\\nNLP and Multi-Modal Tasks\\nBenchmarks'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='help LLM-based agents better accomplish tasks with multi-agent generation.3 Furthermore, we also\\ndemonstrate that EVOAGENT can be applicable in supporting currently widely used multi-agent\\nframeworks, such as MetaGPT [10], AutoGen [12], and Camel [13].\\n4.1\\nNLP and Multi-Modal Tasks\\nBenchmarks\\nTo align previous experiences (e.g., Self-Refine [41] and Solo Performance Prompt-\\ning [42]), we select three NLP knowledge-intensive and reasoning-intensive tasks from [42] and one\\nmulti-modal task:\\n• Logic Grid Puzzle is a reasoning task with 200 puzzles featuring 2 to 5 unique occupants in\\ndifferent houses. The aim is to identify house numbers for one occupant with provided clues.\\n• Trivia Creative Writing is a knowledge-intensive task consisting of 100 instances. This task\\nrequires a model to write a coherent story while incorporating answers to N trivia questions.\\n• Codenames Collaborative is a reasoning-intensive task with 50 instances. It involves a model\\nidentifying target words based on a given hint and a complete list of words.\\n• MMMU [43] is a comprehensive benchmark for college-level, multi-discipline multi-modal\\nunderstanding and reasoning. MMMU has three levels of difficulty: easy, medium, and hard. We\\nevaluate EVOAGENT against baselines using the multiple-choice questions in the validation set of\\nMMMU, which includes 847 questions spanning 30 different domains.\\n3The data examples of EVOAGENT on these tasks are provided in Appendix F.\\n5\\nCoT\\nSR\\nSPP Ours\\n45\\n50\\n55\\n60\\n65\\nAccuracy(%)\\nAll\\nCoT\\nSR\\nSPP Ours\\n55\\n65\\n75\\nEasy\\nCoT\\nSR\\nSPP Ours\\n45\\n50\\n55\\n60\\nMedium\\nCoT\\nSR\\nSPP Ours\\n25\\n35\\n45\\n55\\nHard\\nGPT-4V\\nGemini-Pro\\nFigure 2: Overall results of GPT-4V and Gemini-Pro with different methods on the MMMU validation\\nset. We also compare the performance of GPT-4V and Gemini-Pro across three difficulty levels.\\nBaselines\\nFor NLP tasks, we select LLama2-13B-Chat [3], GPT-3.5 [44] and GPT-4 [1] as our\\nbackbone networks. We compare EVOAGENT with 0-shot learning (Direct), Chain-of-thought (CoT)\\nprompting [45] and Self-Refine [41] and Solo Performance Prompting (SPP) [42]. For Self-Refine,\\nwe follow [41] to design feedback and refine prompts with three iterations. SPP is not a multi-agent\\ncollaboration framework but a prompting strategy that asks a single LLM to identify and discuss with\\nmultiple personas with few-shot learning. For SPP, we follow the original setting [42] to make a fair\\ncomparison. For MMMU, we select GPT-4V [46] and Gemini-Pro as the backbone and compare\\nEVOAGENT with CoT prompting, Self-Refine, and SPP. 4\\nEvaluation Metrics\\nFor all benchmarks, we adhere to the evaluation metrics specified in the\\noriginal setting. Specifically, for Logic Grid Puzzle and MMMU tasks, we report the accuracy of\\nall questions. For Trivia Creative Writing, we measure the ratio of correctly mentioned answers in\\nthe trivia questions. For Codenames Collaborative, we calculate the overlapping ratio between the\\npredicted words from the Guesser and the target words as the metric.\\nResult & Analysis\\nIn our experiments, we adopt the agent settings of [42] (for NLP tasks) and\\n[43] (for MMMU) as the initial agent. For our method, we denote it as EVOAGENT(N,T ), where N\\nis the population size generated in each iteration, and T is the number of iterations. Here, to align\\nwith Self-Refine, we set N as 1 and T as 3, which means we conduct three iterations, each of which\\ngenerates a new expert agent. Our results are reported in Table 1, and we can observe:\\n1. By utilizing multiple generated agents, EVOAGENT can greatly improve LLM performances in\\nboth NLP knowledge and reasoning tasks. We also compare EVOAGENT with some pre-defined'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='with Self-Refine, we set N as 1 and T as 3, which means we conduct three iterations, each of which\\ngenerates a new expert agent. Our results are reported in Table 1, and we can observe:\\n1. By utilizing multiple generated agents, EVOAGENT can greatly improve LLM performances in\\nboth NLP knowledge and reasoning tasks. We also compare EVOAGENT with some pre-defined\\nagent generation frameworks, e.g., AgentVerse [25] and AutoAgent [31]. The results shown in\\nAppendix B prove that the EVOAGENT is even better than these agent generation frameworks.\\n2. When using weaker LLMs (e.g., LLama2-13B-Chat), SPP usually produces poor performances,\\nconsistent with the findings in [42]. This suggests the limited effectiveness of SPP in smaller and\\nless capable models. However, EVOAGENT can provide consistent improvements among each\\nLLM, proving its strong generalization by using diverse generated agents.\\nIn addition, Figure 2 shows that Self-Refine (SR) and SPP degrade performance compared to CoT\\nprompting in MMMU task. However, EVOAGENT can generate multiple domain-specific agents and\\nthus improve multi-modal models in addressing scientific questions across various difficulty levels.\\n4.2\\nInteractive Scientific Solving Simulation\\nBenchmark\\nCompared with traditional NLP or multi-modal tasks, autonomous agents usually need\\nto perform problem-solving abilities akin to humans in interactive and open-world environments.\\nCurrently, we choose ScienceWorld [47], a complex interactive environment requiring skills in\\nlong-term memory, sub-task decomposition, and scientific and commonsense knowledge. Here, we\\nevaluate 30 scientific tasks in ScienceWorld to demonstrate the capability of EVOAGENT in solving\\ntasks in more challenging open-world environments.\\n4The detailed model parameters and versions and full prompts for these methods can be found in Appendix A.\\n6\\nTable 2: Average Scores of different methods on Sci-\\nenceWorld. We also report performance on three\\ndifficult-level groups based on the average length of\\nthe oracle agent’s trajectories [48].\\nModel\\nOverall\\nLong Medium Short\\nGPT-3.5\\n17.12\\n6.28\\n19.91\\n27.90\\nw/ EVOAGENT(1,1)\\n19.02\\n7.25\\n18.87\\n33.26\\nGPT-4\\n27.97 10.58\\n36.00\\n42.41\\nw/ EVOAGENT(1,1)\\n30.42 11.38\\n36.17\\n48.67\\nBaseline and Evaluation Metrics\\nFollow-\\ning [48], we require LLMs to perform an\\naction at each step by using in-context learn-\\ning 5. For evaluation, each task in Science-\\nWorld includes some sub-tasks, and we re-\\nport the results by calculating the completed\\nsub-tasks for the whole task.\\nResult & Analysis\\nFor EVOAGENT, we\\nadopt the agent framework with original set-\\ntings in [48] as the initial agent. Since each\\nstep in ScienceWorld requires using EA, we\\nset the population size N as 1 and the itera-\\ntions T as 1 for efficiency, denoted as EVOA-\\nGENT(1,1). Results in Table 2 show that:\\n1. EVOAGENT can also extend interactive agents to multi-agent systems in solving complete scientific\\ntasks in dynamic, open-world environments and consistently improve the performance of LLMs.\\n2. Our method exhibits the most substantial improvement in short-trajectory tasks, with less sig-\\nnificant gains in medium and long-trajectory tasks. We argue that the capability of multi-agent\\nsystems will also be affected by a longer context. We also expect to investigate the effect of long\\ncontext on multi-agent systems in the future.\\nGenerally, these results also demonstrate the generalization of EVOAGENT, which can also be used\\nfor solving interactive tasks in an open-world environment.\\n4.3\\nReal-World Scenarios\\nBenchmark\\nMoreover, in addition to performing actions in interactive environments, planning in\\ncomplex and realistic environments is also a crucial skill for building autonomous agents. To validate\\nthis point, we also select TravelPlanner [49], a benchmark designed to evaluate language agents in\\nreal-world complex planning with multiple constraints.\\nBaseline and Evaluation Metrics\\nFollowing [49], we select Mistral-7B [50], GPT-3.5, Gemini-'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='Benchmark\\nMoreover, in addition to performing actions in interactive environments, planning in\\ncomplex and realistic environments is also a crucial skill for building autonomous agents. To validate\\nthis point, we also select TravelPlanner [49], a benchmark designed to evaluate language agents in\\nreal-world complex planning with multiple constraints.\\nBaseline and Evaluation Metrics\\nFollowing [49], we select Mistral-7B [50], GPT-3.5, Gemini-\\nPro [2] and GPT-4 as our backbone models. We compare EVOAGENT with 0-shot learning (Direct),\\nCoT prompting, SPP, and Self-Refine within each backbone model. Furthermore, we also attempt\\nthe ReAcT method [51] for GPT-3.5, which introduces a virtual ‘think’ action to generate sub-\\ntasks during the action planning process. For evaluation, we adhere to the original metrics from\\nTravelPlanner, reporting the delivery rate, commonsense constraint pass rate, hard constraint pass\\nrate, and final pass rate for all methods 6.\\nResult & Analysis\\nFor EVOAGENT, we adopt the original settings in TravelPlanner as the initial\\nagent. Results in Table 3 show that:\\n1. EVOAGENT can generate specialized agents, such as those focused on culinary experiences,\\ntransportation, and attractions. Therefore, the generated travel plans are more aligned with user\\npreferences (hard constraints) and commonsense rules;\\n2. Although existing paradigms (e.g., CoT, ReAct, Self-Refine, SPP) have demonstrated decent re-\\nsults in some conventional NLP tasks, they still lack capability in handling complex planning tasks\\nwithin TravelPlanner. These results also demonstrate that only using human-design prompting\\nstrategies is insufficient to handle complex planning tasks.\\n3. By using EVOAGENT to automatically generate multiple agents and forming a multi-agent\\ncollaboration paradigm, we can develop higher-quality plans that better meet user preferences.\\nThat also indicates the significance of multi-agent systems for complex planning tasks.\\n5The introduction of the settings of LLMs are shown in Appendix C.\\n6Detailed introduction of experiment settings is provided in Appendix D.\\n7\\nTable 3: Main results of different LLMs and planning strategies on the TravelPlanner validation set.\\nEVOAGENT(N,T ) indicates that the population size per iteration is N and the number of iterations is\\nT. The best results are bolded, and the second best ones are underlined.\\nModel\\nMethod\\nDelivery Commonsense Hard Constraint Final\\nRate\\nMicro Macro Micro\\nMacro\\nMistral-7B\\nDirect\\n100.0\\n64.7\\n2.2\\n3.1\\n0.0\\n0.0\\nCoT\\n100.0\\n60.5\\n1.1\\n1.0\\n0.0\\n0.0\\nSPP\\n100.0\\n55.1\\n0.0\\n0.7\\n0.6\\n0.0\\nSelf-Refine3\\n100.0\\n58.3\\n0.0\\n0.7\\n0.0\\n0.0\\nEVOAGENT(1,3)\\n100.0\\n60.1\\n2.2\\n4.5\\n0.6\\n0.0\\nGPT-3.5\\nDirect\\n100.0\\n57.3\\n3.9\\n11.0\\n3.3\\n0.0\\nCoT\\n100.0\\n61.0\\n2.8\\n10.0\\n3.3\\n0.0\\nReAct\\n82.2\\n42.3\\n0.6\\n11.9\\n4.6\\n0.0\\nSPP\\n99.4\\n54.6\\n1.7\\n3.8\\n1.1\\n0.0\\nSelf-Refine3\\n100.0\\n56.0\\n1.7\\n3.1\\n1.1\\n0.0\\nEVOAGENT(1,3)\\n100.0\\n64.2\\n7.8\\n11.0\\n4.4\\n1.1\\nEVOAGENT(1,5)\\n100.0\\n61.0\\n5.0\\n12.6\\n5.0\\n0.5\\nGemini-Pro\\nDirect\\n90.0\\n61.7\\n7.8\\n16.4\\n7.8\\n0.6\\nCoT\\n90.0\\n61.4\\n7.2\\n10.0\\n6.1\\n1.7\\nSPP\\n100.0\\n67.6'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='100.0\\n61.0\\n5.0\\n12.6\\n5.0\\n0.5\\nGemini-Pro\\nDirect\\n90.0\\n61.7\\n7.8\\n16.4\\n7.8\\n0.6\\nCoT\\n90.0\\n61.4\\n7.2\\n10.0\\n6.1\\n1.7\\nSPP\\n100.0\\n67.6\\n7.8\\n10.2\\n3.9\\n1.1\\nSelf-Refine3\\n95.6\\n65.8\\n6.1\\n15.0\\n4.4\\n0.6\\nEVOAGENT(1,3)\\n100.0\\n73.5\\n12.8\\n16.9\\n7.2\\n1.7\\nEVOAGENT(1,5)\\n100.0\\n74.0\\n8.9\\n21.2\\n11.7\\n2.2\\nGPT-4\\nDirect\\n100.0\\n79.4\\n15.8\\n27.5\\n16.1\\n2.2\\nCoT\\n100.0\\n76.7\\n11.7\\n22.4\\n12.8\\n2.2\\nSPP\\n96.7\\n70.6\\n5.6\\n11.4\\n7.8\\n0.6\\nSelf-Refine3\\n98.9\\n75.3\\n7.2\\n12.4\\n7.2\\n1.1\\nEVOAGENT(1,3)\\n100.0\\n81.5\\n21.1\\n31.4\\n18.9\\n7.2\\n4.4\\nAblation Studies\\nTo better understand the value of EVOAGENT, we conduct detailed analyses on TravelPlanner,\\nfocusing on the impact of population size and the effectiveness of the quality-check module in the\\nselection stage.\\nTable 4: Average commonsense constraint\\npass rate (Com.) and hard constraint pass rate\\n(Hard) of ablated variants on TravelPlanner.\\nMethod\\nw/o QC\\nw/ QC\\nCom. Hard Com. Hard\\nDirect\\n-\\n-\\n59.5\\n13.7\\nSuggest3\\n-\\n-\\n61.7\\n8.4\\nOvergen3\\n-\\n-\\n61.4\\n10.7\\nPromptRefine3\\n-\\n-\\n63.0\\n13.8\\nDifferent Population Size\\nEVOAGENT(1,3)\\n68.9\\n14.0\\n68.9\\n14.0\\nEVOAGENT(2,3)\\n62.8\\n12.7\\n67.0\\n15.2\\nEVOAGENT(3,3)\\n62.7\\n13.7\\n66.8\\n15.8\\nDifferent Selection Stategies\\nRandom\\n62.9\\n12.7\\n67.1\\n15.0\\nPK\\n63.5\\n13.6\\n66.4\\n14.5\\nAll-in\\n61.9\\n13.2\\n67.1\\n17.0\\nExperiment Settings\\nWe evaluate the performance\\nof different LLMs at varying population sizes N\\nwith fixed iteration number 3, denoted as EVOA-\\nGENT(N,3), both with and without the quality-check\\nmodule (QC). We employ an LLM that shares the\\nsame backbone as the initial agent for updates. To\\nselect results from candidates for this LLM to update,\\nwe adopt three different selection strategies: 1) Ran-\\ndom: one result is selected randomly from the pool\\nof candidates; 2) PK: we ask an agent with the same\\nbackbone as the initial agent to identify the optimal\\nresults from the pool of candidates; 3) All-in: Rather\\nthan selecting a single result, we update using all\\ncandidates.\\nMoreover, we also attempt Suggest3, Overgen3 and\\nPromptRefine3 as variants to prove the effectiveness\\nof our method. For Suggest3, instead of generating\\nnew results, we ask new generated agents to only give\\nsuggestions for initial agents to revise their results.\\nFor Overgen3, we first ask initial agents to generate\\n3 different results at one time, and then these agents\\n8\\nEvoAgent\\nInvesting in clean energy not only addresses the climate \\ncrisis but also creates jobs and strengthens our economy.\\n...transitioning to renewable energy can create \\nmillions of good-paying, union jobs without \\nsignificant unemployment or economic fallout...\\nLabor \\nEconomist \\nAgent\\nPresident Opinion: Support'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content=\"For Overgen3, we first ask initial agents to generate\\n3 different results at one time, and then these agents\\n8\\nEvoAgent\\nInvesting in clean energy not only addresses the climate \\ncrisis but also creates jobs and strengthens our economy.\\n...transitioning to renewable energy can create \\nmillions of good-paying, union jobs without \\nsignificant unemployment or economic fallout...\\nLabor \\nEconomist \\nAgent\\nPresident Opinion: Support\\n...Thrusting forward with renewable energy \\nstrengthens our international ties and \\npropels economies reliant on fossil fuel \\nexports towards clean energy transitions...\\nGeopolitical \\nAnalyst \\nAgent\\n...Every moment we delay increases the \\nseverity of climate-related illnesses, straining \\nour health infrastructure and costing us $820 \\nbillion annually...\\nPublic Health \\nAgent\\nMetaGPT Framework\\nTopic: The U.S. should commit more in climate change fighting\\nEvoAgent\\nEnergy Sector \\nAnalyst Agent\\nThe promises of ample job creation overlook \\nthe reality that many displaced workers \\nfrom conventional sectors may struggle to \\nfind roles in the nascent green economy. \\nAn abrupt transition to renewable energy \\ncould cause economic tremors and job \\nlosses.\\nRisk \\nManagement \\nAgent\\nThis isn‘t about alarmism or denial, it’s \\nabout carefully leading our nation \\ntowards a sustainable, prosperous future. \\nAn abrupt shift spells risk!\\nTransition \\nStrategist \\nAgent\\nThe real crisis is the economic disaster under His policies. He \\ntalks about investments, but it's your tax dollars he's spending. \\nPresident Opinion: Oppose\\nFigure 3: The adaption of EVOAGENT on MetaGPT framework. With the EA, we can extend the\\noriginal role in the debate scenario to different expert agents to enrich the opinions.\\ncan output the final results based on these multiple\\ncandidates. For PromptRefine3, instead of generating agents, we ask the initial agent to refine its\\nprompts three times to better answer the query. 7\\nResult & Analysis\\nTo obtain stable findings, we first obtain results from GPT-3.5 and Gemini-Pro\\nacross different population sizes and selection strategies. We then average their results over various\\nmetrics to clearly compare the strengths and weaknesses of these variants.\\nThe results are shown in Table 4.8 We find that EVOAGENT significantly outperforms the Overgen,\\ndemonstrating the effectiveness of generating specialized agents to assist with complex planning.\\nAlthough obtaining suggestions from new generated agents can improve the performance on com-\\nmonsense constraints, these methods greatly harm the agents to meet the user preference. Modifying\\nthe prompt can improve the performance of agents, yet it remains less effective than EVOAGENT.\\nWhen the population size exceeds one, agents may generate similar agents. Thus, lacking a quality-\\ncheck module leads to reduced travel plan quality. Furthermore, when population size increases,\\nthe model aligns travel plans more closely with user preferences but diminishing adherence to\\ncommonsense rules, consistent with the findings in Table 3. Remarkably, the PK strategy initially\\nyields superior results without the quality-check module, but this trend reverses once quality checks\\nare implemented. We speculate that, without the quality-check module, PK partially fulfills this role,\\naiding in selecting better candidates. However, with the quality-check module, PK introduces bias by\\nfavoring specific fields of expertise while neglecting others, resulting in a less effective than random\\nstrategy. Meanwhile, the All-in strategy performs optimally when a quality-check module is included.\\nFuture research can leverage long-context LLMs to expand more agents with EVOAGENT to better\\nsolve complex real-world tasks.\\n4.5\\nEVOAGENT Application\\nPrevious experiments have demonstrated that our method can automatically extend existing agent\\nframeworks to multi-agent systems, thus greatly improving LLM-based agents in various scenarios.\\nWe also attempt to extend our work to real-world multi-agent applications (e.g., MetaGPT [10],\\nCamel [13] and AutoGen [12]), to verify it can scale up the number of agents in building multi-agent\\nscenarios, just as shown in Figure 3. Here, we choose the debate scenario used in MetaGPT, which\\nincludes two debaters with different opinions, leading to dull and repetitive content generation. Here,\\ninstead of manually assigning new roles, we applied EVOAGENT to extend each debate team to more\"), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='Camel [13] and AutoGen [12]), to verify it can scale up the number of agents in building multi-agent\\nscenarios, just as shown in Figure 3. Here, we choose the debate scenario used in MetaGPT, which\\nincludes two debaters with different opinions, leading to dull and repetitive content generation. Here,\\ninstead of manually assigning new roles, we applied EVOAGENT to extend each debate team to more\\nagents with diverse settings, increasing the variety of opinions and the quality of the debate 9.\\n7The full prompts of different ablation settings are shown in Appendix A.1.\\n8The complete results with further analysis are shown in Appendix E\\n9The details of MetaGPT, and the adaption of EVOAGENT on Camel and AutoGen are shown in Appendix G.\\n9\\n5\\nConclusion\\nIn this paper, we propose EVOAGENT, an automatic multi-agent generation system by leveraging\\nevolutionary algorithms. Different from previous methods, EVOAGENT is suitable to any existing\\nagent framework and extends it to multi-agent systems with diverse and effective agents by using\\na series of evolutionary operations, including mutation, crossover, and selection. Experiments on\\nmultiple tasks show that EVOAGENT can significantly improve the capabilities of LLM-based agents\\nin solving complex tasks.\\nReferences\\n[1] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.\\n[2] Gemini Team. Gemini: A family of highly capable multimodal models, 2023.\\n[3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\\nKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-\\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\\nand fine-tuned chat models. CoRR, abs/2307.09288, 2023.\\n[4] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.\\n[5] Significant Gravitas. Auto-gpt: An autonomous gpt-4 experiment. https://github.com/\\nSignificant-Gravitas/Auto-GPT, 2023.\\n[6] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\\nginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580,\\n2023.\\n[7] Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.\\n[8] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\\nthemselves to use tools. CoRR, abs/2302.04761, 2023.'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='[8] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\\nthemselves to use tools. CoRR, abs/2302.04761, 2023.\\n[9] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\\nmodels. In NeurIPS, 2022.\\n[10] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang,\\nCeyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng\\nXiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent\\ncollaborative framework. In The Twelfth International Conference on Learning Representations,\\n2024.\\n[11] Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang,\\nand Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In\\nProceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,\\nUIST 2023, San Francisco, CA, USA, 29 October 2023- 1 November 2023, pages 2:1–2:22.\\nACM, 2023.\\n10\\n[12] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\\nLi Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via\\nmulti-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.\\n[13] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\\nGhanem. CAMEL: Communicative agents for ”mind” exploration of large language model\\nsociety. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n[14] Thomas Bäck and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter\\noptimization. Evol. Comput., 1(1):1–23, 1993.\\n[15] Xintao Wang, Yunze Xiao, Jen tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei,\\nZiang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. Incharacter: Evaluating\\npersonality fidelity in role-playing agents through psychological interviews, 2024.\\n[16] Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li,\\nRuihan Yang, Tinghui Zhu, et al. From persona to personalization: A survey on role-playing\\nlanguage agents. arXiv preprint arXiv:2404.18231, 2024.\\n[17] Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, and Kyle Richardson. Put\\nyour money where your mouth is: Evaluating strategic planning and execution of llm agents in\\nan auction arena. arXiv preprint arXiv:2310.05746, 2023.\\n[18] Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, and Jiangjie Chen.\\nTimearena: Shaping efficient multitasking language agents in a time-aware simulation. arXiv\\npreprint arXiv:2402.05733, 2024.\\n[19] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski,\\nYanghua Xiao, and Deqing Yang. Distilling script knowledge from large language models for\\nconstrained language planning. arXiv preprint arXiv:2305.05252, 2023.'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='preprint arXiv:2402.05733, 2024.\\n[19] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski,\\nYanghua Xiao, and Deqing Yang. Distilling script knowledge from large language models for\\nconstrained language planning. arXiv preprint arXiv:2305.05252, 2023.\\n[20] Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu,\\nDongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for task\\nautomation. arXiv preprint arXiv:2311.18760, 2023.\\n[21] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li,\\nand Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv\\npreprint arXiv:2401.06201, 2024.\\n[22] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In 3rd International\\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\\nConference Track Proceedings, 2015.\\n[23] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\\n[24] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human\\nbehaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv\\npreprint arXiv:2310.06500, 2023.\\n[25] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu,\\nYaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong\\nSun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent\\nbehaviors. In The Twelfth International Conference on Learning Representations, 2024.\\n[26] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu,\\nand Maosong Sun.\\nCommunicative agents for software development.\\narXiv preprint\\narXiv:2307.07924, 2023.\\n[27] Zhitao He, Pengfei Cao, Yubo Chen, Kang Liu, Ruopeng Li, Mengshu Sun, and Jun Zhao.\\nLEGO: A multi-agent collaborative framework with role-playing and iterative feedback for\\ncausality explanation generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\\nFindings of the Association for Computational Linguistics: EMNLP 2023, pages 9142–9163,\\nSingapore, December 2023. Association for Computational Linguistics.\\n11\\n[28] Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong\\nMao. Expertprompting: Instructing large language models to be distinguished experts. arXiv\\npreprint arXiv:2305.14688, 2023.\\n[29] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik\\nNarasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. In Houda\\nBouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational\\nLinguistics: EMNLP 2023, pages 1236–1270, Singapore, December 2023. Association for\\nComputational Linguistics.\\n[30] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceed-\\nings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='Computational Linguistics.\\n[30] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceed-\\nings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST\\n’23, New York, NY, USA, 2023. Association for Computing Machinery.\\n[31] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu,\\nand Yemin Shi. Autoagents: A framework for automatic agent generation. arXiv preprint\\narXiv:2309.17288, 2023.\\n[32] Thomas Bartz-Beielstein, Jürgen Branke, Jörn Mehnen, and Olaf Mersmann. Evolutionary\\nalgorithms. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 4(3):178–\\n195, 2014.\\n[33] Agoston E Eiben, James E Smith, AE Eiben, and JE Smith. What is an evolutionary algorithm?\\nIntroduction to evolutionary computing, pages 25–48, 2015.\\n[34] Jeffrey R Sampson. Adaptation in natural and artificial systems (john h. holland), 1976.\\n[35] John H Holland. Adaptation in natural and artificial systems: an introductory analysis with\\napplications to biology, control, and artificial intelligence. MIT press, 1992.\\n[36] Melanie Mitchell. An introduction to genetic algorithms. MIT press, 1998.\\n[37] Lothar M Schmitt. Theory of genetic algorithms. Theoretical Computer Science, 259(1-2):1–61,\\n2001.\\n[38] Seyedali Mirjalili, Jin Song Dong, Ali Safa Sadiq, and Hossam Faris. Genetic algorithm:\\nTheory, literature review, and application in image reconstruction. Nature-inspired optimizers:\\nTheories, literature reviews and applications, pages 69–85, 2020.\\n[39] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang\\nBian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields\\npowerful prompt optimizers. CoRR, abs/2309.08532, 2023.\\n[40] Angelica Chen, David Dohan, and David R. So. Evoprompting: Language models for code-level\\nneural architecture search. In Advances in Neural Information Processing Systems 36: Annual\\nConference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\\nUSA, December 10 - 16, 2023, 2023.\\n[41] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine:\\nIterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information\\nProcessing Systems, 2023.\\n[42] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing\\ncognitive synergy in large language models: A task-solving agent through multi-persona\\nself-collaboration. arXiv preprint arXiv:2307.05300, 2023.\\n[43] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\\nunderstanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.\\n12\\n[44] OpenAI. Chatgpt, 2022.\\n[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='12\\n[44] OpenAI. Chatgpt, 2022.\\n[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\\nmodels. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\\nAdvances in Neural Information Processing Systems, 2022.\\n[46] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\\narXiv:2309.17421, 9(1):1, 2023.\\n[47] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Science-\\nWorld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa Kozareva, and\\nYue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 11279–11298, Abu Dhabi, United Arab Emirates, December 2022.\\nAssociation for Computational Linguistics.\\n[48] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula,\\nPrithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with\\nfast and slow thinking for complex interactive tasks. In Thirty-seventh Conference on Neural\\nInformation Processing Systems, 2023.\\n[49] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao,\\nand Yu Su. Travelplanner: A benchmark for real-world planning with language agents. arXiv\\npreprint arXiv:2402.01622, 2024.\\n[50] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\\nLavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.\\n[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and\\nYuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh\\nInternational Conference on Learning Representations, 2023.\\nA\\nExperiment Settings\\nA.1\\nPrompt for Baselines and EVOAGENT\\nListing 1 and 2 shows the full prompt for 0-shot learning (Direct), Chain-of-thought (CoT) prompt-\\ning [45] and Self-Refine [41] and Solo Performance Prompting, i.e., SPP [42]. Listing 3 and 4 show\\nthe prompt of EVOAGENT and different ablation settings.\\nA.2\\nModel Selection\\nFor OpenAI models,\\nwe use gpt-35-turbo and gpt-4-32k with the version of\\n2024-02-15-preview in Azure.10 For Gemini-pro, we use Google Gemini-Pro APIs to ob-\\ntain results. We set the temperature to 0 for all models.\\nB\\nEVOAGENT v.s. Human-designed Agent Framework\\nAgentVerse [25] and AutoAgent [31] are frameworks designed to generate an unlimited number\\nof agents for collaborative tasks automatically. Despite this automation, they still rely on human-\\ndesigned interventions. AutoAgents requires agent settings to satisfy a “Planner - Agent Observer\\n- Plan Observer” framework, while AgentVerse formulates a pipeline of “Expert Recruitment -\\nCollaborative Decision Making - Action Execution - Evaluation” to build agents. We argue that these\\nhuman-designed architectures limit their scalability and functionality.\\n10https://azure.microsoft.com/en-us/products/ai-services/openai-service\\n13'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='- Plan Observer” framework, while AgentVerse formulates a pipeline of “Expert Recruitment -\\nCollaborative Decision Making - Action Execution - Evaluation” to build agents. We argue that these\\nhuman-designed architectures limit their scalability and functionality.\\n10https://azure.microsoft.com/en-us/products/ai-services/openai-service\\n13\\nTable 5: Comparison of EVOAGENT\\nwith human-designed agent frameworks\\nin Logic Grid Puzzle, Trivia Creative\\nWriting tasks.\\nFramework Logic Writing\\nEVOAGENT\\n77.00\\n84.40\\nAgentVerse\\n66.50\\n-\\nAutoAgents\\n-\\n82.00\\nIn the original papers, AgentVerse [25] and AutoA-\\ngent [31] also conduct experiments on the Logic Grid\\nPuzzle task and Trivia Creative Writing task, respectively.\\nWe follow their experimental settings and compared them\\nwith our method. As demonstrated in Table 5, EVOAGENT\\noutperforms both AgentVerse and AutoAgent, highlight-\\ning the effectiveness and generality of EVOAGENT.\\nC\\nExperimental Details of ScienceWorld\\nFollowing [48], we adopt the REACT [51] method for\\neach LLM, which introduces a virtual ’think’ action. This\\naction allows LLMs to generate subgoals during the action\\nplanning process.\\nD\\nEvaluation Details of TravelPlanner\\nGrounding to travel planning, a real-world use-case that inherently involves various constraints like\\nuser preferences and commonsense rules, TravelPlanner evaluates whether agents can formulate\\nflexible travel plans using gathered information to meet these constraints. We test EVOAGENT and all\\nbaselines on the TravelPlanner validation set, which consists of 180 user queries with the collected\\ninformation. To evaluate the travel plans generated by agents, TravelPlanner adopts the following\\nevaluation metrics:\\n• Delivery Rate: Assesses if agents can complete a plan within a limited number of steps (30 in our\\nexperimental setting). Failures are due to dead loops, numerous failed attempts, or exceeding the\\nstep limit.\\n• Commonsense Constraint Pass Rate: Evaluates if an agent can incorporate commonsense into their\\nplan.\\n• Hard Constraint Pass Rate: Measures if a plan meets all explicit hard constraints in the query,\\ntesting the agent’s ability to adapt to diverse user preferences.\\n• Final Pass Rate: Indicates the proportion of viable plans that meet all criteria, reflecting the agent’s\\nproficiency in creating practical plans.\\nFurthermore, TravelPlanner uses micro and macro strategies to assess the Commonsense and Hard\\nConstraint Pass Rates. The micro strategy calculates the ratio of met constraints to the total. The\\nmacro strategy measures the proportion of plans that meet all commonsense or hard constraints.\\nTogether, these strategies assess an agent’s ability to satisfy individual constraints and all constraints\\ncomprehensively.\\nE\\nMore Analysis of Ablation Studies\\n0\\n1\\n3\\n5\\n7\\nIteration\\n56\\n58\\n60\\n62\\nAnswer Ratio (%)\\nFigure 4: The performance of\\nGPT-3.5 with EVOAGENT under\\ndifferent iterations on Trivia Cre-\\native Writing task.\\nThe complete results of ablation studies on TravelPlanner are\\nshown in Table 6. This result indicates that the absence of the\\nquality-check module significantly lowers the delivery pass rate\\nwhen the All-in strategy is applied. To explore the reasons, we\\nrevisit the results and discover that sometimes unsuitable agents\\ncreate overly lengthy travel plans that fail to meet the criteria.\\nFor example, the model might erroneously assign a nutritionist to\\ndevise travel plans, resulting in excessively detailed meal arrange-\\nments and nutritional breakdowns. Therefore, the input length\\nsurpasses the context window of LLMs, preventing the final result\\ngeneration.\\nMoreover, we also conduct experiments on the Trivia Creative\\nWriting task to investigate the impact of the number of iterations\\non model performance in traditional NLP tasks. As shown in\\n14\\nTable 6: Comparison of different popularity selection strategies for LLMs on TravelPlanner. The best\\nresults are bolded, and the second best ones are underlined.\\nModel\\nStrategy Method\\nw/o Quality Check\\nw/ Quality Check\\nDelivery Com. Hard Delivery Com. Hard\\nGPT-3.5\\nDirect\\n-\\n-\\n-\\n100.0\\n57.3\\n11.0\\nSuggest3\\n-\\n-\\n-\\n100.0\\n57.5\\n5.7\\nOvergen3\\n-\\n-\\n-'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='results are bolded, and the second best ones are underlined.\\nModel\\nStrategy Method\\nw/o Quality Check\\nw/ Quality Check\\nDelivery Com. Hard Delivery Com. Hard\\nGPT-3.5\\nDirect\\n-\\n-\\n-\\n100.0\\n57.3\\n11.0\\nSuggest3\\n-\\n-\\n-\\n100.0\\n57.5\\n5.7\\nOvergen3\\n-\\n-\\n-\\n98.3\\n56.3\\n9.0\\nPromptRefine3\\n-\\n-\\n-\\n100.0\\n61.2\\n11.0\\nEVOAGENT(1,3)\\n100.0\\n64.2\\n11.0\\n100.0\\n64.2\\n11.0\\nRandom\\nEVOAGENT(2,3)\\n100.0\\n59.4\\n10.2\\n100.0\\n65.4\\n13.8\\nEVOAGENT(3,3)\\n98.9\\n59.2\\n11.4\\n100.0\\n65.8\\n14.0\\nPK\\nEVOAGENT(2,3)\\n99.4\\n59.4\\n7.1\\n100.0\\n66.0\\n11.7\\nEVOAGENT(3,3)\\n98.9\\n58.5\\n11.2\\n100.0\\n61.3\\n12.4\\nAll-in\\nEVOAGENT(2,3)\\n97.2\\n59.4\\n10.0\\n100.0\\n64.2\\n15.5\\nEVOAGENT(3,3)\\n93.3\\n56.0\\n8.3\\n100.0\\n65.2\\n12.6\\nGemini-Pro\\nDirect\\n-\\n-\\n-\\n90.0\\n61.7\\n16.4\\nSuggest3\\n-\\n-\\n-\\n100.0\\n65.8\\n11.0\\nOvergen3\\n-\\n-\\n-\\n100.0\\n66.5\\n12.4\\nPromptRefine3\\n-\\n-\\n-\\n96.7\\n64.9\\n16.7\\nEVOAGENT(1,3)\\n100.0\\n73.5\\n16.9\\n100.0\\n73.5\\n16.9\\nRandom\\nEVOAGENT(2,3)\\n96.7\\n65.9\\n13.1\\n99.4\\n67.3\\n14.0\\nEVOAGENT(3,3)\\n97.2\\n67.0\\n16.0\\n100.0\\n70.0\\n18.1\\nPK\\nEVOAGENT(2,3)\\n97.2\\n67.4\\n19.0\\n99.4\\n69.8\\n17.1\\nEVOAGENT(3,3)\\n97.2\\n68.5\\n17.1\\n99.4\\n68.4\\n16.7\\nAll-in\\nEVOAGENT(2,3)\\n95.0\\n65.1\\n16.7\\n99.4\\n69.0\\n19.0\\nEVOAGENT(3,3)\\n95.0\\n66.9\\n17.9\\n100.0\\n70.1\\n20.7\\nFigure 4, model performance improves with increasing iterations.\\nHowever, the improvement plateaus when the iteration count exceeds three. We suggest that traditional\\nNLP tasks are relatively simple, and beyond a certain iteration number, even with a quality-check\\nmodule in place, the generated agents tend to be similar and thus converge.\\nF\\nExamples of EVOAGENT\\nF.1\\nEVOAGENT Examples of NLP reasoning and knowledge tasks\\nListing 5, 6 and7 presents some multi-agent generation examples generated by GPT-4 based EVOA-\\nGENT in Logic Grid Puzzle, Trivia Creative Writing and Codenames Collaborative for a better\\nunderstanding.\\nF.2\\nEVOAGENT Examples of MMMU\\nListing 8 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nMMMU dataset for a better understanding.\\nF.3\\nEVOAGENT Examples of ScienceWorld\\nListing 9 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nScienceWorld for a better understanding.\\nF.4\\nEVOAGENT Examples of TravelPlanner\\nListing 10 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nTravelPlanner for a better understanding.\\n15'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='F.3\\nEVOAGENT Examples of ScienceWorld\\nListing 9 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nScienceWorld for a better understanding.\\nF.4\\nEVOAGENT Examples of TravelPlanner\\nListing 10 presents some multi-agent generation examples generated by GPT-4 based EVOAGENT in\\nTravelPlanner for a better understanding.\\n15\\nCamel Framework\\nAI Assistant\\nAI User\\nAI User\\nAI User\\nEvoAgent\\nAutoGen Framework\\nAssistant Agent\\nUserProxy \\nAgent\\nGeopolitical Analyst \\nAgent\\nLabor Economist \\nAgent\\nEvoAgent\\nPublic Health \\nAgent\\nGroup Chat Manager\\nTopic: The U.S. should commit more in climate change fighting\\nLabor Economist \\nAgent\\nGeopolitical Analyst \\nAgent\\nFigure 5: The adaption of EVOAGENT on Camel and AutoGen frameworks.\\nG\\nExamples of EVOAGENT’s Adaption to Multi-agent Collaboration\\nFrameworks\\nG.1\\nEVOAGENT for MetaGPT\\nMetaGPT [10] is a meta-programming framework that enhances LLM-based multi-agent collabo-\\nrations by integrating efficient human workflows. It employs an assembly line approach to assign\\ndiverse roles to agents, effectively simplifying complex tasks into manageable subtasks that multi-\\nple agents can execute collaboratively. As shown in Figure 3, instead of manually creating roles,\\nEVOAGENT can be used to automatically generate specialized agents for effective collaboration.\\nG.2\\nEVOAGENT for Camel\\nCamel [13] is recognized for its framework that supports communicative role-playing agents. Initially,\\nhumans establish this framework by conceptualizing an idea and designing specific roles, such as the\\nAI assistant role and the AI user role. These roles are then assigned to the assistant and user agents,\\nrespectively, enabling them to fulfill the task. As illustrated in Figure 5, EVOAGENT can be utilized\\nto automatically produce agents from AI assistants for interaction with AI users, bypassing the need\\nfor manual role design.\\nG.3\\nEVOAGENT for AutoGen\\nAutoGen [12] offers a framework that enables the creation of customizable and conversable agents\\nby integrating various LLMs. Initially, humans configure the assistant agents along with a user proxy\\nagent. Then, a group chat manager is responsible for selecting a speaker, gathering responses, and\\ndisseminating the message. As depicted in Figure 5, EVOAGENT facilitates the creation of multiple\\nexpert roles from a single assistant agent, thereby increasing the agent number in group chats without\\nthe need for manual design.\\n16\\nListing 1: Instruction templates for for 0-shot learning (Direct), Chain-of-thought (CoT) prompting\\nand Self-Refine method\\nDirect Method:\\n{question}\\nAnswer:\\nCoT Method:\\n{question}\\nYou need to give reasons first and then give the answer.\\nAnswer:\\nSelf-Refine Method:\\nStep One: Feedback Generation:\\nYou are a helpful assistant that provides feedback on {task}\\n{question}\\nThis is the answer from a student: {answer}.\\nPlease do not refine the answer but give some insightful suggestions for\\nthe student to help him better answer the question.\\nSuggestion:\\nStep Two: Result Refine:\\n{question}\\nThis is your answer:\\n{answer}\\nThere is the suggestion from an assistant:\\nSuggestion: {feedback}\\nNow you can refine your answer with his suggestion to better answer the\\nquestion.\\nKeep in mind that his suggestion may not be correct, so critically decide\\nwhether to accept his response or stick with your original one.\\nYou need to give reasons first and then give the answer.\\nRevised Answer:\\n17\\nListing 2: Instruction templates for SPP method\\nWhen faced with a task, begin by identifying the participants who will\\ncontribute to solving the task. Then, initiate a multi-round\\ncollaboration process until a final solution is reached. The participants\\nwill give critical comments and detailed suggestions whenever necessary.\\nHere are some examples:\\n---\\nExample Task 1: Use numbers and basic arithmetic operations (+ - * /) to\\nobtain 24. You need to use all numbers, and each number can only be used\\nonce.\\nInput: 6 12 1 1\\nParticipants: AI Assistant (you); Math Expert\\nStart collaboration!\\nMath Expert: Let’s analyze the task in detail. You need to make sure that\\nyou meet the requirement, that you need to use exactly the four numbers'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='---\\nExample Task 1: Use numbers and basic arithmetic operations (+ - * /) to\\nobtain 24. You need to use all numbers, and each number can only be used\\nonce.\\nInput: 6 12 1 1\\nParticipants: AI Assistant (you); Math Expert\\nStart collaboration!\\nMath Expert: Let’s analyze the task in detail. You need to make sure that\\nyou meet the requirement, that you need to use exactly the four numbers\\n(6 12 1 1) to construct 24. To reach 24, you can think of the common\\ndivisors of 24 such as 4, 6, 8, 3 and try to construct these first. Also\\nyou need to think of potential additions that can reach 24, such as 12 +\\n12.\\nAI Assistant (you): Thanks for the hints! Here’s one initial solution:\\n(12 / (1 + 1)) * 6 = 24\\nMath Expert: Let’s check the answer step by step. (1+1) = 2, (12 / 2) =\\n6, 6 * 6 = 36 which is not 24! The answer is not correct. Can you fix\\nthis by considering other combinations? Please do not make similar\\nmistakes.\\nAI Assistant (you): Thanks for pointing out the mistake. Here is a\\nrevised solution considering 24 can also be reached by 3 * 8: (6 + 1 + 1)\\n* (12 / 4) = 24.\\nMath Expert: Let’s first check if the calculation is correct. (6 + 1 + 1)\\n= 8, 12 / 4 = 3, 8 * 3 = 24. The calculation is correct, but you used 6\\n1 1 12 4 which is not the same as the input 6 12 1 1. Can you avoid using\\na number that is not part of the input?\\nAI Assistant (you): You are right, here is a revised solution considering\\n24 can be reached by 12 + 12 and without using any additional numbers: 6\\n* (1 - 1) + 12 = 24.\\nMath Expert: Let’s check the answer again. 1 - 1 = 0, 6 * 0 = 0, 0 + 12 =\\n12. I believe you are very close, here is a hint: try to change the \"1 -\\n1\" to \"1 + 1\".\\nAI Assistant (you): Sure, here is the corrected answer:\\n6 * (1+1) + 12 =\\n24\\nMath Expert: Let’s verify the solution. 1 + 1 = 2, 6 * 2 = 12, 12 + 12 =\\n12. You used 1 1 6 12 which is identical to the input 6 12 1 1.\\nEverything looks good!\\nFinish collaboration!\\nFinal answer: 6 * (1 + 1) + 12 = 24\\n---\\n{question}\\n18\\nListing 3: Instruction templates for EVOAGENT\\nCrossover and Mutation:\\n{question}\\nThis is your result:\\n{answer}\\nNow, you can create and collaborate with multiple experts to improve your\\nresult. Therefore, please describe in as much detail as possible the\\ndifferent skills and focuses you need from multiple experts individually.\\nWe will provide each expert with the same information and query. However\\n, please note that each profession has its own specialization, so you can\\nassign each expert to just one sub-task to ensure a more refined\\nresponse. We will relay their responses to you in turn, allowing you to\\nreorganize them into a better answer. Please note that the description\\nshould be narrated in the second person, for example: You are a XXX.\\nThese are the descriptions of the experts you have created before for\\nthis task:\\n{description}\\nTherefore, please remember you should not repeatedly create the same\\nexperts as described above.\\nNow, you can give the description for a new expert (Please note that only\\nbe one, do not give multiple at one time):\\nQuality Check:\\n{question}\\nWe employ mulitple experts to answer this query. The following is a\\nsecond-person introduction to the experts we have hired:\\n{description_ls}\\nNow, we will hire a new expert to help better respond to user query. Here\\nis a second person description of the new expert: {description}\\nPlease evaluate the new expert based on the following criteria to decide\\nwhether they should be retained or not:\\n1. The new expert is distinct and does not duplicate any previously hired\\nexperts.\\n2. Based on the new expert’s description, determine if they can\\neffectively assist in answering users’ questions.\\nGive the reason first and then give the choice. If retaining, please'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='is a second person description of the new expert: {description}\\nPlease evaluate the new expert based on the following criteria to decide\\nwhether they should be retained or not:\\n1. The new expert is distinct and does not duplicate any previously hired\\nexperts.\\n2. Based on the new expert’s description, determine if they can\\neffectively assist in answering users’ questions.\\nGive the reason first and then give the choice. If retaining, please\\nreply with: Retain. If discarding, please reply with: Discard.\\nResult Update:\\n{question}\\nThis is your result:\\n{old_answer}\\nYou invite an expert whose description is: {description}\\nThis expert also give his answer based on his own professional knowledge:\\n{new_answer}.\\nNow you can refine your result with his answer to better answer the\\nquestion.\\nKeep in mind that his answer may not be correct, so critically decide\\nwhether to accept his response or stick with your original one.\\nRevised Answer:\\n19\\nListing 4: Instruction templates of different ablation settings in EVOAGENT.\\nPK:\\n{question}\\nWe invite {n} experts. They give the results based on their own\\nprofessional knowledge:\\nHere are second-person descriptions of these experts with their answers:\\n{select}\\nNow you can should help us select the best result which can meet the\\nquery.\\nYou need to give reasons first and then give the answer with the format:\\n\"Final Answer: Expert #XX\"\\nAll-in:\\n{question}\\nThis is your answer: {old_answer}.\\nFurthermore, you also invite {n} experts. They also give answers based on\\ntheir own professional knowledge:\\nHere are second person descriptions of these experts with their answers:\\n{description_ls}\\nNow you can refine your answer with these answers to better meet the\\nquery.\\nSuggest:\\n{Expert_Agent_description}\\n{question}\\nThis is the result from an AI assistant: {answer}.\\nPlease do not refine the plan but give some insightful suggestions for\\nthe AI assistant to help it better meet the user’s query.\\nSuggestion:\\nOverGen:\\n{question}\\nPlease generate three different results at one time for user to choose\\nfrom. The format can be:\\nResult #1:\\nResult #2:\\nResult #3:\\nThree Different Candidate Results:\\nPromptRefine:\\n{question}\\nThis is the result from an AI assistant, which description is\\n\"{original_description}\":\\n{answer}.\\nPlease do not refine the result but refine the description of the AI\\nassistant to help it better answer the user’s query.\\nPlease note that the description should be narrated in the second person,\\nfor example: You are a XXX.\\nDescription:\\n20\\nListing 5: An example of multi-agent generation from EVOAGENT for Logic Grid Puzzle\\nQuestion: There are 5 houses in a row, numbered 1 on the left to 5 on the\\nright. There is one person living in each house. The people in these\\nhouses have different characteristics:\\n- Each person has a different kind of car: one owns an SUV, one owns a\\nsedan, one drives a convertible, one drives a minivan, and one drives a\\ntruck\\n- Each person has a favorite color: one likes red, one likes yellow, one\\nlikes green, one likes white, and one likes blue\\n- Each person is wearing different shoes: one is wearing running shoes,\\none is wearing boots, one is wearing high heels, one is wearing sandals,\\nand one is wearing loafers\\nClue(s):\\n1. There is one house between where the person who is wearing loafers\\nlives and where the person who drives a minivan lives. 2. The person who\\ndrives a truck does not live in the fourth house. 3. The person who\\ndrives a truck lives directly left of the person who is wearing sandals.\\n4. The person who likes red lives somewhere to the right of the person\\nwho likes yellow. 5. The person who is wearing loafers lives somewhere to\\nthe left of the person who drives a truck. 6. The person who likes blue\\nlives in the fifth house. 7. There are two houses between where the\\nperson who owns a sedan lives and where the person who is wearing running\\nshoes lives. 8. The person who is wearing boots lives somewhere to the\\nleft of the person who drives a truck. 9. The person who likes white\\nlives in the fourth house. 10. There is one house between where the'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='lives in the fifth house. 7. There are two houses between where the\\nperson who owns a sedan lives and where the person who is wearing running\\nshoes lives. 8. The person who is wearing boots lives somewhere to the\\nleft of the person who drives a truck. 9. The person who likes white\\nlives in the fourth house. 10. There is one house between where the\\nperson who likes green lives and where the person who drives a truck\\nlives. 11. The person who owns an SUV does not live in the first house.\\nWhat is the number of the house where the person who is wearing boots\\nlives? choice: 2, choice: 5, choice: 3, choice: 1, choice: 4.\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a logician, specializing in logical reasoning and problem-solving\\n. Your task is to analyze the given clues and use deductive reasoning to\\ndetermine the correct sequence of houses, their occupants, and their\\ncharacteristics. You will need to consider each clue carefully,\\nunderstanding how it relates to the others and how it narrows down the\\npossibilities. Your focus should be on ensuring the logical consistency\\nof the solution, making sure that all clues are satisfied without\\ncontradiction.\\nReason: From clue 6 and 9, we know that the person who likes blue lives\\nin the fifth house and the person who likes white lives in the fourth\\nhouse. From clue 4, the person who likes red lives somewhere to the right\\nof the person who likes yellow. Since the fourth and fifth houses are\\noccupied by the people who like white and blue, the person who likes red\\nmust live in the third house and the person who likes yellow must live in\\nthe first or second house.From clue 10, there is one house between where\\nthe person who likes green lives and where the person who drives a truck\\nlives. Since the third, fourth, and fifth houses are occupied by the\\npeople who like red, white, and blue, the person who likes green must\\nlive in the first or second house. Therefore, the person who likes yellow\\nmust live in the first house and the person who likes green must live in\\nthe second house.From clue 11, the person who owns an SUV does not live\\nin the first house. So, the person who owns an SUV must live in the\\nsecond, third, or fourth house.From clue 2, the person who drives a truck\\ndoes not live in the fourth house. So, the person who drives a truck\\nmust live in the second or third house.From clue 3, the person who drives\\na truck lives directly left of the person who is wearing sandals. So,\\nthe person who drives a truck must live in the second house and the\\nperson who is wearing sandals must live in the third house.From clue 8,\\nthe person who is wearing boots lives somewhere to the left of the person\\nwho drives a truck. So, the person who is wearing boots must live in the\\nfirst house.\\nAnswer: choice: 1\\n21\\nListing 6: An example of multi-agent generation from EVOAGENT for Trivia Creative Writing\\nWrite a short and coherent story about Zootopia that incorporates the\\nanswers to the following 5 questions:\\n1. Following success at the 1979 election whose party conference speeech\\nincluded the lines ’you turn if you want to, the lady’s not for turning’?\\n2. Which snooker player was simply known as ’The Grinder’?\\n3. Which former West Indian fast bowler was known as ’Whispering Death’?\\n4. Which Eastenders character had a dog called Willy?\\n5. In which year did the first episode of Minder take place?\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a sports historian with a deep understanding of snooker and\\ncricket from the 1970s and 1980s. You are familiar with the careers and\\nplaying styles of Cliff Thorburn, known as ’The Grinder’, and Michael\\nHolding, known as ’Whispering Death’. Your role in this task will be to\\nprovide context and insight into these sports figures and their\\nsignificance in the story about Zootopia.\\nExpert Agent #2 Description:\\nYou are a political historian with a deep understanding of the political\\nclimate and key figures of the 1970s and 1980s, particularly in the\\nUnited Kingdom. You have extensive knowledge about Margaret Thatcher, her\\npolicies, and her impact on society. Your role in this task will be to'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='provide context and insight into these sports figures and their\\nsignificance in the story about Zootopia.\\nExpert Agent #2 Description:\\nYou are a political historian with a deep understanding of the political\\nclimate and key figures of the 1970s and 1980s, particularly in the\\nUnited Kingdom. You have extensive knowledge about Margaret Thatcher, her\\npolicies, and her impact on society. Your role in this task will be to\\nprovide context and insight into the significance of the reference to\\nThatcher’s famous speech in the story about Zootopia.\\nStory: In the vibrant city of Zootopia, the year was 1979, a time of\\nsignificant change and excitement. The city had just elected its first\\nfemale mayor, a lioness named Margaret Thatcher, known for her unwavering\\ndetermination and strong leadership. Her victory speech at the party\\nconference echoed the famous line from the human world’s Margaret\\nThatcher, \"You turn if you want to, the lady’s not for turning.\" This\\nphrase resonated with the citizens of Zootopia, who admired her\\nsteadfastness and determination. In the heart of the city, a meticulous\\nbadger known as ’The Grinder’ ran a popular snooker club. His nickname\\nwas a tribute to the legendary human snooker player Cliff Thorburn, known\\nfor his relentless and meticulous style of play. The Grinder’s snooker\\nclub was a hub of social activity, where animals from all walks of life\\ncame to unwind and engage in friendly competition. Meanwhile, a new\\nrecruit had joined the Zootopia Police Department, a cheetah named\\nMichael Holding. Known as ’Whispering Death’ during his cricketing days,\\na moniker borrowed from the former West Indian fast bowler, he was now a\\nrespected figure in the community, using his speed and stealth to uphold\\nthe law. In another part of the city, a popular soap opera was being\\nfilmed. The show was called \"Eastenders of Zootopia,\" and one of the\\nbeloved characters was a rabbit named Ethel Skinner, who had a dog named\\nWilly. This was a charming reference to the human Eastenders character of\\nthe same name, symbolizing the enduring friendships that were a\\ncornerstone of Zootopian society. The year 1979 was also significant as\\nit marked the airing of the first episode of the Zootopian version of the\\nhuman show \"Minder\". The show was a reflection of the city’s vibrant and\\ndiverse culture, much like the original British series. It was a\\ntestament to the city’s ability to adapt and evolve, while still holding\\nonto its rich history and traditions. In conclusion, Zootopia was a\\nmelting pot of cultures and histories, much like the television shows and\\ncharacters it referenced. These references served as a bridge between\\nthe past and the present, reminding the citizens of their roots while\\nencouraging them to embrace the future.\\nEvaluation: correct_count=5/5\\n22\\nListing 7: An example of multi-agent generation from EVOAGENT for Codenames Collaborative\\nTry to identify the 3 words best associated with the word \"fruit\" from\\nthe following word list:\\n[\"bowl\", \"einstein\", \"banana\", \"brain\", \"judge\", \"groom\", \"crusader\", \"\\nstamp\", \"peach\", \"halloween\", \"troll\", \"slipper\", \"earthquake\", \"tip\", \"\\nsun\", \"quarter\", \"pad\", \"nude\", \"helmet\", \"laundry\", \"wedding\", \"mirror\",\\n\"sleep\", \"minotaur\", \"wish\"].\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a cultural anthropologist with a focus on symbolism and rituals.\\nYour expertise in understanding how different objects, including fruits,\\nare used and symbolized in various cultures will be beneficial in this\\ntask. You can provide insights into how certain words might be associated\\nwith \"fruit\" from a cultural or symbolic perspective. Your focus will be\\non the cultural and symbolic aspects of the task.\\nReason:\\n1. ’Banana’: This is a type of fruit, rich in potassium and dietary fiber\\n. It is commonly consumed raw and is used in a variety of dishes like\\ndesserts, smoothies, and baked goods.\\n2. ’Peach’: This is also a type of fruit, known for its sweet and juicy\\nnature. Peaches are rich in vitamins A and C and are often eaten raw or'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='Reason:\\n1. ’Banana’: This is a type of fruit, rich in potassium and dietary fiber\\n. It is commonly consumed raw and is used in a variety of dishes like\\ndesserts, smoothies, and baked goods.\\n2. ’Peach’: This is also a type of fruit, known for its sweet and juicy\\nnature. Peaches are rich in vitamins A and C and are often eaten raw or\\nused in dishes like pies, jams, and salads.\\n3. ’Bowl’: This word is associated with fruit in the context of a fruit\\nbowl, which is a common way to store and display various types of fruit.\\nA bowl of fruit can also be a healthy snack or dessert option.\\nAnswer: Banana, Peach, Bowl\\nExpert Agent #2 Description:\\nYou are a nutritionist with a deep understanding of different types of\\nfood, including fruits. Your knowledge about the nutritional value,\\ncommon uses, and cultural significance of various fruits will be helpful\\nin this task. You can provide insights into how certain words might be\\nassociated with \"fruit\" from a dietary or nutritional perspective. Your\\nfocus will be on the nutritional and dietary aspects of the task.\\nReason:\\n1. ’Banana’: This is a direct association as a banana is a type of fruit.\\nIn many cultures, bananas symbolize fertility and prosperity. In\\nHinduism, for example, bananas are considered a sacred fruit and are used\\nin religious rituals.\\n2. ’Peach’: Similar to banana, a peach is also a type of fruit. In\\nChinese culture, peaches are symbols of immortality and unity. They are\\noften used in traditional rituals and ceremonies.\\n3. ’Bowl’: A bowl is often used to hold fruits. In many cultures, a bowl\\nof fruit is a symbol of abundance, fertility, and prosperity. It is often\\nused in rituals and ceremonies as an offering or a centerpiece.\\nAnswer: banana, peach, bowl\\nFinal Answer:\\nAfter considering the expert’s input, I find that their reasoning aligns\\nwith my initial thoughts. The words ’banana’ and ’peach’ are indeed types\\nof fruit, and their nutritional value and common uses further strengthen\\ntheir association with the word ’fruit’. The word ’bowl’ is also\\nassociated with fruit in the context of a fruit bowl, which is a common\\nway to store and display various types of fruit. Therefore, I agree with\\nthe expert’s choices.\\nFinal Answer: banana, peach, bowl.\\n23\\nListing 8: An example of multi-agent generation from EVOAGENT for MMMU dataset.\\nQuestion: <image 1> The elliptical orbit of a planet around the Sun is\\nshown on the diagram. Which of the following statements is true?\\nA. the eccentricity of the orbit is less than zero\\nB. the eccentricity of the orbit is greater than 1\\nC. the sun might be at point C\\nD. the sun might be at point D\\nE. the sun might be at point B (Correct)\\nCoT:\\nReason: In an elliptical orbit, the Sun is located at one of the two foci\\nof the ellipse. The eccentricity of an orbit is a measure of how much\\nthe orbit deviates from a perfect circle, with 0 being a perfect circle\\nand 1 being a parabola. Since the orbit shown is an ellipse, the\\neccentricity must be between 0 and 1. Points A and E are the farthest\\npoints from the center of the ellipse, so the Sun cannot be located at\\neither of those points. Point B is at the center of the ellipse, so the\\nSun cannot be located there either. This leaves points C and D as\\npossible locations for the Sun. However, since the Sun must be at one of\\nthe foci and point D is closer to the center of the ellipse than point C,\\nit is more likely that the Sun is located at point C.\\nAnswer: C. the sun might be at point C\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a mathematician with a specialization in geometry, particularly\\nin the study of conic sections which include ellipses. Your expertise\\nallows you to understand the properties and definitions of ellipses,\\nincluding the concept of foci. You can provide a mathematical perspective\\non why the Sun must be located at one of the foci of the elliptical\\norbit and not at any other point. Your knowledge will help explain the'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='Expert Agent #1 Description:\\nYou are a mathematician with a specialization in geometry, particularly\\nin the study of conic sections which include ellipses. Your expertise\\nallows you to understand the properties and definitions of ellipses,\\nincluding the concept of foci. You can provide a mathematical perspective\\non why the Sun must be located at one of the foci of the elliptical\\norbit and not at any other point. Your knowledge will help explain the\\ngeometric reasons behind the positioning of the Sun in relation to the\\nplanet’s orbit, and why the given options in the statements do not align\\nwith the mathematical definition of an ellipse.\\nReason: An ellipse is defined as the set of all points for which the sum\\nof the distances to two fixed points, called foci, is constant. The Sun,\\nbeing one of the massive bodies in the solar system, must be located at\\none of the foci of the elliptical orbit of a planet due to the\\ngravitational forces that govern the motion of the planets. Option E is\\ncorrect because the Sun might be at point B, which is one of the foci of\\nthe ellipse. The other focus would be at a point symmetrically opposite\\nto B along the major axis.\\nAnswer: E. the sun might be at point B.\\nExpert Agent #2 Description:\\nYou are an astronomer with a specialization in observational astronomy.\\nYour expertise lies in the practical observation and data collection of\\ncelestial bodies, including the use of telescopes and other instruments\\nto study the positions and movements of planets and stars. You have a\\ndeep understanding of how the apparent motion of celestial bodies can be\\nused to infer their actual motion in space. You can provide insight into\\nhow observations of planetary orbits, such as the one depicted in the\\ndiagram, have historically been used to deduce the laws of planetary\\nmotion and the positions of stars like the Sun. Your knowledge will help\\nexplain how practical observations support the theoretical understanding\\nthat the Sun must be located at one of the foci of the elliptical orbit.\\nReason: The eccentricity of an orbit is a measure of how much it deviates\\nfrom being a perfect circle. It ranges from 0 (a perfect circle) to 1 (a\\nparabola). An eccentricity less than zero or greater than 1 is not\\npossible for an elliptical orbit. Therefore, options A and B are\\nincorrect. According to Kepler’s laws of planetary motion, the Sun must\\nbe located at one of the foci of the elliptical orbit. Point B is located\\nat a position that could be one of the foci of the ellipse.\\nAnswer: E. the sun might be at point B.\\n24\\nListing 9: An example of multi-agent generation from EVOAGENT for ScienceWorld.\\nTask 1: Your task is to determine if a metal fork is electrically\\nconductive. The metal fork is located around the kitchen. First, focus on\\nthe metal fork. If conductive, place it in the blue box. If\\nnonconductive, place it in the green box.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a thermodynamics expert. Your expertise in heat transfer and\\ntemperature measurement is crucial for this task. You understand how to\\nproperly use a thermometer and interpret its readings. You also have\\nknowledge about the properties of various substances, including their\\ntypical temperature ranges. Your task is to guide the assistant in using\\nthe thermometer to accurately measure the temperature of the unknown\\nsubstance B.\\nTask 2: Your task is to boil gallium. For compounds without a boiling\\npoint, combusting the substance is also acceptable. First, focus on the\\nsubstance. Then, take actions that will cause it to change its state of\\nmatter.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a metallurgist with a deep understanding of the properties and\\nbehaviors of metals, including gallium. Your expertise includes knowledge\\nof the melting and boiling points of various metals, as well as the\\nprocesses and equipment needed to heat them to these temperatures. You\\nare also familiar with safety procedures and precautions when handling\\nand heating metals. Your task is to provide detailed instructions on how\\nto safely and effectively boil gallium in the foundry.\\nTask 3: Your task is to change the state of matter of gallium. First,\\nfocus on the substance. Then, take actions that will cause it to change\\nits state of matter.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a chemist with a specialization in the properties and behaviors'), Document(metadata={'Published': '2024-07-11', 'Title': 'EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms', 'Authors': 'Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang', 'Summary': 'The rise of powerful large language models (LLMs) has spurred a new trend in\\nbuilding LLM-based autonomous agents for solving complex tasks, especially\\nmulti-agent systems. Despite the remarkable progress, we notice that existing\\nworks are heavily dependent on human-designed frameworks, which greatly limits\\nthe functional scope and scalability of agent systems. How to automatically\\nextend the specialized agent to multi-agent systems to improve task-solving\\ncapability still remains a significant challenge. In this paper, we introduce\\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\\nframeworks as the initial individual and then apply a series of evolutionary\\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\\nagents with diverse agent settings. EvoAgent can be generalized to any\\nLLM-based agent framework, and can automatically extend the existing agent\\nframework to multi-agent systems without any extra human designs. Experimental\\nresults across various tasks have shown that EvoAgent can automatically\\ngenerate multiple expert agents and significantly enhance the task-solving\\ncapabilities of LLM-based agents.'}, page_content='and heating metals. Your task is to provide detailed instructions on how\\nto safely and effectively boil gallium in the foundry.\\nTask 3: Your task is to change the state of matter of gallium. First,\\nfocus on the substance. Then, take actions that will cause it to change\\nits state of matter.\\nEvoAgent:\\nExpert Agent Description:\\nYou are a chemist with a specialization in the properties and behaviors\\nof substances. You have a deep understanding of the state changes of\\nmatter, specifically how substances like gallium can transition from\\nsolid to liquid or gas. You are familiar with the conditions required for\\nthese changes to occur, such as temperature and pressure. You also have\\nknowledge of the equipment and procedures used in a foundry, which will\\nbe useful in this task. Your role is to provide guidance on how to\\nmanipulate the conditions in the foundry to achieve the desired state\\nchange in gallium.\\n25\\nListing 10: An example of multi-agent generation from EVOAGENT for TravelPlanner dataset.\\nQuestion: Can you help me with a travel plan departing from Buffalo to\\nAtlanta for a duration of 3 days, specifically from March 2nd to March 4\\nth, 2022? I plan to travel alone and my planned budget for the trip is\\naround $1,100.\\nEvoAgent:\\nExpert Agent #1 Description:\\nYou are a culinary expert with a deep understanding of food culture in\\nAtlanta. Your expertise includes knowledge of the best local restaurants,\\npopular dishes, and dietary restrictions. You can provide\\nrecommendations for breakfast, lunch, and dinner spots that offer a\\nunique taste of Atlanta’s food scene. Your suggestions should consider\\nthe traveler’s budget and the proximity of the restaurants to the\\nattractions and accommodation listed in the plan.\\nExpert Agent #2 Description:\\nYou are a travel logistics expert with extensive knowledge of\\ntransportation options and costs between Buffalo and Atlanta. Your\\nexpertise includes understanding flight schedules, costs, and durations,\\nas well as alternative transportation methods such as self-driving and\\ntaxi services. You can provide the most cost-effective and time-efficient\\ntransportation options for the traveler, considering their budget and\\nthe travel dates provided in the plan.\\nExpert Agent #3 Description:\\nYou are a local tourism expert with comprehensive knowledge of Atlanta’s\\nattractions. Your expertise includes understanding the city’s popular\\ntourist spots, historical sites, museums, parks, and unique local\\nexperiences. You can provide recommendations for attractions that the\\ntraveler can visit during their stay, considering their budget and the\\nproximity of these attractions to the traveler’s accommodation and meal\\nspots. Your suggestions should also take into account the operating hours\\nof these attractions to ensure they align with the traveler’s schedule.\\nAnswer:\\nDay 1:\\nCurrent City: from Buffalo to Atlanta\\nTransportation: Flight Number: F3502691, from Buffalo to Atlanta,\\nDeparture Time: 18:48, Arrival Time: 20:51\\nBreakfast: Not applicable as the flight is in the evening.\\nAttraction: Not applicable as the flight is in the evening.\\nLunch: Not applicable as the flight is in the evening.\\nDinner: Saffron, Atlanta\\nccommodation: Spacious private room close St. Barnabas Hospital, Atlanta\\nDay 2:\\nCurrent City: Atlanta\\nTransportation: -\\nBreakfast: Adda, Atlanta\\nAttraction: Atlanta Botanical Garden, Atlanta; World of Coca-Cola,\\nAtlanta.\\nLunch: Baba Au Rhum, Atlanta\\nDinner: Asian Bistro, Atlanta\\nAccommodation: Spacious private room close St. Barnabas Hospital, Atlanta\\nDay 3:\\nCurrent City: from Atlanta to Buffalo\\nTransportation: Flight Number: F3500648, from Atlanta to Buffalo,\\nDeparture Time: 21:24, Arrival Time: 23:26\\nBreakfast: Chef Style, Atlanta\\nAttraction: Georgia Aquarium, Atlanta; Martin Luther King, Jr. National\\nHistorical Park, Atlanta.\\nLunch: Pizza Central, Atlanta\\nDinner: Daawat-e-Kashmir, Atlanta\\nAccommodation: -.\\n26')]\n",
            "57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = doc_splits + img_docs + arxiv_docs_split\n",
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyWqUkkPIhJz",
        "outputId": "e084d123-0b37-42ac-e047-078e3d427065"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    collection_name=\"multimodal-rag\",\n",
        "    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "A6mlX-o_E1xg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retrieve_information\",\n",
        "    \"Search and return information about LLM Agents.\",\n",
        ")\n",
        "\n",
        "tools = [retriever_tool]"
      ],
      "metadata": {
        "id": "EPxoDC3lInBn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, Sequence, TypedDict\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    # The add_messages function defines how an update should be processed\n",
        "    # Default is to replace. add_messages says \"append\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]"
      ],
      "metadata": {
        "id": "N74PohukIsuX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "dn3wO4NmIuiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, Literal, Sequence, TypedDict\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langgraph.prebuilt import tools_condition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI0jMSnTIvBN",
        "outputId": "28754004-4a1e-4ce3-caa4-aa72a40c060a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Edges\n",
        "\n",
        "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (messages): The current state\n",
        "\n",
        "    Returns:\n",
        "        str: A decision for whether the documents are relevant or not\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK RELEVANCE---\")\n",
        "\n",
        "    # Data model\n",
        "    class grade(BaseModel):\n",
        "        \"\"\"Binary score for relevance check.\"\"\"\n",
        "\n",
        "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
        "\n",
        "    # LLM\n",
        "    model = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.0-flash-exp\", streaming=True)\n",
        "\n",
        "    # LLM with tool and validation\n",
        "    llm_with_tool = model.with_structured_output(grade)\n",
        "\n",
        "    # Prompt\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
        "        Here is the user question: {question} \\n\n",
        "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "    )\n",
        "\n",
        "    # Chain\n",
        "    chain = prompt | llm_with_tool\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    question = messages[0].content\n",
        "    docs = last_message.content\n",
        "\n",
        "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
        "\n",
        "    score = scored_result.binary_score\n",
        "\n",
        "    if score == \"yes\":\n",
        "        print(\"---DECISION: DOCS RELEVANT---\")\n",
        "        return \"generate\"\n",
        "\n",
        "    else:\n",
        "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
        "        print(score)\n",
        "        return \"rewrite\""
      ],
      "metadata": {
        "id": "j1FWil5mI3tg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Nodes\n",
        "\n",
        "\n",
        "def agent(state):\n",
        "    \"\"\"\n",
        "    Invokes the agent model to generate a response based on the current state. Given\n",
        "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
        "\n",
        "    Args:\n",
        "        state (messages): The current state\n",
        "\n",
        "    Returns:\n",
        "        dict: The updated state with the agent response appended to messages\n",
        "    \"\"\"\n",
        "    print(\"---CALL AGENT---\")\n",
        "    messages = state[\"messages\"]\n",
        "    model = ChatGoogleGenerativeAI(temperature=0, streaming=True, model=\"gemini-2.0-flash-exp\")\n",
        "    model = model.bind_tools(tools)\n",
        "    response = model.invoke(messages)\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def rewrite(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (messages): The current state\n",
        "\n",
        "    Returns:\n",
        "        dict: The updated state with re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "\n",
        "    msg = [\n",
        "        HumanMessage(\n",
        "            content=f\"\"\" \\n\n",
        "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
        "    Here is the initial question:\n",
        "    \\n ------- \\n\n",
        "    {question}\n",
        "    \\n ------- \\n\n",
        "    Formulate an improved question: \"\"\",\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Grader\n",
        "    model = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.0-flash-exp\", streaming=True)\n",
        "    response = model.invoke(msg)\n",
        "    return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "vz80-d0dJGXO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (messages): The current state\n",
        "\n",
        "    Returns:\n",
        "         dict: The updated state with re-phrased question\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    docs = last_message.content\n",
        "\n",
        "    # Prompt\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "    # LLM\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0, streaming=True)\n",
        "\n",
        "    # Post-processing\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Chain\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Run\n",
        "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "print(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\n",
        "prompt = hub.pull(\"rlm/rag-prompt\").pretty_print()  # Show what the prompt looks like"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvemS3DsJJjy",
        "outputId": "e5cce0b5-3047-43f6-fabc-deea09743ba7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************Prompt[rlm/rag-prompt]********************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
            "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
            "Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the nodes we will cycle between\n",
        "workflow.add_node(\"agent\", agent)  # agent\n",
        "retrieve = ToolNode([retriever_tool])\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
        "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
        "workflow.add_node(\n",
        "    \"generate\", generate\n",
        ")  # Generating a response after we know the documents are relevant\n",
        "# Call agent node to decide to retrieve or not\n",
        "workflow.add_edge(START, \"agent\")\n",
        "\n",
        "# Decide whether to retrieve\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    # Assess agent decision\n",
        "    tools_condition,\n",
        "    {\n",
        "        # Translate the condition outputs to nodes in our graph\n",
        "        \"tools\": \"retrieve\",\n",
        "        END: END,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Edges taken after the `action` node is called.\n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve\",\n",
        "    # Assess agent decision\n",
        "    grade_documents,\n",
        ")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "workflow.add_edge(\"rewrite\", \"agent\")\n",
        "\n",
        "# Compile\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "XR969ePTJVgc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "r78RF6x_JZmx",
        "outputId": "8f451ab6-9c21-4d07-85b6-3543af5c04f4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "inputs = {\n",
        "    \"messages\": [\n",
        "        (\"user\", \"What are the LLM Agents?\"),\n",
        "    ]\n",
        "}\n",
        "for output in graph.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        pprint.pprint(f\"Output from node '{key}':\")\n",
        "        pprint.pprint(\"---\")\n",
        "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIw6DWglJcWE",
        "outputId": "184cd9f4-5000-487b-a5e9-6f5a6fa04760"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---CALL AGENT---\n",
            "\"Output from node 'agent':\"\n",
            "'---'\n",
            "{ 'messages': [ AIMessage(content='', additional_kwargs={'function_call': {'name': 'retrieve_information', 'arguments': '{\"query\": \"What are the LLM Agents?\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-c1cb95b3-c5c5-4d51-984a-aa78128b861d-0', tool_calls=[{'name': 'retrieve_information', 'args': {'query': 'What are the LLM Agents?'}, 'id': '3439cadb-39a5-49f2-9ade-2f1f565dc521', 'type': 'tool_call'}], usage_metadata={'input_tokens': 54, 'output_tokens': 11, 'total_tokens': 65, 'input_token_details': {'cache_read': 0}})]}\n",
            "'\\n---\\n'\n",
            "---CHECK RELEVANCE---\n",
            "---DECISION: DOCS RELEVANT---\n",
            "\"Output from node 'retrieve':\"\n",
            "'---'\n",
            "{ 'messages': [ ToolMessage(content='The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nCertainly! Here\\'s a three-paragraph summary of the image, focusing on the details provided:\\n\\nThe image depicts the structure of a typical Large Language Model (LLM) agent using a flowchart-like diagram. At the center lies the \"LLM Agent,\" represented by a bright orange rectangle and connected by red lines to other components. These components are categorized as either mandatory (yellow) or optional (blue). The prompt recipe, consisting of \"Instructions\" (mandatory) and \"Persona\" (optional), guides the agent\\'s task execution and output processing. Additionally, the agent interacts with various other elements, including an \"Interface\" (mandatory), \"Tools\" (optional), \"Knowledge\" (optional), and \"Memory\" (optional). The components and their connections illustrate how different inputs and functionalities are integrated into the overall operation of the LLM agent.\\n\\nThe image also emphasizes the agent\\'s need for interaction and the capacity for memory and specialized knowledge. The text states that an agent typically interfaces with a human, another agent, or an API. This underscores the agent\\'s role as an interactive tool within a broader ecosystem. The diagram further highlights that the agent can generate \"memories,\" suggesting the capability to learn and recall past interactions or data. The agent also has access to specific domain knowledge and tools, indicating that it can be tailored to perform particular tasks or operate within specific contexts. These features showcase the potential of LLM agents to be dynamic, adaptable, and context-aware.\\n\\nLastly, the image includes a legend that clearly defines the meaning of the colors used in the diagram. Yellow represents a \"Mandatory Component,\" while blue signifies an \"Optional Component.\" This distinction is crucial for understanding the essential elements required for the LLM agent to function effectively, versus those that can be added to enhance its capabilities. The inclusion of the \"PromptEngineering.org\" logo at the bottom suggests the source of this model and its relevance to the field of prompt engineering, further solidifying its informative nature. The diagram effectively portrays a modular approach to building LLM agents, allowing for flexible and customizable designs.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.', name='retrieve_information', id='9b56f862-4567-4827-967e-0049d61bc49d', tool_call_id='3439cadb-39a5-49f2-9ade-2f1f565dc521')]}\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Output from node 'generate':\"\n",
            "'---'\n",
            "{ 'messages': [ 'LLM Agents are systems that use large language models to '\n",
            "                'interact with tools and perform tasks. They are provided with '\n",
            "                'tool names, descriptions, and input/output details. The '\n",
            "                'agents follow a specific format, such as ReAct, to reason and '\n",
            "                'take actions using the tools.']}\n",
            "'\\n---\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "inputs = {\n",
        "    \"messages\": [\n",
        "        (\"user\", \"What are the best practices for Memory in LLM Agents?\"),\n",
        "    ]\n",
        "}\n",
        "for output in graph.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        pprint.pprint(f\"Output from node '{key}':\")\n",
        "        pprint.pprint(\"---\")\n",
        "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gutTOH6uJgN-",
        "outputId": "b407aed6-d03f-4785-8fb2-eb0af775a4ab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---CALL AGENT---\n",
            "\"Output from node 'agent':\"\n",
            "'---'\n",
            "{ 'messages': [ AIMessage(content='', additional_kwargs={'function_call': {'name': 'retrieve_information', 'arguments': '{\"query\": \"best practices for Memory in LLM Agents\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-4e4b0e5b-8d55-4489-932e-71a090fb2fca-0', tool_calls=[{'name': 'retrieve_information', 'args': {'query': 'best practices for Memory in LLM Agents'}, 'id': '0a082cca-0b2f-49d1-bf8d-c06f84fd5894', 'type': 'tool_call'}], usage_metadata={'input_tokens': 59, 'output_tokens': 12, 'total_tokens': 71, 'input_token_details': {'cache_read': 0}})]}\n",
            "'\\n---\\n'\n",
            "---CHECK RELEVANCE---\n",
            "---DECISION: DOCS RELEVANT---\n",
            "\"Output from node 'retrieve':\"\n",
            "'---'\n",
            "{ 'messages': [ ToolMessage(content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\n}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:', name='retrieve_information', id='e2b81fc7-3771-4d8b-bb6d-ebee2f1d5814', tool_call_id='0a082cca-0b2f-49d1-bf8d-c06f84fd5894')]}\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Output from node 'generate':\"\n",
            "'---'\n",
            "{ 'messages': [ 'Based on the provided text, best practices for memory in LLM '\n",
            "                'agents involve understanding the limitations of finite '\n",
            "                'context length and utilizing external vector stores for '\n",
            "                'long-term memory.  These vector stores should support fast '\n",
            "                'maximum inner-product search (MIPS) using approximate nearest '\n",
            "                'neighbors (ANN) algorithms. Additionally, the text suggests '\n",
            "                'that short-term memory is akin to in-context learning, '\n",
            "                \"limited by the transformer's context window.\"]}\n",
            "'\\n---\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "inputs = {\n",
        "    \"messages\": [\n",
        "        (\"user\", \"Can you show me some LLM Architectures for Resilient Agents?\"),\n",
        "    ]\n",
        "}\n",
        "for output in graph.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        pprint.pprint(f\"Output from node '{key}':\")\n",
        "        pprint.pprint(\"---\")\n",
        "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2MwnktJJolF",
        "outputId": "601cc43d-554a-42de-845f-e631585ebde7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---CALL AGENT---\n",
            "\"Output from node 'agent':\"\n",
            "'---'\n",
            "{ 'messages': [ AIMessage(content='', additional_kwargs={'function_call': {'name': 'retrieve_information', 'arguments': '{\"query\": \"LLM Architectures for Resilient Agents\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-fd98eb4d-2afb-447a-910b-4e9ded0fd021-0', tool_calls=[{'name': 'retrieve_information', 'args': {'query': 'LLM Architectures for Resilient Agents'}, 'id': 'caf00ce6-9b6f-464c-a600-4105d80a2b9c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 12, 'total_tokens': 73, 'input_token_details': {'cache_read': 0}})]}\n",
            "'\\n---\\n'\n",
            "---CHECK RELEVANCE---\n",
            "---DECISION: DOCS NOT RELEVANT---\n",
            "no\n",
            "\"Output from node 'retrieve':\"\n",
            "'---'\n",
            "{ 'messages': [ ToolMessage(content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nNlp\\nLanguage-Model\\nAgent\\nSteerability\\nPrompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\nNlp\\nLanguage-Model\\nSafety\\nAdversarial Attacks\\nRobustness\\nRedteam\\n\\n\\n\\n« \\n\\nThinking about High-Quality Human Data\\n\\n\\n »\\n\\nLLM Powered Autonomous Agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"', name='retrieve_information', id='d31ee493-879c-4320-9802-74d46e8dc33a', tool_call_id='caf00ce6-9b6f-464c-a600-4105d80a2b9c')]}\n",
            "'\\n---\\n'\n",
            "---TRANSFORM QUERY---\n",
            "\"Output from node 'rewrite':\"\n",
            "'---'\n",
            "{ 'messages': [ AIMessage(content='Okay, here\\'s my attempt at formulating an improved question, considering the underlying intent:\\n\\n**Improved Question:**\\n\\n\"What are some established or promising architectural patterns and design principles for building Large Language Model (LLM)-based agents that are robust, reliable, and can handle unexpected situations or errors gracefully?\"\\n\\n**Reasoning for the Improvement:**\\n\\n* **\"LLM Architectures\" is vague:** The original question uses \"LLM Architectures,\" which could refer to the internal structure of the LLM itself (e.g., transformer layers) or the overall system architecture of an agent using an LLM. The improved question clarifies that we\\'re interested in the latter – the architecture of the *agent* that *uses* an LLM.\\n* **\"Resilient Agents\" is broad:** \"Resilient\" is a good starting point, but it\\'s a broad term. The improved question specifies what \"resilient\" means in this context: \"robust, reliable, and can handle unexpected situations or errors gracefully.\" This provides more specific criteria for the desired architectures.\\n* **Focus on Patterns and Principles:** Instead of just asking for \"architectures,\" the improved question asks for \"architectural patterns and design principles.\" This encourages a more conceptual and reusable answer, rather than just a list of specific implementations.\\n* **Explicitly mentions LLM-based agents:** The improved question makes it clear that we\\'re talking about agents that are built using LLMs, which is the core of the original question\\'s intent.\\n\\n**Why this is better:**\\n\\nThe improved question is more precise and actionable. It guides the response towards a discussion of:\\n\\n* **Specific techniques:** Like error handling, feedback loops, planning mechanisms, memory management, etc.\\n* **Generalizable patterns:** Like modular design, layered architectures, or the use of specific components.\\n* **Design considerations:** Like how to handle uncertainty, how to recover from failures, and how to ensure the agent\\'s behavior is predictable.\\n\\nBy being more specific, the improved question is more likely to elicit a helpful and informative response.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-f772b181-8797-4f77-82d5-4a6676fe0a60-0', usage_metadata={'input_tokens': 69, 'output_tokens': 431, 'total_tokens': 500, 'input_token_details': {'cache_read': 0}})]}\n",
            "'\\n---\\n'\n",
            "---CALL AGENT---\n",
            "\"Output from node 'agent':\"\n",
            "'---'\n",
            "{ 'messages': [ AIMessage(content='', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-990812a0-4571-475f-865e-f1f93a3236d2-0', usage_metadata={'input_tokens': 2152, 'output_tokens': 1, 'total_tokens': 2153, 'input_token_details': {'cache_read': 0}})]}\n",
            "'\\n---\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "inputs = {\n",
        "    \"messages\": [\n",
        "        (\"user\", \"What are some established or promising architectural patterns and design principles for building Large Language Model (LLM)-based agents that are robust, reliable, and can handle unexpected situations or errors gracefully?\"),\n",
        "    ]\n",
        "}\n",
        "for output in graph.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        pprint.pprint(f\"Output from node '{key}':\")\n",
        "        pprint.pprint(\"---\")\n",
        "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ0ejGdZJ-OZ",
        "outputId": "358df43f-6681-4846-eaaf-173c043019d1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---CALL AGENT---\n",
            "\"Output from node 'agent':\"\n",
            "'---'\n",
            "{ 'messages': [ AIMessage(content='', additional_kwargs={'function_call': {'name': 'retrieve_information', 'arguments': '{\"query\": \"architectural patterns and design principles for building robust and reliable LLM-based agents\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-65cedfd3-2b8a-415e-8495-cbf75a622d61-0', tool_calls=[{'name': 'retrieve_information', 'args': {'query': 'architectural patterns and design principles for building robust and reliable LLM-based agents'}, 'id': 'dad1734b-215d-4c14-b753-0c2c2f3046f2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 84, 'output_tokens': 19, 'total_tokens': 103, 'input_token_details': {'cache_read': 0}})]}\n",
            "'\\n---\\n'\n",
            "---CHECK RELEVANCE---\n",
            "---DECISION: DOCS RELEVANT---\n",
            "\"Output from node 'retrieve':\"\n",
            "'---'\n",
            "{ 'messages': [ ToolMessage(content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\n}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\nKaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui\\nZhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.\\nAgentbench: Evaluating LLMs as agents. In ICLR, 2024.\\nPotsawee Manakul, Adian Liusie, and Mark John Francis Gales. Selfcheckgpt: Zero-resource\\nblack-box hallucination detection for generative large language models. 2023.\\n11\\nPaper Under Review\\nStanley Milgram. Behavioral study of obedience. The Journal of Abnormal and Social Psychology,\\n67(4):371–378, 1963.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke\\nZettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\\nprecision in long form text generation. 2023.\\nMixtral.AI. Mixtral of experts. 2024.\\nRaymond S. Nickerson. Confirmation bias: A ubiquitous phenomenon in many guises. Review of\\nGeneral Psychology, 1998.\\nRichard E Nisbett and Timothy D Wilson. The halo effect: Evidence for unconscious alteration of\\njudgments. Journal of personality and social psychology, 35(4):250, 1977.\\nAbiodun Finbarrs Oketunji, Muhammad Anas, and Deepthi Saina. Large language model (llm) bias\\nindex–llmbi. arXiv preprint arXiv:2312.14769, 2023.\\nOpenAI. Gpt-4 technical report. 2023.\\nCharlie Pilgrim, Adam Sanborn, Eugene Malthouse, and Thomas T Hills. Confirmation bias emerges\\nfrom an approximation to bayesian reasoning. Cognition, 245:105693, 2024.\\nKariyushi Rao and Reid Hastie. Predicting outcomes in a sequence of binary events: Belief updating\\nand gambler’s fallacy reasoning. Cognitive Science, 47(1):e13211, 2023.\\nVipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation models,\\n2023.\\nSiyue Ren, Zhiyao Cui, Ruiqi Song, Zhen Wang, and Shuyue Hu. Emergence of social norms in\\nlarge language model-based agent societies. arXiv preprint arXiv:2403.08251, 2024.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\\nence on computer vision and pattern recognition, pp. 10684–10695, 2022.\\nVinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit,\\nAmeet Deshpande, Karthik Narasimhan, and Vishvak Murahari. Personagym: Evaluating persona\\nagents and llms. arXiv preprint arXiv:2407.18416, 2024.\\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Common-\\nsense reasoning about social interactions. In EMNLP, 2019.\\nClaude E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 1948.\\nYunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-LLM: A trainable agent for\\nrole-playing. In EMNLP, 2023.\\nTianhao Shen, Sun Li, and Deyi Xiong. Roleeval: A bilingual role evaluation benchmark for large\\nlanguage models. arXiv preprint arXiv:2312.16132, 2023.\\nS. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman\\nChadha, and Amitava Das. A comprehensive survey of hallucination mitigation techniques in large\\nlanguage models, 2024.\\nSoroush Vosoughi, Deb Roy, and Sinan Aral. The spread of true and false news online. Science,\\n2018.\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"', name='retrieve_information', id='c86e6dfe-4482-4134-aa63-603fb6ebea35', tool_call_id='dad1734b-215d-4c14-b753-0c2c2f3046f2')]}\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Output from node 'generate':\"\n",
            "'---'\n",
            "{ 'messages': [ 'The provided text discusses challenges in building LLM-based '\n",
            "                'agents, such as limited context length, difficulties in '\n",
            "                'long-term planning, and the unreliability of natural language '\n",
            "                'interfaces. However, it does not explicitly mention '\n",
            "                'established or promising architectural patterns and design '\n",
            "                'principles for building robust and reliable agents. '\n",
            "                'Therefore, I cannot answer your question based on the given '\n",
            "                'context.']}\n",
            "'\\n---\\n'\n"
          ]
        }
      ]
    }
  ]
}